[{"categories":["tutoriales"],"content":"En este post vamos a presentar un ejemplo de un pipeline de modelado predictivo típico usando datos tabulares, es decir, que pueden ser estructurados en una tabla de 2 dimensiones. En primer lugar, analizaremos el dataset usado y posteriormente entrenaremos un primer pipeline predictivo. Después prestaremos atención a los tipos de datos que tiene que manejar nuestro modelo: numéricos y categóricos. Por último, extenderemos nuestro pipeline para tipos de datos mixtos, es decir, numéricos y categóricos.\nEl objetivo a conseguir es construir intuiciones respecto a un dataset desconocido, identificar y discriminar features numéricas y categóricas y, finalmente, crear un pipeline predictivo avanzado con scikit-learn.\nEn concreto mostraremos los siguientes aspectos:\n identificar datos númericos en un dataset heterogéneo seleccionar el subconjunto de columnas correspondientes a datos numéricos usar la función de scikit-learn train_test_split para dividir los datos en entrenamiento y prueba entrenar y evaluar un modelo de regresión logística la importancia de evaluar el rendimiento de generalización en los datos de prueba usar un predictor dummy para obtener una línea base de referencia ver la importancia de escalar las variables numéricas usar un pipeline para encadenar el escalado y el entrenamiento de una regresión logística evaluar el rendimiento de generalización de nuestro modelo a partir de validación cruzada mostrar las dos estrategias comunes para codificar variables categóricas: ordinal y one-hot usar un pipeline para utilizar un codificador one-hot antes de entrenar un predictor usar un ColumnTransformer para aplicar preprocesamientos diferentes a variables numéricas y categóricas usar un pipeline para encadenar el preprocesamiento ColumnTransformer y entrenar una regresión logística  Primer vistazo al dataset Antes de llevar a cabo cualquier tarea de machine learning hay que realizar un serie de pasos:\n cargar los datos. observar las variables del dataset, diferenciando entre variables numéricas y categóricas, las cuales necesitarán un preprocesamiento diferente en la mayoría de los flujos de machine learning. visualizar la distribución de las variables para obtener algún tipo de conocimiento o idea del dataset.  Usaremos el dataset “credit-g”. Para más detalles sobre dicho dataset puedes acceder al link https://www.openml.org/d/31. El objetivo del dataset es clasificar a las personas por un conjunto de atributos como buenas o malas respecto al riesgo crediticio. Los datos están disponibles en un fichero CSV y usaremos pandas para leerlo.\nimport numpy as np import pandas as pd credit = pd.read_csv(\"credit-g.csv\") Las variables del dataset Los datos se almacenan en un dataframe de pandas. Un dataframe es una estructura de datos de 2 dimensiones. Este tipo de datos también se denominan datos tabulares.\nCada fila representa un “ejemplo”. En el ámbito de machine learning se usan normalmente los términos equivalentes de “registro”, “instancia” u “observación”.\nCada columna representa un tipo de información que ha sido recopilada y se denominan “features”. En el campo de machine learning es normal usar los términos equivalentes de “variable”, “atributo” o “covariable”.\nEchemos un vistazo rápido al dataframe para mostrar las primeras filas:\ncredit.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status duration credit_history purpose credit_amount savings_status employment installment_commitment personal_status other_parties ... property_magnitude age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class     0 '\u003c0' 6 'critical/other existing credit' radio/tv 1169 'no known savings' '\u003e=7' 4 'male single' none ... 'real estate' 67 none own 2 skilled 1 yes yes good   1 '0\u003c=X\u003c200' 48 'existing paid' radio/tv 5951 '\u003c100' '1\u003c=X\u003c4' 2 'female div/dep/mar' none ... 'real estate' 22 none own 1 skilled 1 none yes bad   2 'no checking' 12 'critical/other existing credit' education 2096 '\u003c100' '4\u003c=X\u003c7' 2 'male single' none ... 'real estate' 49 none own 1 'unskilled resident' 2 none yes good   3 '\u003c0' 42 'existing paid' furniture/equipment 7882 '\u003c100' '4\u003c=X\u003c7' 2 'male single' guarantor ... 'life insurance' 45 none 'for free' 1 skilled 2 none yes good   4 '\u003c0' 24 'delayed previously' 'new car' 4870 '\u003c100' '1\u003c=X\u003c4' 3 'male single' none ... 'no known property' 53 none 'for free' 2 skilled 2 none yes bad    5 rows × 21 columns\n credit.shape (1000, 21)  El dataset está compuesto de 1.000 instancias y 21 variables. La columna llamada class es nuestra variable objetivo (es decir, la variable que queremos predecir). Las dos posibles clases son good (bajo riesgo credicitio) y bad (alto riesgo crediticio). El problema de predicción resultante es, por tanto, un problema de clasificación binaria. Usaremos el resto de columnas como variables de entrada para nuestro modelo.\ncredit[\"class\"].value_counts() good 700 bad 300 Name: class, dtype: int64  credit[\"class\"].value_counts().plot.pie(autopct='%1.2f%%'); Vemos que las clases están desbalanceadas, lo que significa que tenemos más instancias de una o más clases comparada con las otras. El desequilibro de clases sucede frecuentemente en la práctica y puede requerir de técnicas especiales al construir el modelo predictivo. Veremos este tipo de técnicas en otros posts.\ncredit.dtypes checking_status object duration int64 credit_history object purpose object credit_amount int64 savings_status object employment object installment_commitment int64 personal_status object other_parties object residence_since int64 property_magnitude object age int64 other_payment_plans object housing object existing_credits int64 job object num_dependents int64 own_telephone object foreign_worker object class object dtype: object  credit.dtypes.value_counts() object 14 int64 7 dtype: int64  Comprobamos que el dataset contiene tanto datos numéricos (7 features) como categóricos (14 features, incluyendo la variable objetivo). En este caso sus tipos son int64 y object, respectivamente.\nInspección visual de los datos Antes de construir cualquier modelo predictivo es buena idea echar un vistazo a los datos:\n quizás la tarea que estamos intentando conseguir se pueda resolver sin utilizar machine learning; debemos comprobar que la información que necesitamos se encuentra presente realmente en el dataset; inspeccionar los datos en una buena forma de encontrar peculiaridades. Estas pueden aparecer durante la recolección de los datos (por ejemplo, debido al malfuncionamiento de sensores o valores faltantes) o en la forma en que los datos son procesados posteriormente (por ejemplo, valores “capados”).  Echemos un vistazo a las distribuciones de las features individualmente para obtener algún conocimiento adicional sobre los datos. Podemos empezar dibujando histogramas, aunque esto solo aplicaría a las features numéricas:\n_ = credit.hist(figsize=(20, 14)) C:\\Program Files\\Python39\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col():  Algunos comentarios sobre estas variables:\n duration: la mayoría de las personas a las que se les concede el crédito su duración está entre aproximadamente 4 y 24 meses, principalmente entre 12 y 24 meses. credit_amount: la mayoría de las personas solicita un crédito menor de 4.000 aproximadamente. age: la mayoría de las personas que solicitan un crédito son menores de 40 años.  Veamos la distribución de algunas variables categóricas:\nimport seaborn as sns _ = sns.countplot(x=\"checking_status\", data=credit) ax = sns.countplot(x=\"credit_history\", data=credit) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\"); Bueno, hasta ahora hemos visto cómo cargar un dataset, calcular su tamaño y visualizar de forma rápida las primeras filas del mismo. En un primer análisis de las variables que lo componen, hemos identificado nuestra variable objetivo y diferenciado las variables numéricas y categóricas. También hemos podido observar cómo se distribuyen sus valores.\nModelo simple con scikit-learn Vamos a crear un primer modelo predictivo, para lo cual solo usaremos las variables numéricas. Los datos numéricos son el tipo de datos más natural en machine learning y (casi) pueden incorporarse directamente a los modelos predictivos.\nComo hemos visto, el archivo CSV contiene toda la información que necesitamos: el objetivo que nos gustaría predecir (es decir, class) y los datos que queremos usar para entrenar nuestro modelo predictivo (es decir, las columnas restantes). El primer paso es separar las columnas para obtener de un lado el objetivo y del otro lado los datos.\nSeparar los datos y el objetivo target_name = \"class\" y = credit[target_name] data = credit.drop(columns=[target_name]) Vamos a usar una función de sklearn que nos permite seleccionar las columnas en función del tipo de dato.\nfrom sklearn.compose import make_column_selector as selector numerical_columns_selector = selector(dtype_include=np.number) numerical_columns = numerical_columns_selector(data) numerical_columns ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']  X = data[numerical_columns] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     0 6 1169 4 4 67 2 1   1 48 5951 2 2 22 1 1   2 12 2096 2 3 49 1 2   3 42 7882 2 4 45 1 2   4 24 4870 3 4 53 2 2     Entrenar un modelo y hacer predicciones Vamos a construir un modelo de clasificación usando regresión logística, que pertenece a la familia de los modelos lineales.\nBrevemente, los modelos lineales buscan un conjunto de pesos para combinar linealmente las features y predecir el objetivo. Por ejemplo, el modelo puede generar un regla como la siguiente:\n  si 0.1 * duration + 3.3 * credit_amount - 15.1 * installment_commitment + 3.2 * residence_since - 0.2 * age + 1.3 * existing_credits - 0.9 * num_dependents + 13.2 \u003e 0, predice good\n  en caso contrario predice bad\n  El metodo fit se llama para entrenar el modelo a partir de los datos de entrada (features) y objetivo.\nfrom sklearn import set_config set_config(display=\"diagram\") from sklearn.linear_model import LogisticRegression model = LogisticRegression(max_iter=500) model.fit(X, y) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}LogisticRegressionLogisticRegression(max_iter=500) El proceso de aprendizaje puede representarse de la siguiente forma:\nEl método fitse compone de dos elementos: un algoritmo de aprendizaje y algunos estados del modelo. El algoritmo de aprendizaje toma los datos y el objetivo de entrenamiento como entrada y establece los estados del modelo. Estos estados del modelo se utilizarán posteriormente para predecir (por clasificadores o regresores) o transformar los datos (por transformadores).\nTanto el algoritmo de aprendizaje como el tipo de estados del modelo son específicos para cada tipo de modelo.\nUsaremos ahora nuestro modelo para llevar a cabo algunas predicciones usando el mismo dataset.\ny_predicted = model.predict(X) El mecanismo de predicción puede representarse de la siguiente forma:\nPara predecir, un modelo usa una función de predicción que utilizará los datos de entrada junto con los estados del modelo. Como el algoritmo de aprendizaje y los estados del modelo, la función de predicción es específica para cada tipo de modelo.\nVamos a revisar las predicciones calculadas. Por simplicidad vamos a echar un vistazo a los primeros cinco objetivos predichos.\ny_predicted[:5] array(['good', 'bad', 'good', 'good', 'good'], dtype=object)  De hecho, podemos comparar estas predicciones con los datos reales:\ny[:5] 0 good 1 bad 2 good 3 good 4 bad Name: class, dtype: object  e incluso podríamos comprobar si las predicciones concuerdan con los objetivos reales:\ny_predicted[:5] == y[:5] 0 True 1 True 2 True 3 True 4 False Name: class, dtype: bool  print(f\"Nº de predicciones correctas: {(y_predicted[:5] == y[:5]).sum()} de las 5 primeras\") Nº de predicciones correctas: 4 de las 5 primeras  En este caso, parece que nuestro modelo comete un error al predecir la quinta instancia. Para obtener un mejor evaluación podemos calcular la tasa promedio de éxito:\n(y_predicted == y).mean() 0.706  ¿Podemos confiar en esta evaluación? ¿Es buena o mala?\nDivisión de los datos en entrenamiento y prueba Cuando construimos un modelo de machine learning es muy importante evaluar el modelo entrenado en datos que no se hayan usado para entrenarlo, ya que la generalización es más que la memorización (significa que queremos una regla que generalice a nuevos datos, sin comparar los datos memorizados). Es más difícil concluir sobre datos nunca vistos que sobre los ya vistos.\nLa evaluación correcta se realiza fácilmente reservando un subconjunto de los datos cuando entrenamos el modelo y usándolos posteriormente para evaluar el modelo. Los datos usados para entrenar un modelo se denominan datos de entrenamiento mientras que los datos usados para evaluar el modelo se denominan datos de prueba.\nEn ocasiones podemos contar con dos datasets separados, uno para el entrenamiento y otro para pruebas. Sin embargo, esto suele ser bastante inusual. La mayoría de las veces tendremos un único archivo que contiene todos los datos y necesitaremos dividirlo una vez cargado en memoria.\nScikit-learn proporciona la función sklearn.model_selection.train_test_split, que usaremos para dividir automáticamente el dataset en dos subconjuntos.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=42, test_size=0.20) Cuando llamamos a la función train_test_split, especificamos que queremos tener el 20% de las instancias en el conjunto de prueba y las instancias restantes (80%) estarán disponibles para el conjunto de entrenamiento.\nEstablecimiento de una línea base Para avaluar el rendimiento de nuestro modelo predictivo resulta de utilidad establecer una línea base simple. La línea base más simple para un clasificador es aquella que predice siempre la misma clase, independientemente de los datos de entrada. Para ello usaremos un DummyClassifier.\nfrom sklearn.dummy import DummyClassifier clf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=42) clf_dummy.fit(X_train, y_train) accuracy_dummy = clf_dummy.score(X_test, y_test) print(f\"Accuracy línea base: {accuracy_dummy}\") Accuracy línea base: 0.705  Este clasificador dummy predice siempre la clase más frecuente (en nuestro caso, la clase good). Como vimos anteriormente la proporción de clase good era del 70%, que coincide con la puntuación obtenido por este clasificador. Bien, ya tenemos una linea base con la que comparar nuestro modelo.\nVamos a entrenar el modelo exactamente de la misma forma que vimos anteriormente, excepto que usaremos para ello los subconjuntos de entrenamiento:\nmodel = LogisticRegression(max_iter=500) model.fit(X_train, y_train) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}LogisticRegressionLogisticRegression(max_iter=500) En lugar de calcular la predicción y calcular manualmente la tasa media de éxito, podemos usar el método score. Cuando se trata de clasificadores este método devuelve su métrica de rendimiento.\naccuracy_lgr = model.score(X_test, y_test) print(f\"Accuracy: {accuracy_lgr:.3f}\") Accuracy: 0.740  Veamos el mecanismo subyacente cuando se llama al método score:\nPara calcular la puntuación, el predictor primero calcula las predicciones (usando el metodo predict) y luego usa una función de puntuación para comparar los objetivos reales y las predicciones. Por último, se devuelve la puntuación.\nPor norma general, nos referimos al rendimiento de generalización de un modelo cuando nos refiramos a la puntuación de prueba o al error de prueba obtenido al comparar la predicción de un modelo con los objetivos reales. También son términos equivalentes rendimiento predictivo y rendimiento estadístico. Nos referimos al rendimiento computacional de un modelo predictivo cuando accedemos al coste computacional de entrenar un modelo predictivo o usarlo para hacer predicciones.\nBueno, la puntuación de nuestro modelo apenas mejora la linea base que establecimos:\nprint(f\"Accuracy línea base = {accuracy_dummy}\") print(f\"Accuracy regresión logística = {accuracy_lgr}\") Accuracy línea base = 0.705 Accuracy regresión logística = 0.74  Seguro que podemos hacerlo mejor. Veamos cómo.\nPreprocesamiento de features numéricas En los siguientes apartados vamos a introducir el uso del preprocesamiento, en este caso del escalado de variables numéricas; y del uso de un pipeline para encadenar el preprocesamiento y el entrenamiento del modelo.\nScikit-learn cuenta con una amplia gama de algoritmos de preprocesamiento que nos permiten transformar los datos de entrada antes de entrenar un modelo. Es este caso, vamos a estandarizar los datos y después entrenaremos un nuevo modelo de regresión logística es esta nueva versión del dataset.\nEn primer lugar, vamos a mostrar algunas estadísticas sobre los datos de entrenamiento.\nX_train.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     count 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000   mean 21.095000 3360.618750 2.965000 2.846250 35.558750 1.411250 1.143750   std 11.807211 2898.174863 1.122653 1.105277 11.411587 0.578828 0.351056   min 4.000000 250.000000 1.000000 1.000000 19.000000 1.000000 1.000000   25% 12.000000 1380.000000 2.000000 2.000000 27.000000 1.000000 1.000000   50% 18.000000 2333.000000 3.000000 3.000000 33.000000 1.000000 1.000000   75% 24.000000 4154.500000 4.000000 4.000000 42.000000 2.000000 1.000000   max 60.000000 18424.000000 4.000000 4.000000 75.000000 4.000000 2.000000     Observamos que las features del dataset abarcan diferentes rangos. Algunos algoritmos hacen suposiciones con respecto a las distribuciones de las features y, en general, la normalización de estas features resultará de utilidad para abordar estas suposiciones. Algunas razones para el escalado de features son las siguientes:\n  Los modelos basados en distancias entre pares de instancias, por ejemplo, k-nearest neighbors, deben ser entrenados con features normalizadas para hacer que cada feature contribuya aproximadamente por igual a los cálculos de distancias.\n  Muchos modelos, como la regresión logística usan solucionadores numéricos (basados en descenso de gradiente) para encontrar sus parámetros óptimos. Este solucionador converge más rápido cuando las features están escaladas.\n  El hecho de que un modelo de machile learning requiera o no de escalado de features depende de la familia del modelo. Los modelos lineales como la regresión logística generalmente se benefician del escalado mientras que otros modelos como los árboles de decisión no necesitan de este preprocesado (pero tampoco se verían penalizados).\nVamos a mostrar cómo aplicar tal normalización usando un transformador de scikit-learn llamado StandardScaler. Este transformador escala cada feature individualmente para que todas tengan media 0 y desviación estándar 1. Analizaremos los diferentes pasos usados por scikit-learn para conseguir esta transformación de los datos.\nLo primero que necesitamos es llamar al método fit para que aprenda el escalado de los datos.\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}StandardScalerStandardScaler() El método fit de los transformadores es similar al método fit de los predictores. La diferencia principal es que el transformador tiene un único argumento, la matriz de datos, mientras que el último tiene dos argumentos, la matriz de datos y el objetivo). En este caso, el algoritmo necesita calcular la media y la desviación típica de cada feature y almacenarla en algunas arrays de Numpy. Aquí, estos estadísticos son los estados del modelo. El hecho de que los estados del modelo de este scaler sean arrays de medias y desviaciones típicas es específico del StandardScale. Otros transformadores de scikit-learn calcularán diferentes estadísticos y los almacenarán como estados del modelo de la misma forma.\nVamos a inspeccionar las medias y desviaciones típicas calculadas.\nscaler.mean_ array([2.10950000e+01, 3.36061875e+03, 2.96500000e+00, 2.84625000e+00, 3.55587500e+01, 1.41125000e+00, 1.14375000e+00])  scaler.scale_ array([1.17998294e+01, 2.89636294e+03, 1.12195142e+00, 1.10458632e+00, 1.14044530e+01, 5.78466453e-01, 3.50836055e-01])  Ya sabemos, por convención de scikit-learn, que si un atributo aprende de los datos, su nombre termina con _, como mean_ y scale_ para el StandardScaler.\nEl escalado de los datos se aplica a cada feature de forma individual. Para cada feature, restamos su media y dividimos por su desviación típica. Una vez que hemos llamado al método fit podemos ejecutar la transformación llamando al método transform.\nX_train_scaled = scaler.transform(X_train) X_train_scaled array([[ 3.29708155, 1.19991221, 0.03119565, ..., 2.4061873 , 1.01777726, -0.40973554], [-0.00805096, -0.35962991, -0.86010854, ..., -0.22436411, -0.71093146, -0.40973554], [-1.27925578, -0.73354714, -0.86010854, ..., 1.26628169, -0.71093146, -0.40973554], ..., [ 0.24619 , 0.84360327, 0.92249983, ..., -0.7504744 , 1.01777726, -0.40973554], [-0.77077385, -0.64792251, -0.86010854, ..., -0.92584449, -0.71093146, -0.40973554], [-1.27925578, -0.83850636, 0.03119565, ..., -1.01352954, -0.71093146, -0.40973554]])  Vamos a analizar el mecanismo interno del método transform y lo pondremos en perspectiva con lo que ya vimos con los predictores.\nEl método transform para los transformadores es similar al método predict para los predictores. Usa una función predefinida, llamada función de transformación, y usa los estados del modelo y los datos de entrada. Sin embargo, en lugar de devolver predicciones, el trabajo del método transform es devolver una versión transformada de los datos de entrada.\nPor último, el método fit_transform es un método abreviado para llamar sucesivamente a fity después a transform.\nX_train_scaled = scaler.fit_transform(X_train) X_train_scaled array([[ 3.29708155, 1.19991221, 0.03119565, ..., 2.4061873 , 1.01777726, -0.40973554], [-0.00805096, -0.35962991, -0.86010854, ..., -0.22436411, -0.71093146, -0.40973554], [-1.27925578, -0.73354714, -0.86010854, ..., 1.26628169, -0.71093146, -0.40973554], ..., [ 0.24619 , 0.84360327, 0.92249983, ..., -0.7504744 , 1.01777726, -0.40973554], [-0.77077385, -0.64792251, -0.86010854, ..., -0.92584449, -0.71093146, -0.40973554], [-1.27925578, -0.83850636, 0.03119565, ..., -1.01352954, -0.71093146, -0.40973554]])  X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns) X_train_scaled.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     count 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02   mean 9.769963e-17 -1.776357e-17 1.465494e-16 2.664535e-17 -3.075318e-16 2.242651e-16 -8.437695e-17   std 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00   min -1.448750e+00 -1.073974e+00 -1.751413e+00 -1.671440e+00 -1.451955e+00 -7.109315e-01 -4.097355e-01   25% -7.707739e-01 -6.838296e-01 -8.601085e-01 -7.661239e-01 -7.504744e-01 -7.109315e-01 -4.097355e-01   50% -2.622919e-01 -3.547963e-01 3.119565e-02 1.391924e-01 -2.243641e-01 -7.109315e-01 -4.097355e-01   75% 2.461900e-01 2.740959e-01 9.224998e-01 1.044509e+00 5.648013e-01 1.017777e+00 -4.097355e-01   max 3.297082e+00 5.200792e+00 9.224998e-01 1.044509e+00 3.458408e+00 4.475195e+00 2.440599e+00     Observemos que la media de todas las columnas es cercana a 0 y la desviación típica en todos los casos es cercano a 1. Podemos visualizar el efecto de StandarScaler usando un joinplot para mostrar ambos histogramas de distribución y un scatterplot de cada par de features numéricas al mismo tiempo. Observamos que StandardScaler no cambia la estructura de los datos en si mismos sino que los ejes han sido desplazados y escalados.\nimport matplotlib.pyplot as plt # number of points to visualize to have a clearer plot num_points_to_plot = 300 sns.jointplot(data=X_train[:num_points_to_plot], x=\"age\", y=\"credit_amount\", marginal_kws=dict(bins=15)) plt.suptitle(\"Jointplot de 'age' vs 'credit_amount' \\nantes de StandardScaler\", y=1.1) sns.jointplot(data=X_train_scaled[:num_points_to_plot], x=\"age\", y=\"credit_amount\", marginal_kws=dict(bins=15)) _ = plt.suptitle(\"Jointplot de 'age' vs 'credit_amount' \\ndespués de StandardScaler\", y=1.1) Podemos combinar fácilmente operaciones secuenciales con un pipeline de scikit-learn, que encadena juntas operaciones y se usa como cualquier otro clasificador o regresor. La función make_pipeline creará un Pipeline: toma como argumentos las sucesivas transformaciones a ejecutar, seguido por el modelo clasificador o regresor.\nimport time from sklearn.pipeline import make_pipeline model = make_pipeline(StandardScaler(), LogisticRegression()) model div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}PipelinePipeline(steps=[('standardscaler', StandardScaler()),  ('logisticregression', LogisticRegression())])\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-serial\"\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"8d859484-a933-41a0-9166-7e42f1304392\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"8d859484-a933-41a0-9166-7e42f1304392\"\u003eStandardScaler\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eStandardScaler()\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"2770c1bf-a52a-4dd4-b674-8382c7cdd256\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"2770c1bf-a52a-4dd4-b674-8382c7cdd256\"\u003eLogisticRegression\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eLogisticRegression()\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e  La función make_pipeline no requiere que demos un nombre a cada paso. De hecho, se lo asigna automáticamente basado en el nombre de la clase suministrada; un StandardScaler tendrá un paso llamado \"standardscaler\" en el pipeline resultante. Podemos comprobar el nombre de cada paso del modelo:\nmodel.named_steps {'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression()}  Este pipeline predictivo expone los mismos métodos que el predictor final: fit y predict (y adicionalmente predict_proba, decision_function o score).\nstart = time.time() model.fit(X_train, y_train) elapsed_time = time.time() - start Cuando llamamos a model.fit, se llamará al método fit_transform para cada transformador subyacente (en este caso, un único transformador) para:\n aprender sus estados de modelo internos transformar los datos de entrenamiento. Finalmente, los datos preprocesados se suministrarán para entrenar el predictor.  Para predecir los objetivos dado un conjunto de prueba se usa el método predict.\ny_predicted = model.predict(X_test) y_predicted[:5] array(['good', 'good', 'good', 'good', 'good'], dtype=object)  Mostremos el mecanismo subyacente:\nSe llama al método transform de cada transformador (en este caso, un único transformador) para preprocesar los datos. Tengamos en cuenta que no es necesario llamar al método fit de esos transformadores porque estamos usando los estados de modelo internos calculados cuando llamamos a model.fil. Los datos preprocesados son entonces proporcionados al predictor que devolverá los objetivos predichos llamando al método predict.\nComo atajo, podemos comprobar la puntuación del pipeline predictivo completo llamando al método model.score. Por tanto, vamos a verificar el rendimiento computacional y de generalización de este pipeline predictivo.\nmodel_name = model.__class__.__name__ score = model.score(X_test, y_test) print(f\"La precisión usando un {model_name} es {score:.3f} \" f\"con un tiempo de entrenamiento de {elapsed_time:.3f} segundos \" f\"en {model[-1].n_iter_[0]} iteraciones\") La precisión usando un Pipeline es 0.730 con un tiempo de entrenamiento de 0.009 segundos en 10 iteraciones  model = LogisticRegression() start = time.time() model.fit(X_train, y_train) elapsed_time = time.time() - start model_name = model.__class__.__name__ score = model.score(X_test, y_test) print(f\"La precisión usando {model_name} es {score:.3f} \" f\"con un tiempo de entrenamiento de {elapsed_time:.3f} segundos \" f\"en {model.n_iter_[0]} iteraciones\") La precisión usando LogisticRegression es 0.740 con un tiempo de entrenamiento de 0.025 segundos en 88 iteraciones  Vemos que escalar los datos antes de entrenar la regresión logística fue beneficioso en términos de rendimiento computacional. De hecho, el número de iteracioens decrece así como el tiempo de entrenamiento. El rendimiento de generalización no cambió dado que ambos modelos comvergen.\nTrabajar con datos no escalados forzará potencialmente al algoritmo a iterar más como hemos visto. También existe el escenario catastrófico donde el número de iteraciones requeridas sea mayor que el número de iteraciones permitidas por el parámetro del predictor (controlado por max_iter). Por lo tanto, antes de incrementar max_iter, asegurémosnos de que los datos están escalados.\nEvaluación del modelo usando validación cruzada Vamos a discutir algunos aspectos prácticos de evaluar el rendimiento de generalización de nuestro modelo a través de la validación cruzada, en lugar de usar una única división entrenamiento-prueba.\nLa necesidad de validación cruzada Anteriormente dividimos los datos originales en un conjunto de entrenamiento y en un conjunto de pruebas. En general, la puntuación de un modelo dependerá de la forma en que hacemos esta división. Un inconveniente de hacer una única división es que no proporciona ninguna información sobre su variablidad. Otro inconveniente, en una configuración donde la cantidad de datos es pequeña, es que la cantidad de datos disponibles para entrenamiento y prueba será incluso más pequeña después de la división.\nEn su lugar, podemos usar validación cruzada. La validación cruzada consiste en repetir el procedimiento de modo que los conjuntos de entrenamiento y prueba sean diferentes cada vez. Las métricas de rendimiento de generalización se recopilan en cada repetición y luego se agregan. Como resultado, podemos evaluar la variabilidad de nuestra medida del rendimiento de generalización del modelo.\nExisten varias estrategias de validación cruzada, cada una de ellas define cómo repetir el procedimiento de fit / score. En nuestro caso, usaremos la estrategia K-fold: el dataset completo se divide en K particiones. El procesimiento fit / score se repite K veces, donde en cada iteración se usan K-1 particiones para entrenar el modelo y 1 partición para prueba. El siguiente diagrama muestra esta estrategia de K-fold.\nEl diagrama muestra el caso particular de K-fold. Para cada división de validación cruzada, el procedimiento entrena un clon del modelo en todas las instancias rojas y evalúa la puntuación del modelo en las instancias azules.\nLa validación cruzada es, por tanto, computacionalmente intensiva porque requiere entrenar varios modelos, en lugar de uno solo. En scikit-learn, la función cross_validate permite hacer validación cruzada y necesitamos pasar el modelo, los datos y el objetivo. Dado que existen varias estrategias, cross_validate toma un parámetro cv que define la estrategia de división.\n%%time from sklearn.model_selection import cross_validate model = make_pipeline(StandardScaler(), LogisticRegression()) cv_result = cross_validate(model, X, y, cv=5) cv_result Wall time: 50.5 ms {'fit_time': array([0.00750613, 0.00700498, 0.00700617, 0.00750685, 0.00700593]), 'score_time': array([0.002002 , 0.00150108, 0.00200152, 0.00200224, 0.00150156]), 'test_score': array([0.715, 0.71 , 0.69 , 0.715, 0.735])}  La salida de cross_validate es un diccionario de Python, que contiene tres entradas por defecto:\n el tiempo de entrenamiento del modelo en los datos de entrenamiento en cada una de las particiones el tiempo de predicción con el modelo en los datos de prueba en cada una de las particiones la puntuación por defecto en los datos de prueba en cada una de las particiones  Establecer cv=5 crea 5 divisiones distintas para obtener 5 variaciones distintas para los conjuntos de entrenamiento y prueba. Cada conjunto de entrenamiento se usa para entrenar un modelo que después se evalúa en el conjunto de prueba. La estrategia por defecto cuando se establece cv=int es la validación cruzada K-fold, donde K corresponde al número (entero) de divisiones. Establecer cv=5 o cv=10 es una práctica común, ya que mantiene un equilibrio entre el tiempo de cálculo y la estabilidad de la variabilidad estimada.\nHay que tener en cuenta que la función cross-validate, por defecto, descarta los K modelos que se entrenaron en los diferentes subconjuntos superpuestos del dataset. El objetivo de la validación cruzada no es entrenar un modelo sino estimar aproximadamente el rendimiento de generalización de un modelo que se habría entrenado en el conjunto completo de entrenamiento, junto con una estimación de la variabilidad (incertidumbre sobre la precisión de la generalización).\nPodemos pasar parámetros adiciones a sklearn.mode_selection.cross_validate para recopilar información adicional, tales como las puntuaciones de entrenamiento de los modelos obtenidos en cada ciclo o incluso devolver los propios modelos en lugar de descartarlos.\nVamos a extraer las puntuaciones calculadas en las particiones de prueba de cada ciclo de validación cruzada a partir del diccionario cv_result y calcular la precisión media y la variación de la precisión a lo largo de las particiones.\nscores = cv_result[\"test_score\"] print( \"La precisión media de validación cruzada es: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\" ) La precisión media de validación cruzada es: 0.713 +/- 0.014  Hay que tener en cuenta que al calcular la desviación típica de las puntuaciones de validación cruzada, podemos estimar la incertidumbre del rendimiento de generalización de nuestro modelo. Esta es la principal ventaja de la validación cruzada y puede ser crucial en la práctica, por ejemplo cuando comparamos diferentes modelos para averiguar cuál de ellos es mejor que los demás o si nuestras medidas del rendimiento de generalización de cada modelo están dentro de las barras de error de uno u otro.\nCodificación de variables categóricas Bien, hasta ahora hemos visto como manejarnos con variables numéricas y codificarlas. Vamos a ver como codificar variables categóricas usando codificación ordinal y one-hot.\nYa vimos anteriormente que una variable numérica se una cantidad representada por un número entero o real. Estas variables se manejan de forma natural por los algoritmos de machine learning, que generalmente se componen de una secuencia de instrucciones aritméticas, como sumas y multiplicaciones.\nPor el contrario, las variables categóricas están representadas normalmente por etiquetas de texto (pero no solo) tomadas de entre una lista finita de opciones posibles. Por ejemplo, la variable personal_status de nuestro dataset es una variable categórica porque codifica los datos usando una lista finita de posibles estados:\ncredit[\"personal_status\"].value_counts() 'male single' 548 'female div/dep/mar' 310 'male mar/wid' 92 'male div/sep' 50 Name: personal_status, dtype: int64  ¿Cómo podemos reconocer las columnas categóricas dentro del dataset? Una parte de la respuesta tiene que ver con el tipo de dato de las columnas:\ncredit.dtypes checking_status object duration int64 credit_history object purpose object credit_amount int64 savings_status object employment object installment_commitment int64 personal_status object other_parties object residence_since int64 property_magnitude object age int64 other_payment_plans object housing object existing_credits int64 job object num_dependents int64 own_telephone object foreign_worker object class object dtype: object  Si observamos la columna personal_status podemos comprobar que su tipo de dato es object, lo que significar que contiene valores de texto.\nSeleccionar features en función del su tipo de datos Para seleccionar columnas basadas en su tipo de datos podemos usar la función make_column_selector de scikit-learn, como vimos anteriormente.\ntarget_name = \"class\" y = credit[target_name] data = credit.drop(columns=[target_name]) categorical_columns_selector = selector(dtype_include=object) categorical_columns = categorical_columns_selector(data) categorical_columns ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']  Hemos creado el selector pasándole el tipo de datos que queremos incluir; hemos pasado el dataset de entrada al objeto selector, que devuelve una listado de nombres de columnas que tienen el tipo de datos requerido. Ahora podemos filtrar las columnas que no queremos:\nX = data[categorical_columns] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status credit_history purpose savings_status employment personal_status other_parties property_magnitude other_payment_plans housing job own_telephone foreign_worker     0 '\u003c0' 'critical/other existing credit' radio/tv 'no known savings' '\u003e=7' 'male single' none 'real estate' none own skilled yes yes   1 '0\u003c=X\u003c200' 'existing paid' radio/tv '\u003c100' '1\u003c=X\u003c4' 'female div/dep/mar' none 'real estate' none own skilled none yes   2 'no checking' 'critical/other existing credit' education '\u003c100' '4\u003c=X\u003c7' 'male single' none 'real estate' none own 'unskilled resident' none yes   3 '\u003c0' 'existing paid' furniture/equipment '\u003c100' '4\u003c=X\u003c7' 'male single' guarantor 'life insurance' none 'for free' skilled none yes   4 '\u003c0' 'delayed previously' 'new car' '\u003c100' '1\u003c=X\u003c4' 'male single' none 'no known property' none 'for free' skilled none yes     Vamos a presentar diferentes estrategias de codificación de datos categóricos a datos numéricos que puedan ser usados por un algoritmo de machine learning.\nEstrategias para codificar categorías Codificando categorías ordinales La estrategia más intuitiva es codificar cada categoría con un número diferente. OrdinalEncoder transforma los datos de esta forma. Empezaremos codificando una única columna para comprender como funciona:\nfrom sklearn.preprocessing import OrdinalEncoder personal_status_column = X[[\"personal_status\"]] encoder = OrdinalEncoder() personal_status_encoded = encoder.fit_transform(personal_status_column) personal_status_encoded[:10] array([[3.], [0.], [3.], [3.], [3.], [3.], [3.], [3.], [1.], [2.]])  Vemos que cada categoría de personal_status ha sido reemplazada por un valor numérico. Podemos comprobar el mapeo entre las categorías y los valoras numéricos comprobando el atributo entrenado categories_.\nencoder.categories_ [array([\"'female div/dep/mar'\", \"'male div/sep'\", \"'male mar/wid'\", \"'male single'\"], dtype=object)]  Ahora podemos comprobar la codificación aplicada en todas las categorías.\nX_encoded = encoder.fit_transform(X) X_encoded[:5] array([[1., 1., 7., 4., 3., 3., 2., 2., 1., 1., 3., 1., 1.], [0., 3., 7., 2., 0., 0., 2., 2., 1., 1., 3., 0., 1.], [3., 1., 4., 2., 1., 3., 2., 2., 1., 1., 2., 0., 1.], [1., 3., 5., 2., 1., 3., 1., 0., 1., 0., 3., 0., 1.], [1., 2., 1., 2., 0., 3., 2., 1., 1., 0., 3., 0., 1.]])  Vemos que todas las categorías se han codificado en cada feature (columna) de forma independiente. También podemos comprobar que el número de features antes y después de la codificación sigue siendo el mismo.\nSin embargo, debemos ser cuidadosos cuando apliquemos esta estrategia de codificación: usar esta representación de enteros conduce a los modelos predictivos posteriores a asumir que los valores están ordenados (es decir, 0 \u003c 1 \u003c 2 \u003c 3 …).\nPor defecto, OrdinalEncoder usa una estrategia lexicográfica para mapear etiquetas categóricas de texto a enteros. Esta estrategia es arbitraria y a menudo sin sentido. Por ejemplo, supongamos que el dataset tiene una variable categórica llamada size con categorías como “S”, “M”, “L” y “XL”. Nos gustaría que la representación numérica respetase el significado de los tamaños mapeandolos incrementalmente con enteros, tal que 0, 1, 2, 3. Sin embargo, la estrategia lexicográfica usada por defecto podría mapear las etiquetas “S”, “M”, “L”, “XL” como 2, 1, 0, 3, siguiendo un orden alfabético.\nLa clase OrdinalEncoder acepta un argumento constructor categories para pasar explícitamente las categorías en el orden esperado.\nSi una variable categórica no contiene ninguna información significativa de orden esta codificación podría se engañosa para los modelos estadísticos posteriores y deberíamos considerar una codificación one-hot en su lugar.\nCodificando categorías nominales (sin asumir ningún orden) OneHotEncoder es un codificador alternativo que previene que los modelos posteriores hagan una falsa asunción sobre el orden de las categorías. Para una feature dada, crea tantas columnas como categorías posibles. Para una instancia dada, el valor de la columna correspondiente a la categoría se establecerá a 1 mientras que las columnas de las otras categorías se establecerán a 0.\nEmpecemos codificando una única feature (por ejemplo personal_status):\nfrom sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder(sparse=False) personal_status_encoded = encoder.fit_transform(personal_status_column) personal_status_encoded array([[0., 0., 0., 1.], [1., 0., 0., 0.], [0., 0., 0., 1.], ..., [0., 0., 0., 1.], [0., 0., 0., 1.], [0., 0., 0., 1.]])  sparce=False se usa en OneHotEncoder a modo didáctico, para tener un visualización más fácil de los datos. Las matrices dispersas son estructuras eficientes de datos donde la mayoría de los elementos de la matriz son ceros.\nVemos que codificar una única columna nos dará una matriz NumPy repleta de ceros y unos. Lo comprenderemos mejor usando los nombres asociados de las features resultado de la transformación.\nfeature_names = encoder.get_feature_names(input_features=[\"personal_status\"]) personal_status_encoded = pd.DataFrame(personal_status_encoded, columns=feature_names) personal_status_encoded  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  personal_status_'female div/dep/mar' personal_status_'male div/sep' personal_status_'male mar/wid' personal_status_'male single'     0 0.0 0.0 0.0 1.0   1 1.0 0.0 0.0 0.0   2 0.0 0.0 0.0 1.0   3 0.0 0.0 0.0 1.0   4 0.0 0.0 0.0 1.0   ... ... ... ... ...   995 1.0 0.0 0.0 0.0   996 0.0 1.0 0.0 0.0   997 0.0 0.0 0.0 1.0   998 0.0 0.0 0.0 1.0   999 0.0 0.0 0.0 1.0    1000 rows × 4 columns\n Como podemos ver, cada categoría se convierte en una columna; la codificación devolvió, para cada ejemplo, un 1 para especificar a qué categoría pertenece. Vamos a aplicarlo a todo el dataset.\nX_encoded = encoder.fit_transform(X) X_encoded[:5] array([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.], [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.], [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.], [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.]])  columns_encoded = encoder.get_feature_names(X.columns) pd.DataFrame(X_encoded, columns=columns_encoded).head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status_'0\u003c=X\u003c200' checking_status_'\u003c0' checking_status_'\u003e=200' checking_status_'no checking' credit_history_'all paid' credit_history_'critical/other existing credit' credit_history_'delayed previously' credit_history_'existing paid' credit_history_'no credits/all paid' purpose_'domestic appliance' ... housing_own housing_rent job_'high qualif/self emp/mgmt' job_'unemp/unskilled non res' job_'unskilled resident' job_skilled own_telephone_none own_telephone_yes foreign_worker_no foreign_worker_yes     0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0   1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0   2 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0   3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0   4 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0    5 rows × 54 columns\n Echemos un vistazo a cómo la variable purpose ha sido codificada y comparémosla con su original representación. El número de features después del codificado es 10 veces mayor que su representación original, debido al elevado número de posibles categorías.\nEligiendo una estrategia de codificación La elección de una estrategia de codificación dependerá de los modelos subyacentes y del tipo de categorías (es decir, ordinales vs nominales). En general, OneHotEncoder es la estrategia usada cuando los modelos posteriores son modelos lineales mientras que OrdinalEncoder es frecuentemente una buena estrategia con modelos basados en árboles.\nUsar un OrdinalEncoder devolverá categorias ordinales. Este significa que existe un orden en las categorías resultantes (es decir, 0 \u003c 1 \u003c 2). El impacto de violar esta asunción de ordenación realmente depende de los modelos posteriores. Los modelos lineales se verán impactados por categorías desordenadas mientras que los modelos basados en árbol no.\nAun así podemos usar un OrdinalEncoder con modelos lineales pero necesitamos asegurarnos de que:\n las categorías originales (antes de codificar) tienen un orden; las categorías codificadas siguen el mismo orden que las categorías originales.  La codificación One-hot de variables categóricas con una alta cardinalidad pueden provocar ineficiencia computacional en modelos basados en árbol. Debido a esto, no es recomendable usar OneHotEncoder en tales casos, incluso aunque las categorías originales no tengan un orden dado.\nEvaluar nuestro pipeline predictivo Ahora podemos integrar este codificador dentro de un pipeline de machine learning como hicimos con los datos numéricos: entrenemos un clasificador lineal en los datos codificados y verifiquemos el rendimiento de generalización desde este pipeline de machine learning usando validación cruzada.\nAntes de crear el pipeline veamos algunas situaciones que pueden ocurrir en la validación cruzada. Puede ocurrir que algunos valores de una variable categórica sucedan de forma muy infrecuente. Por ejemplo, con el ejemplo anterior de la variable size podemos tener que para la categoría S ocurra 24 veces, M ocurra 25 veces, L ocurra 21 veces y XL ocurra 1 vez. Esto puede ser un problema durante la validación cruzada: si la muestra termina en el conjunto de prueba durante la división, entonces el clasificador no vería esta categoría durante el entrenamiento y no sería capaz de codificarla. En scikit-learn, existen dos soluciones para solventar este problema:\n enumenar todos las posible categorías y proporcionarlas al codificador a través del parámetro categories; usar el parámetro handle_unknown.  Por simplicidad, usaremos esta última solución.\nTengamos en cuenta que OrdinalEncoder también expone un parámetro handle_unknown. Puede ser establecido en use_encoded_value y estableciendo unknown_value para manejar categorías raras.\nAhora podemos crear nuestro pipeline de machine learning.\nmodel = make_pipeline( OneHotEncoder(handle_unknown=\"ignore\"), LogisticRegression(max_iter=500) ) Aquí necesitamos incrementar el número máximo de iteraciones para obtener una covergencia plena de LogisticRegresion y evitar un ConvergenceWarning. Al contrario que las features numéricas, las features categóricas codificadas one-hot tiene todas la misma escala (los valores son 0 o 1), por no que no obtenemos ningún beneficio del escalado. En este caso, incrementar max_iter es la forma correcta de hacer.\nFinalmente, vamos a comprobar el rendimiento de generalización del modelo usando únicamente las columnas categóricas.\ncv_results = cross_validate(model, X, y) cv_results {'fit_time': array([0.03252769, 0.02702308, 0.02702284, 0.02552271, 0.02652216]), 'score_time': array([0.00300288, 0.00350356, 0.00300264, 0.00300264, 0.00300312]), 'test_score': array([0.74 , 0.75 , 0.765, 0.72 , 0.73 ])}  scores = cv_results[\"test_score\"] print(f\"La precisión es: {scores.mean():.3f} +/- {scores.std():.3f}\") La precisión es: 0.741 +/- 0.016  En este caso, esta representación de las variables categóricas es ligeramente más predictiva del riesgo crediticio que las variables numéricas usadas anteriormente.\nUsando juntas variables numéricas y categóricas Hasta el momento hemos visto el preprocesamiento requerido cuando manejamos variables numéricas y categóricas. Sin embargo, desvinculamos el proceso para tratar cada tipo individualmente. Vamos mostrar cómo combinar estos pasos de preprocesamiento.\nSelección basada en tipos de datos Repitamos de nuevo el procedimiento para separar las variables categóricas y numéricas según sus tipos de dato:\ntarget_name = \"class\" y = credit[target_name] X = credit.drop(columns=[target_name]) numerical_columns_selector = selector(dtype_exclude=object) categorical_columns_selector = selector(dtype_include=object) numerical_columns = numerical_columns_selector(X) categorical_columns = categorical_columns_selector(X) Atención, en este ejemplo sabemos que el tipo de datos object se usa para representar textos y, por tanto, features categóricas. Tengamos precaución porque esto no es siempre el caso. Algunas veces el tipo de dados object podría contener otros tipos de información, como fechas que no tenían el formato adecuado y, sin embargo, se relacionan a una cantidad de tiempo transcurrido.\nEn un escenario más general, deberíamos inspeccionar manualmente el contenido de nuestro dataframe para no usar equivocadamente make_column_selector.\nEnviar columnas a un procesador específico Ya vimos anteriormente que necesitamos tratar los datos de forma diferente dependiendo de su naturaleza (numérica o categórica). Scikit-learn proporciona una clase ColumnTransformer que enviará columnas específicas a transformadores específicos, haciendo fácil entrenar un único modelo predictivo en un dataset que combina ambos tipos de variables juntas (datos tabulares tipados heterogéneamente).\nEn primer lugar definimos las columnas dependiendo de su tipo de dato:\n La codificación one-hot se aplicará a las columnas categóricas. Además, usaremos handle_unknown=\"ignore\" para solventar el potencial problema debido a cartegorías raras. El escalado numérico de las features numéricas será estandarizado.  Ahora creamos nuestro ColumnTransformer especificando los tres valores: el nombre del preprocesador, el transformador y las columnas. En primer lugar, vamos a crear los preprocesadores para las partes numéricas y categóricas.\ncategorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\") numerical_preprocessor = StandardScaler() Ahora creamos el transformador y asociamos cada uno de los preprocesadores con sus respectivas columnas.\nfrom sklearn.compose import ColumnTransformer preprocessor = ColumnTransformer([ (\"one-hot-encoder\", categorical_preprocessor, categorical_columns), (\"standard_scaler\", numerical_preprocessor, numerical_columns) ]) Vamos a representar gráficamente la estructura de ColumnTransformer:\nUn ColumnTransformer hace lo siguiente:\n divide las columnas del dataset original basándose en los nombres de las columnas o índices proporcionados. Obtendremos tantos subconjuntos como números de transformadores pasados al ColumnTransformer. transforma cada subconjunto. Se aplica un transformador específico a cada subconjunto: internamente llamará a fit_transform o transform. La salida de este paso es un conjunto de datasets transformados. por último, concatena los datasets transformados en un único dataset.  Lo importante es que ColumnTransformer es como cualquier otro transformador de scikit-learn. Puede ser combinado con un clasificador en un Pipeline:\nmodel = make_pipeline(preprocessor, LogisticRegression(max_iter=500)) set_config(display=\"diagram\") model div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}PipelinePipeline(steps=[('columntransformer',  ColumnTransformer(transformers=[('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'), ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']), ('standard_scaler', StandardScaler(), ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents'])])), ('logisticregression', LogisticRegression(max_iter=500))])\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-serial\"\u003e\u003cdiv class=\"sk-item sk-dashed-wrapped\"\u003e\u003cdiv class=\"sk-label-container\"\u003e\u003cdiv class=\"sk-label sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"28b13a79-07ce-4ef7-bb21-70e5639f0d70\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"28b13a79-07ce-4ef7-bb21-70e5639f0d70\"\u003ecolumntransformer: ColumnTransformer\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eColumnTransformer(transformers=[('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'), ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']), ('standard_scaler', StandardScaler(), ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents'])])\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-parallel\"\u003e\u003cdiv class=\"sk-parallel-item\"\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-label-container\"\u003e\u003cdiv class=\"sk-label sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"ac8c34c9-f6a7-4691-94de-9a0ea7a204ad\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"ac8c34c9-f6a7-4691-94de-9a0ea7a204ad\"\u003eone-hot-encoder\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003e['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-serial\"\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"f2ec9dd6-7b7f-4908-8c77-c46773019d1f\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"f2ec9dd6-7b7f-4908-8c77-c46773019d1f\"\u003eOneHotEncoder\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eOneHotEncoder(handle_unknown='ignore')\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-parallel-item\"\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-label-container\"\u003e\u003cdiv class=\"sk-label sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"bfbe5363-1d03-4406-a5f7-fad8ed53b97f\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"bfbe5363-1d03-4406-a5f7-fad8ed53b97f\"\u003estandard_scaler\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003e['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-serial\"\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"30d7d67f-3d6e-4933-bf7e-0022b086c45d\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"30d7d67f-3d6e-4933-bf7e-0022b086c45d\"\u003eStandardScaler\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eStandardScaler()\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"c5b127e3-cbaa-4fdc-8a1f-bed669fd64c2\" type=\"checkbox\" \u003e\u003clabel class=\"sk-toggleable__label\" for=\"c5b127e3-cbaa-4fdc-8a1f-bed669fd64c2\"\u003eLogisticRegression\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eLogisticRegression(max_iter=500)\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e  El modelo final es más complejo que los que hemos visto previamente pero aún sigue la misma API (el mismo conjunto de métodos que pueden ser llamados por el usuario):\n el metodo fit es llamado para preprocesar los dantos y luego entrenar el clasificador en los datos preprocesados; el metodo predict hace predicciones en datos nuevos; el metodo score es usado para predecir en los datos de prueba y comparar las predicciones con las etiquetas de prueba esperadas para calcular la precisión.  Empecemos dividiendo nuestros datos en los conjuntos de entrenamiento y prueba:\nX_train, X_test, y_train, y_test = train_test_split( X, y, random_state=42) _ = model.fit(X_train, y_train) Luego, podemos enviar el dataset en bruto directamente al pipeline. De hecho, no necesitamos hacer ningún preprocesamiento manual (llamando a los métodos transform o fit_transform) ya que será manejado cuando llamemos al método predict. Como ejemplo, predeciremos em los primeros cinco ejemplos del conjunto de prueba.\nX_test.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status duration credit_history purpose credit_amount savings_status employment installment_commitment personal_status other_parties residence_since property_magnitude age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker     521 '\u003c0' 18 'existing paid' radio/tv 3190 '\u003c100' '1\u003c=X\u003c4' 2 'female div/dep/mar' none 2 'real estate' 24 none own 1 skilled 1 none yes   737 '\u003c0' 18 'existing paid' 'new car' 4380 '100\u003c=X\u003c500' '1\u003c=X\u003c4' 3 'male single' none 4 car 35 none own 1 'unskilled resident' 2 yes yes   740 '\u003c0' 24 'all paid' 'new car' 2325 '100\u003c=X\u003c500' '4\u003c=X\u003c7' 2 'male single' none 3 car 32 bank own 1 skilled 1 none yes   660 '\u003e=200' 12 'existing paid' radio/tv 1297 '\u003c100' '1\u003c=X\u003c4' 3 'male mar/wid' none 4 'real estate' 23 none rent 1 skilled 1 none yes   411 'no checking' 33 'critical/other existing credit' 'used car' 7253 '\u003c100' '4\u003c=X\u003c7' 3 'male single' none 2 car 35 none own 2 'high qualif/self emp/mgmt' 1 yes yes     model.predict(X_test)[:5] array(['good', 'bad', 'bad', 'good', 'good'], dtype=object)  y_test[:5] 521 bad 737 good 740 good 660 good 411 good Name: class, dtype: object  Para obtener directamente la puntuación de precisión, necesitamos llamar al método score. Calculemos la puntuación de precisión del conjunto de pruebas completo.\nmodel.score(X_test, y_test) 0.768  Evaluación del modelo con validación cruzada Como vimos anteriormente, un modelo predictivo debe ser evaluado con validación cruzada. Nuestro modelo es utilizable con herramientas de validación cruzada de scikit-learn como cualquier otro predictor:\ncv_results = cross_validate(model, X, y, cv=5) cv_results {'fit_time': array([0.04668927, 0.04341388, 0.04553866, 0.04053569, 0.03930879]), 'score_time': array([0.00600529, 0.00600529, 0.006495 , 0.00568366, 0.00550437]), 'test_score': array([0.75, 0.76, 0.76, 0.74, 0.75])}  scores = cv_results[\"test_score\"] print(\"La precisión media por validación cruzada es: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") La precisión media por validación cruzada es: 0.752 +/- 0.007  El modelo compuesto tiene una mayor precisión predicitiva que los dos modelos que utilizan variables numéricas y categóricas por separado.\n","description":"","tags":["pipeline","regresión"],"title":"Pipeline de modelado predictivo","uri":"/posts/predictive-modeling-pipeline/"},{"categories":["tutoriales"],"content":"Durante el mes de agosto he participado en el evento organizado por Kaggle denominado 30 Days of ML. Las dos primeras semanas consistieron en un repaso a los conceptos básicos de python y machine learning. Las últimas dos semanas participamos en una competición creada para todos los concursantes del evento.\nPara la competición disponíamos de una dataset sintético, pero basado en datos reales. El objetivo era predecir la cantidad de una reclamación del seguro. Las features estaban anonimizadas, pero relacionadas con features del mundo real. Las columnas de features cat0 a cat9 eran categóricas, y las columnas de features cont0 a cont13 continuas.\nNos proporcionan los siguientes archivos:\n train.csv - los datos de entrenamiento con la columna target test.csv - el conjuto de prueba; tendremos que predecir el target para cada una de las filas de este archivo sample_submission.csv - un archivo de envío de ejemplo con el formato correcto  Las semanas previas a la competición, durante el curso de machine learning, trabajamos principalmente con dos modelos:\n Random Forest Uso y optimización de modelos con gradient boosting. En concreto, hacemos uso de la librería XGBoost.  Por tanto, para esta competición seguí las líneas marcadas durantes las semanas de aprendizaje teórico y utilicé ambos modelos. A continuación detallo los pasos seguidos durante los días que trabajé en la competición.\nImportación de librerías necesarias import numpy as np import pandas as pd from sklearn.preprocessing import OrdinalEncoder from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.ensemble import RandomForestRegressor from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error Carga de los datasets # Carga de los datos de entrenamiento y prueba train = pd.read_csv(\"input/train.csv\", index_col=0) test = pd.read_csv(\"input/test.csv\", index_col=0) # Previsualización del dataset train.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13 target   id                          1 B B B C B B A E C N ... 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850 8.113634   2 B B A A B D A F A O ... 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083 8.481233   3 A A A C B D A D A F ... 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846 8.364351   4 B B A C B D A E C K ... 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682 8.049253   6 A A A C B D A E A N ... 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823 7.972260    5 rows × 25 columns\n Preprocesamiento # Separamos el target de las features y = train['target'] features = train.drop(['target'], axis=1) # Previsualización de las features features.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont4 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13   id                          1 B B B C B B A E C N ... 0.610706 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850   2 B B A A B D A F A O ... 0.276853 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083   3 A A A C B D A D A F ... 0.285074 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846   4 B B A C B D A E C K ... 0.284667 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682   6 A A A C B D A E A N ... 0.287595 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823    5 rows × 24 columns\n Seleccionamos y transformamos las variables categóricas a valores numéricos, antes de entrenar y evaluar nuestro modelo. Para ello usamos Ordinal Encoding.\n# Lista de columnas categóricas object_cols = [col for col in features.columns if 'cat' in col] # Aplicamos ordinal-encode a las columnas categóricas X = features.copy() X_test = test.copy() ordinal_encoder = OrdinalEncoder() X[object_cols] = ordinal_encoder.fit_transform(features[object_cols]) X_test[object_cols] = ordinal_encoder.transform(test[object_cols]) # Previsualización de las features con ordinal-encoded X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont4 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13   id                          1 1.0 1.0 1.0 2.0 1.0 1.0 0.0 4.0 2.0 13.0 ... 0.610706 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850   2 1.0 1.0 0.0 0.0 1.0 3.0 0.0 5.0 0.0 14.0 ... 0.276853 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083   3 0.0 0.0 0.0 2.0 1.0 3.0 0.0 3.0 0.0 5.0 ... 0.285074 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846   4 1.0 1.0 0.0 2.0 1.0 3.0 0.0 4.0 2.0 10.0 ... 0.284667 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682   6 0.0 0.0 0.0 2.0 1.0 3.0 0.0 4.0 0.0 13.0 ... 0.287595 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823    5 rows × 24 columns\n Extraemos un conjunto de validación a partir de los datos de entrenamiento:\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0) Intento 1 - Entrenamiento de un modelo Random Forest # Definimos el modelo model_rf = RandomForestRegressor(random_state=1, n_jobs=-1) # Entrenamiento del modelo (puede tarder unos minutos en terminar) model_rf.fit(X_train, y_train) preds_valid = model_rf.predict(X_valid) rmse_rf = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_rf) 0.7375392165180452  Bien, ya tenemos nuestro primer resultado. Estoy ansioso por realizar el proceso completo y comprobar mi posición en la clasificación. Así que sin más demora me lanzo a realizar mi primer submit.\n# Usamos el modelo entrenado para realizar predicciones predictions = model_rf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) Cuando enviamos dicho archivo nos indican cuál es la puntuación obtenida (public score). Kaggle calcula dicha puntuación usando solo una parte de los datos de prueba. La puntuación final (private score) se calculará usando el conjunto completo de prueba. La puntuación privada no será visible para nosotros ni para ninguno de los competidores y solo la conoceremos al final de la competición.\nLa puntuación pública obtenida es de 0.73845. Esta puntuación es resultado de entrenar un modelo Random Forest con los hiperparámetros por defecto, por lo tanto, nuestra posición en la clasificación se ubica en la parte baja de la tabla, igualada a la de otros miles de competidores (en total participamos 7.500 equipos). Por tanto, todavía tenemos mucho margen para seguir mejorando.\nIntento 2 - Entrenamiento de un modelo XGBoost Vamos a entrenar unos de los modelos “estrella” en muchas de las competiciones de Kaggle: XGBoost.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1, n_jobs=-1) # Entrenamiento del modelo model_xgb.fit(X_train, y_train) preds_valid = model_xgb.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7268784689736293  Bueno, hemos mejorado ligeramente respecto al uso de Random Forest. Así que como hicimos anteriormente, generamos nuestra predicciones, exportamos nuestro archivo de envío y lo subimos a Kaggle para ver nuestra puntuación.\n# Usamos el modelo entrenado para realizar predicciones predictions = model_xgb.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72613. Son solo unas décimas respecto al envío previo, pero suficientes para escalar a la zona media de la tabla. Seguro que podemos hacerlo mejor… por ejemplo, afinar algunos hiperparámetros. Vamos a ello.\nIntento 3 - Entrenamiento de un modelo XGBoost - Refinamiento usando Grid Search Para este refinamiento, vamos a usar GridSearch para encontrar cuál es la mejor combinación de algunos hiperparámetros.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1) clf = GridSearchCV(model_xgb, {'max_depth': [2, 4, 6], 'n_estimators': [50, 100, 200, 500]}, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=1) clf.fit(X_train, y_train) print(-clf.best_score_) print(clf.best_params_) Fitting 5 folds for each of 12 candidates, totalling 60 fits 0.7201387194775795 {'max_depth': 2, 'n_estimators': 500}  # Entrenamiento del modelo con los mejores parámetros preds_valid = clf.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7221846356377921  Bien, otra ligera mejora. Así que como hicimos anteriormente, generamos nuestra predicciones, exportamos nuestro archivo de envío y lo subimos a Kaggle para ver nuestra puntuación.\n# Usamos el modelo entrenado para realizar predicciones predictions = clf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72181. Igualmente, son solo unas décimas respecto al envío previo, pero suficientes para seguir escalando posiciones. Hemos superado la zona media de la tabla Sigamos afinando algunos hiperparámetros.\nIntento 4 (y último) - Entrenamiento de un modelo XGBoost - Refinamiento usando Grid Search Seguimos ajustando hiperparámetros. Dado que finalmente el mejor valor para n_estimators era 500, lo que representaba el limite superior de la lista proporcionada, vamos a seguir probando más alla de este límite.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1) clf = GridSearchCV(model_xgb, {'max_depth': [2], 'n_estimators': [500, 1000, 2000, 3000]}, scoring='neg_root_mean_squared_error', verbose=2, n_jobs=1) clf.fit(X_train, y_train) print(-clf.best_score_) print(clf.best_params_) Fitting 5 folds for each of 4 candidates, totalling 20 fits [CV] END ......................max_depth=2, n_estimators=500; total time= 16.7s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.8s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.6s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.7s [CV] END ......................max_depth=2, n_estimators=500; total time= 16.0s [CV] END .....................max_depth=2, n_estimators=1000; total time= 32.1s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.3s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.4s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.2s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.1s [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.0min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.7min 0.7194840350716621 {'max_depth': 2, 'n_estimators': 1000}  # Entrenamiento del modelo con los mejores parámetros preds_valid = clf.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7211765635584879  # Usamos el modelo entrenado para realizar predicciones predictions = clf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72028. Mejoramos ligeramente y subimos posiciones en la clasificación. Finalmente no puedo dedicarle más tiempo (los ciclos de entrenamiento llevan su tiempo) y Kaggle comunica la finalización del evento. Publica las puntuaciones privadas, calculadas sobre la totalidad de los datos de prueba: el score final obtenido es 0.71874. La posición final en la clasificación es 2780 sobre un total de 7572 concursantes. Los diez primeros clasificados se encuentran en una horquilla de 0.71533 a 0.71547.\nEn fin, no está mal. Sin tener más información sobre las features y su significado, podríamos seguir empleando fuerza bruta, potencia de cálculo y tiempo para seguir afinando hiperparámetros con el objetivo de seguir disminuyendo algunas milésimas a la métrica.\n","description":"","tags":["kaggle","regresión"],"title":"30 Days of ML","uri":"/posts/30daysofml/"},{"categories":["tutoriales"],"content":"ANALISIS DE LA CALIDAD DEL VINO - Clasificación multiclase En la primera parte de este análisis enfocamos el problema como aprendizaje supervisado - regresión. El modelo resultante no podemos considerarlo satisfactorio. Vamos a considerar el problema como aprendizaje supervisado - clasificación, concretamente clasificación multiclase.\nCarga de datos Importamos las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.model_selection import RandomizedSearchCV, cross_val_score from sklearn.model_selection import cross_validate, cross_val_predict from sklearn.linear_model import SGDClassifier from sklearn.dummy import DummyClassifier from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier from sklearn import metrics import xgboost as xgb %matplotlib inline import warnings warnings.filterwarnings('ignore') Leemos los datos y creamos un DataFrame\nwine = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Exploración de los datos wine.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     No vamos a profundizar en la exploración de datos, puesto que ya lo hicimos en la primera parte de este análisis (Calidad del vino - Un problema de regresión).\nPreparación de los datos El único preprocesamiento que vamos a realizar es convertir la variable objetivo \"quality\" a categórica.\nwine[\"quality_cat\"] = wine[\"quality\"].astype(\"category\") wine[\"quality_cat\"].value_counts() 5 681 6 638 7 199 4 53 8 18 3 10 Name: quality_cat, dtype: int64  print(f\"Porcentaje de cada una de las puntuaciones de calidad\") wine[\"quality_cat\"].value_counts(normalize=True)*100 Porcentaje de cada una de las puntuaciones de calidad 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality_cat, dtype: float64  Como ya vimos, el dataset se encuentra significativamente desbalanceado. La mayoría de las instancias (82%) tienen puntuaciones de 6 ó 5.\nA continuación creamos el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = wine.columns[:-2] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = wine[predict_columns] y = wine[\"quality_cat\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) Línea base Una pregunta que nos podemos hacer es si está justificado el uso del aprendizaje automático, si nos aporta valor respecto a predecir el azar. Por tanto, lo siguiente que haremos será entrenar un clasificador dummy que utilizaremos como línea base con el que comparar.\nEn primer lugar, entrenaremos un clasificador que genera predicciones uniformemente al azar.\nclf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Predice al azar clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='uniform')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.16108673901331486  Un clasificador que prediga al azar obtiene una puntuación accuracy del 16%.\nProbemos con otro clasificador, pero en este caso, que prediga siempre la clase más frecuente:\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) # Predice siempre la clase más frecuente clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='most_frequent')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.4308052321213254  Un clasificador que siempre prediga la clase más frecuente (en nuestro caso la puntuación de calidad 6) obtiene una accuracy del 43%. Vamos a tomar como línea base la predicción de este clasificador dummy.\npreds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1) Dibujemos su matriz de confusión:\nconf_mx = metrics.confusion_matrix(y_train, preds) conf_mx array([[ 0, 0, 9, 0, 0, 0], [ 0, 0, 43, 0, 0, 0], [ 0, 0, 551, 0, 0, 0], [ 0, 0, 506, 0, 0, 0], [ 0, 0, 157, 0, 0, 0], [ 0, 0, 13, 0, 0, 0]], dtype=int64)  fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=clf_dummy.classes_, yticklabels=clf_dummy.classes_,) accuracy_base = metrics.accuracy_score(y_train, preds) precision_base = metrics.precision_score(y_train, preds, average='weighted', zero_division=0) recall_base = metrics.recall_score(y_train, preds, average='weighted') f1_base = metrics.f1_score(y_train, preds, average='weighted') print(f\"Accuracy: {accuracy_base}\") print(f\"Precision: {precision_base}\") print(f\"Recall: {recall_base}\") print(f\"f1: {f1_base}\") Accuracy: 0.43080531665363564 Precision: 0.18559322085703928 Recall: 0.43080531665363564 f1: 0.25942484095754453  print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.43 1.00 0.60 551 6 0.00 0.00 0.00 506 7 0.00 0.00 0.00 157 8 0.00 0.00 0.00 13 accuracy 0.43 1279 macro avg 0.07 0.17 0.10 1279 weighted avg 0.19 0.43 0.26 1279  Nuestro clasificador dummy es correcto solo el 19% de las veces (precision) y detecta el 43% de las puntuaciones reales (recall). A menudo es conveniente combinar precisión y sensibilidad en una sola métrica llamada puntuación F1, en particular si necesitamos una forma sencilla de comparar dos clasificadores. La puntuación F1 es la media armónica de precisión y sensibilidad. Mientras que la media regular trata a todos los valores por igual, la media armónica otorga mucho más peso a los valores bajos. Como resultado, el clasificador solo obtendrá una puntuación alta en F1 si tanto la sensibilidad como la precisión son altas. En nuestro caso, F1 = 0,26. Bien, tomemos estas tres métricas como nuestra línea base inicial.\nPor tanto, nuestra línea base será:\n Precision: 0.1855 Recall: 0.4308 F1: 0.2594  Entrenamiento de diversos modelos def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = {\"accuracy\": \"accuracy\", \"precision\": \"precision_weighted\", \"recall\": \"recall_weighted\", \"f1\": \"f1_weighted\"} scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring) accuracy, accuracy_std = scores['test_accuracy'].mean(), \\ scores['test_accuracy'].std() precision, precision_std = scores['test_precision'].mean(), \\ scores['test_precision'].std() recall, recall_std = scores['test_recall'].mean(), \\ scores['test_recall'].std() f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std() result = { \"Accuracy\": accuracy, \"Accuracy std\": accuracy_std, \"Precision\": precision, \"Precision std\": precision_std, \"Recall\": recall, \"Recall std\": recall_std, \"f1\": f1, \"f1 std\": f1_std, } if verbose: print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\") print(f\"Precision: {precision} - (std: {precision_std})\") print(f\"Recall: {recall} - (std: {recall_std})\") print(f\"f1: {f1} - (std: {f1_std})\") return result models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed), DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), xgb.XGBClassifier()] model_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\", \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \"Gradient Boosting\", \"XGBoost\"] accuracy = [] precision = [] recall = [] f1 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") clf_scores = evaluate_model(models[model], X_train, y_train) accuracy.append(clf_scores[\"Accuracy\"]) precision.append(clf_scores[\"Precision\"]) recall.append(clf_scores[\"Recall\"]) f1.append(clf_scores[\"f1\"]) Paso 1 de 8 ...running Naive Bayes Gaussian Accuracy: 0.55125 - (std: 0.027102056829233452) Precision: 0.5646348802130249 - (std: 0.020745595731671666) Recall: 0.55125 - (std: 0.027102056829233452) f1: 0.5541082295110215 - (std: 0.023545313928114795) Paso 2 de 8 ...running K Neighbors Classifier Accuracy: 0.4964828431372549 - (std: 0.013777320430796238) Precision: 0.472985448646598 - (std: 0.015072330289309464) Recall: 0.4964828431372549 - (std: 0.013777320430796238) f1: 0.4749703234382818 - (std: 0.01350721905804416) Paso 3 de 8 ...running Random Forest Accuracy: 0.6826194852941176 - (std: 0.03746156433885403) Precision: 0.6585977991402794 - (std: 0.0406774341137893) Recall: 0.6826194852941176 - (std: 0.03746156433885403) f1: 0.6642629277794576 - (std: 0.03850557708999431) Paso 4 de 8 ...running Decision Tree Accuracy: 0.6012714460784314 - (std: 0.028539445741031087) Precision: 0.5978218408820158 - (std: 0.025874687130953537) Recall: 0.6012714460784314 - (std: 0.028539445741031087) f1: 0.5978989958450711 - (std: 0.0264307770802976) Paso 5 de 8 ...running Extra Tree Accuracy: 0.5676348039215686 - (std: 0.032774267548303905) Precision: 0.5697402861119303 - (std: 0.030789932683965727) Recall: 0.5676348039215686 - (std: 0.032774267548303905) f1: 0.5668315018481278 - (std: 0.031722387303563124) Paso 6 de 8 ...running Ada Boost Accuracy: 0.5504748774509804 - (std: 0.03954230035312734) Precision: 0.48457698009594374 - (std: 0.05118366184736229) Recall: 0.5504748774509804 - (std: 0.03954230035312734) f1: 0.5052214324230416 - (std: 0.03764434709325329) Paso 7 de 8 ...running Gradient Boosting Accuracy: 0.6474325980392157 - (std: 0.03472028817662461) Precision: 0.6218203966653049 - (std: 0.03370831758409691) Recall: 0.6474325980392157 - (std: 0.03472028817662461) f1: 0.6328837599218248 - (std: 0.03442412231869498) Paso 8 de 8 ...running XGBoost [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. Accuracy: 0.6560079656862745 - (std: 0.023339659857252816) Precision: 0.6346626310195044 - (std: 0.028312439862179448) Recall: 0.6560079656862745 - (std: 0.023339659857252816) f1: 0.6420686275076488 - (std: 0.024663282704859676)  df_result = pd.DataFrame({\"Model\": model_names, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}) df_result.sort_values(by=\"f1\", ascending=False)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model accuracy precision recall f1     2 Random Forest 0.682619 0.658598 0.682619 0.664263   7 XGBoost 0.656008 0.634663 0.656008 0.642069   6 Gradient Boosting 0.647433 0.621820 0.647433 0.632884   3 Decision Tree 0.601271 0.597822 0.601271 0.597899   4 Extra Tree 0.567635 0.569740 0.567635 0.566832   0 Naive Bayes Gaussian 0.551250 0.564635 0.551250 0.554108   5 Ada Boost 0.550475 0.484577 0.550475 0.505221   1 K Neighbors Classifier 0.496483 0.472985 0.496483 0.474970     Vamos a visualizar la comparativa de los diferentes modelos / métricas:\nmetrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"] for metric in metrics_list: df_result.sort_values(by=metric).plot.barh(\"Model\", metric) plt.title(f\"Model by {metric}\") plt.show() Obtenemos que el modelo que tiene mejor rendimiento es Random Forest. Examinemos un poco más en detalle la ejecución de Random Forest:\nclf_rf = RandomForestClassifier(random_state=seed) preds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1) clf_rf.get_params() {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 7 1 0 0   4 1 0 32 9 1 0   5 0 2 434 108 7 0   6 0 1 116 364 25 0   7 0 0 14 70 73 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.79 0.75 551 6 0.65 0.72 0.68 506 7 0.66 0.46 0.54 157 8 1.00 0.15 0.27 13 accuracy 0.68 1279 macro avg 0.50 0.35 0.37 1279 weighted avg 0.66 0.68 0.66 1279  El modelo es correcto el 66% de las veces (precision) y detecta el 68% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,66. Bueno, ha mejorado significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nEl % de mejora del indicador F1 respecto a la línea base es:\n% diferencia F1= (0.66 - 0.26) / 0.66 * 100 = 60.6%  Realmente la mejora respecto a la línea base es considerable, un 60%. Podemos concluir que está justificado el uso de aprendizaje automático para predecir la puntuación de calidad del vino.\n En general, si el porcentaje de mejora respecto a nuestra línea base no es mayor que un 5% deberíamos reconsiderar el uso de aprendizaje automático.\n Al examinar en detalle el resultado de las predicciones, podemos observar que es pésimo en las puntuaciones extremas (3, 4 y 8) y bastante malo en la puntuación 7.\nAjuste fino de hiperparámetros Vamos a realizar un ajuste de hiperparámetros a ver si se consigue alguna mejora.\nparam_grid = [ {\"n_estimators\": range(20, 200, 20), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) Ajuste inicial con Randomize Search En primer lugar hacemos un barrido rápido aleatorio:\nclf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, scoring=\"f1_weighted\", verbose=2, random_state=seed, n_jobs = -1) clf_random.fit(X_train, y_train) Fitting 5 folds for each of 200 candidates, totalling 1000 fits RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_iter=200, n_jobs=-1, param_distributions=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(20, 200, 20)}], random_state=42, scoring='f1_weighted', verbose=2)  clf_random.best_params_ {'n_estimators': 40, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': False}  preds = cross_val_predict(clf_random.best_estimator_, X_train, y_train, cv=5, n_jobs=-1) print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.81 0.76 551 6 0.66 0.71 0.68 506 7 0.66 0.46 0.55 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.51 0.36 0.38 1279 weighted avg 0.66 0.69 0.67 1279  Ajuste final con GridSearch Proseguimos con un ajuste final usando GridSearch:\nparam_grid = [ {\"n_estimators\": range(130, 200, 10), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) grid_search = GridSearchCV(clf_rf, param_grid, cv=5, scoring=\"f1_weighted\", verbose=2, n_jobs=-1) grid_search.fit(X_train, y_train) Fitting 5 folds for each of 6048 candidates, totalling 30240 fits GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_jobs=-1, param_grid=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(130, 200, 10)}], scoring='f1_weighted', verbose=2)  grid_search.best_params_ {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 170}  final_model = grid_search.best_estimator_ preds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1) pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 6 2 0 0   4 1 0 31 10 1 0   5 0 0 451 94 6 0   6 0 0 113 365 28 0   7 0 0 10 78 69 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.74 0.82 0.78 551 6 0.66 0.72 0.69 506 7 0.63 0.44 0.52 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.50 0.36 0.37 1279 weighted avg 0.67 0.69 0.67 1279  Tras el ajuste de hiperparámetros se consigue una muy ligera mejora respecto a los hiperparámetros por defecto. Es correcto el 67% de las veces (precision) y detecta el 69% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,67. Lo que mejora significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nPor último veamos cómo se ejecuta en el conjunto de prueba:\ny_pred = final_model.predict(X_test) pd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 5 6 7 8   Actual         3 1 0 0 0   4 6 4 0 0   5 101 28 1 0   6 37 89 6 0   7 0 22 19 1   8 0 1 4 0     print(metrics.classification_report(y_test, y_pred, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 1 4 0.00 0.00 0.00 10 5 0.70 0.78 0.73 130 6 0.62 0.67 0.64 132 7 0.63 0.45 0.53 42 8 0.00 0.00 0.00 5 accuracy 0.65 320 macro avg 0.32 0.32 0.32 320 weighted avg 0.62 0.65 0.63 320  Es correcto el 62% de las veces (precision) y detecta el 65% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,65.\naccuracy_best = metrics.accuracy_score(y_test, y_pred) precision_best = metrics.precision_score(y_test, y_pred, average='weighted', zero_division=0) recall_best = metrics.recall_score(y_test, y_pred, average='weighted') f1_best = metrics.f1_score(y_test, y_pred, average='weighted') Matriz de confusión conf_mx = metrics.confusion_matrix(y_test, y_pred) fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=final_model.classes_, yticklabels=final_model.classes_,) Feature importances feature_importances = final_model.feature_importances_ feature_importances array([0.06970454, 0.10304422, 0.07397403, 0.06774786, 0.07530372, 0.06051697, 0.09785917, 0.0830556 , 0.06881937, 0.12760515, 0.17236938])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.17236937962448678, 'alcohol'), (0.12760514906291182, 'sulphates'), (0.10304421805642286, 'volatile acidity'), (0.09785917335424621, 'total sulfur dioxide'), (0.0830555965951595, 'density'), (0.0753037227200391, 'chlorides'), (0.07397402652373279, 'citric acid'), (0.06970454021889655, 'fixed acidity'), (0.06881936733049614, 'pH'), (0.06774786106526597, 'residual sugar'), (0.06051696544834242, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances'); Observamos que las características que más influencia tienen en nuestro modelo son alcohol y sulphates, seguidas por volatile acidity y total sulfur dioxide.\nSelección de características Vamos a usar RFECV para determinar el nº de características válidas con cross-validation.\nfrom sklearn.feature_selection import RFECV from sklearn.model_selection import StratifiedKFold selector = RFECV(final_model, step=1, cv=StratifiedKFold()) selector = selector.fit(X_train, y_train) pd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Support     0 fixed acidity True   1 volatile acidity True   2 citric acid True   3 residual sugar True   4 chlorides True   5 free sulfur dioxide True   6 total sulfur dioxide True   7 density True   8 pH True   9 sulphates True   10 alcohol True     pd.DataFrame({\"Feature\": predict_columns, \"Ranking\": selector.ranking_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Ranking     0 fixed acidity 1   1 volatile acidity 1   2 citric acid 1   3 residual sugar 1   4 chlorides 1   5 free sulfur dioxide 1   6 total sulfur dioxide 1   7 density 1   8 pH 1   9 sulphates 1   10 alcohol 1     # Dibuja el número de features vs la puntuación a través de cross-validation plt.figure() plt.xlabel(\"Nº de features seleccionadas\") plt.ylabel(\"Puntuación cross validation\") plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_) plt.show() Observando la gráfica se concluye que todas las variables son importantes para el modelo, ya que se obtiene la máxima puntuación con las 10 características seleccionadas.\nselector.grid_scores_ array([0.49564951, 0.59737132, 0.65132353, 0.6661826 , 0.6739951 , 0.67869792, 0.67790135, 0.68573223, 0.68025123, 0.68808211, 0.69354167])  Guardado del modelo Por último, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model_clf.joblib\", compress=True) #final_model = joblib.load(\"final_model_clf.joblib\") ['final_model_clf.joblib']  Comentarios finales a los resultados Nuestra línea de base de partida, obtenida a partir de un clasificador que siempre predice la clase más frecuente, es la siguiente:\n Precision: 19% Recall: 43% Accuracy: 43% f1: 0.26  Una vez entrenados diversos modelos, el que mejores resultados ha proporcionados es RandomForest. Después de realizar un ajuste fino de hiperparámetros obtenemos las siguientes métricas:\n Precision: 67% Recall: 69% Accuracy: 69% f1: 0.67  La evaluación en el conjunto de prueba es la siguiente:\n Precision: 62% Recall: 65% Accuracy: 65% f1: 0.63  Al ser multiclase, estamos hablando de puntuaciones ponderadas. Sin embargo, las puntuaciones obtenidas por cada clase son muy dispares. Se puede observar que el resultado es pésimo en las puntuaciones extremas (3, 4 y 8). Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nTodas las variables predictoras son relevantes para el modelo. Las tres que más afectan en la predicción son las siguientes:\n alcohol sulphates volatile acidity.  Podría ser interesante evaluar el modelo segmentando nuestra variable objetivo en rangos de calidad (por ejemplo, baja, media y alta) y comprobar si obtenemos mejores resultados.\n ","description":"","tags":["clasificación","clasificación multiclase","random forest"],"title":"Calidad del vino - Clasificación multiclase","uri":"/posts/wine-quality-clasificacion-multiclase/"},{"categories":["tutoriales"],"content":"En este post repasaremos las principales fases que componen un proyecto de Machine Learning.\nExisten ocho pasos principales:\n  Encuadrar el problema y disponer de la visión global.\n  Obtener los datos.\n  Explorar los datos para obtener ideas.\n  Preparar los datos para exponer lo mejor posible los patrones de datos subyacentes a los algoritmos de Machine Learning.\n  Explorar muchos modelos diferentes y preseleccionar los mejores.\n  Afinar nuestros modelos y combinarlos en una gran solución.\n  Presentar nuestra solución.\n  Implantar, monitorizar y mantener nuestro sistema.\n  Disponemos un conjunto de datos que contiene diversas características de variantes de tinto y blanco del vino portugués “Vinho Verde”. Disponemos de variables químicas, como son la cantidad de alcohol, ácido cítrico, acidez, densidad, pH, etc; así como de una variable sensorial y subjetiva como es la puntuación con la que un grupo de expertos calificaron la calidad del vino: entre 0 (muy malo) y 10 (muy excelente).\nEl objetivo es desarrollar un modelo que pueda predecir la puntuación de calidad dados dichos indicadores bioquímicos.\nLo primero que nos viene a la mente son una serie de preguntas básicas:\n  ¿Cómo se enmarcaría este problema (supervisado, no supervisado, etc.)?\n  ¿Cuál es la variable objetivo? ¿Cuáles son los predictores?\n  ¿Cómo vamos a medir el rendimiento de nuestro modelo?\n  El codigo python utilizado en este artículo está disponible en mi repositorio github\nEn primer lugar importamos todas las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet, Ridge from sklearn.dummy import DummyRegressor from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor from sklearn.svm import SVR from sklearn import metrics %matplotlib inline Obtención de los datos El dataset se encuentra igualmente disponible en Kaggle o en UCI\nPodemos cargarlo directamente desde la url o una vez descargado desde nuestra carpeta data.\nred = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Verificamos el tamaño y el tipo de los datos\nred.shape (1599, 12)  red.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     red.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 1599 entries, 0 to 1598 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 fixed acidity 1599 non-null float64 1 volatile acidity 1599 non-null float64 2 citric acid 1599 non-null float64 3 residual sugar 1599 non-null float64 4 chlorides 1599 non-null float64 5 free sulfur dioxide 1599 non-null float64 6 total sulfur dioxide 1599 non-null float64 7 density 1599 non-null float64 8 pH 1599 non-null float64 9 sulphates 1599 non-null float64 10 alcohol 1599 non-null float64 11 quality 1599 non-null int64 dtypes: float64(11), int64(1) memory usage: 150.0 KB  Realizamos una serie de comprobaciones para conocer la naturaleza de los datos con los que vamos a trabajar: tipo, valores únicos, número de valores únicos y su porcentaje, valores medios y desviación estándar.\npd.DataFrame({\"Type\": red.dtypes, \"Unique\": red.nunique(), \"Null\": red.isnull().sum(), \"Null percent\": red.isnull().sum() / len(red), \"Mean\": red.mean(), \"Std\": red.std()})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Type Unique Null Null percent Mean Std     fixed acidity float64 96 0 0.0 8.319637 1.741096   volatile acidity float64 143 0 0.0 0.527821 0.179060   citric acid float64 80 0 0.0 0.270976 0.194801   residual sugar float64 91 0 0.0 2.538806 1.409928   chlorides float64 153 0 0.0 0.087467 0.047065   free sulfur dioxide float64 60 0 0.0 15.874922 10.460157   total sulfur dioxide float64 144 0 0.0 46.467792 32.895324   density float64 436 0 0.0 0.996747 0.001887   pH float64 89 0 0.0 3.311113 0.154386   sulphates float64 96 0 0.0 0.658149 0.169507   alcohol float64 65 0 0.0 10.422983 1.065668   quality int64 6 0 0.0 5.636023 0.807569     Mmmmm, no existen valores nulos, ¡qué buen dataset!\nred.describe().T  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count mean std min 25% 50% 75% max     fixed acidity 1599.0 8.319637 1.741096 4.60000 7.1000 7.90000 9.200000 15.90000   volatile acidity 1599.0 0.527821 0.179060 0.12000 0.3900 0.52000 0.640000 1.58000   citric acid 1599.0 0.270976 0.194801 0.00000 0.0900 0.26000 0.420000 1.00000   residual sugar 1599.0 2.538806 1.409928 0.90000 1.9000 2.20000 2.600000 15.50000   chlorides 1599.0 0.087467 0.047065 0.01200 0.0700 0.07900 0.090000 0.61100   free sulfur dioxide 1599.0 15.874922 10.460157 1.00000 7.0000 14.00000 21.000000 72.00000   total sulfur dioxide 1599.0 46.467792 32.895324 6.00000 22.0000 38.00000 62.000000 289.00000   density 1599.0 0.996747 0.001887 0.99007 0.9956 0.99675 0.997835 1.00369   pH 1599.0 3.311113 0.154386 2.74000 3.2100 3.31000 3.400000 4.01000   sulphates 1599.0 0.658149 0.169507 0.33000 0.5500 0.62000 0.730000 2.00000   alcohol 1599.0 10.422983 1.065668 8.40000 9.5000 10.20000 11.100000 14.90000   quality 1599.0 5.636023 0.807569 3.00000 5.0000 6.00000 6.000000 8.00000     Exploración de los datos El siguiente paso será realizar un análisis exploratorio de los datos. ¿Cómo se distribuyen las características?\nred.hist(bins=50, figsize=(15,12)); Verifiquemos ahora cómo se distribuye nuestra variable objetivo (la puntuación de calidad):\nprint(f\"Percentage of quality scores\") red[\"quality\"].value_counts(normalize=True) * 100 Percentage of quality scores 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality, dtype: float64  Podemos comprobar que se encuentra significativamente desbalanceada. La mayoría de las instancias (82%) tienen puntuaciones de 5 ó 6.\nVamos a verificar las correlaciones entre las características del dataset:\ncorr_matrix = red.corr() corr_matrix  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     fixed acidity 1.000000 -0.256131 0.671703 0.114777 0.093705 -0.153794 -0.113181 0.668047 -0.682978 0.183006 -0.061668 0.124052   volatile acidity -0.256131 1.000000 -0.552496 0.001918 0.061298 -0.010504 0.076470 0.022026 0.234937 -0.260987 -0.202288 -0.390558   citric acid 0.671703 -0.552496 1.000000 0.143577 0.203823 -0.060978 0.035533 0.364947 -0.541904 0.312770 0.109903 0.226373   residual sugar 0.114777 0.001918 0.143577 1.000000 0.055610 0.187049 0.203028 0.355283 -0.085652 0.005527 0.042075 0.013732   chlorides 0.093705 0.061298 0.203823 0.055610 1.000000 0.005562 0.047400 0.200632 -0.265026 0.371260 -0.221141 -0.128907   free sulfur dioxide -0.153794 -0.010504 -0.060978 0.187049 0.005562 1.000000 0.667666 -0.021946 0.070377 0.051658 -0.069408 -0.050656   total sulfur dioxide -0.113181 0.076470 0.035533 0.203028 0.047400 0.667666 1.000000 0.071269 -0.066495 0.042947 -0.205654 -0.185100   density 0.668047 0.022026 0.364947 0.355283 0.200632 -0.021946 0.071269 1.000000 -0.341699 0.148506 -0.496180 -0.174919   pH -0.682978 0.234937 -0.541904 -0.085652 -0.265026 0.070377 -0.066495 -0.341699 1.000000 -0.196648 0.205633 -0.057731   sulphates 0.183006 -0.260987 0.312770 0.005527 0.371260 0.051658 0.042947 0.148506 -0.196648 1.000000 0.093595 0.251397   alcohol -0.061668 -0.202288 0.109903 0.042075 -0.221141 -0.069408 -0.205654 -0.496180 0.205633 0.093595 1.000000 0.476166   quality 0.124052 -0.390558 0.226373 0.013732 -0.128907 -0.050656 -0.185100 -0.174919 -0.057731 0.251397 0.476166 1.000000     plt.figure(figsize=(15,10)) sns.heatmap(red.corr(), annot=True, cmap='coolwarm') plt.show() Existen correlaciones positivas entre las características:\n fixed acidity con citric acid y densidad, free sulfur dioxide con total sulfur dioxide, alcohol con quality  y correlaciones negativas entre las caracteríticas:\n fixed acidity con pH, volatile acidity con citric acid, citric acid con pH, density con alcohol  Mostremos sólo las correlaciones de la variable objetivo con el resto de características:\ncorr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False) alcohol 0.476166 sulphates 0.251397 citric acid 0.226373 fixed acidity 0.124052 residual sugar 0.013732 free sulfur dioxide -0.050656 pH -0.057731 chlorides -0.128907 density -0.174919 total sulfur dioxide -0.185100 volatile acidity -0.390558 Name: quality, dtype: float64  plt.figure(figsize=(8,5)) corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False).plot(kind='bar') plt.title(\"Attribute correlations with quality\") plt.show() Podemos observar una correlación positiva con el atributo alcohol y negativa con volatile acidity.\nPreparación de los datos En primer lugar vamos a crear el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = red.columns[:-1] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = red[predict_columns] y = red[\"quality\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) X_train.shape, y_train.shape ((1279, 11), (1279,))  X_test.shape, y_test.shape ((320, 11), (320,))  Línea base Para determinar adecuadamente si nuestro modelo es mejor o peor, primero tenemos que definir una línea base con la que poder comparar. Para ello vamos a entrenar algunos regresores dummy cuyos resultados usaremos como línea base de comparación.\nEste regresor dummy predice de manera constante la puntuación 5, la más frecuente:\nrg_dummy = DummyRegressor(strategy=\"constant\", constant=5) rg_dummy.fit(X_train, y_train) DummyRegressor(constant=array(5), strategy='constant')  Nos creamos una función que nos permitirá evaluar nuestro modelo a lo largo de este análisis:\ndef evaluate_model(estimator, X_train, y_train, cv=10, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"] scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring) val_mae_mean, val_mae_std = -scores['test_neg_mean_absolute_error'].mean(), \\ -scores['test_neg_mean_absolute_error'].std() train_mae_mean, train_mae_std = -scores['train_neg_mean_absolute_error'].mean(), \\ -scores['train_neg_mean_absolute_error'].std() val_mse_mean, val_mse_std = -scores['test_neg_mean_squared_error'].mean(), \\ -scores['test_neg_mean_squared_error'].std() train_mse_mean, train_mse_std = -scores['train_neg_mean_squared_error'].mean(), \\ -scores['train_neg_mean_squared_error'].std() val_rmse_mean, val_rmse_std = np.sqrt(-scores['test_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['test_neg_mean_squared_error']).std() train_rmse_mean, train_rmse_std = np.sqrt(-scores['train_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['train_neg_mean_squared_error']).std() val_r2_mean, val_r2_std = scores['test_r2'].mean(), scores['test_r2'].std() train_r2_mean, train_r2_std = scores['train_r2'].mean(), scores['train_r2'].std() result = { \"Val MAE\": val_mae_mean, \"Val MAE std\": val_mae_std, \"Train MAE\": train_mae_mean, \"Train MAE std\": train_mae_std, \"Val MSE\": val_mse_mean, \"Val MSE std\": val_mse_std, \"Train MSE\": train_mse_mean, \"Train MSE std\": train_mse_std, \"Val RMSE\": val_rmse_mean, \"Val RMSE std\": val_rmse_std, \"Train RMSE\": train_rmse_mean, \"Train RMSE std\": train_rmse_std, \"Val R2\": val_r2_mean, \"Val R2 std\": val_r2_std, \"Train R2\": train_rmse_mean, \"Train R2 std\": train_r2_std, } if verbose: print(f\"val_MAE_mean: {val_mae_mean} - (std: {val_mae_std})\") print(f\"train_MAE_mean: {train_mae_mean} - (std: {train_mae_std})\") print(f\"val_MSE_mean: {val_mse_mean} - (std: {val_mse_std})\") print(f\"train_MSE_mean: {train_mse_mean} - (std: {train_mse_std})\") print(f\"val_RMSE_mean: {val_rmse_mean} - (std: {val_rmse_std})\") print(f\"train_RMSE_mean: {train_rmse_mean} - (std: {train_rmse_std})\") print(f\"val_R2_mean: {val_r2_mean} - (std: {val_r2_std})\") print(f\"train_R2_mean: {train_r2_mean} - (std: {train_r2_std})\") return result rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.719365157480315 - (std: -0.06352462970037416) train_MAE_mean: 0.7193126146346173 - (std: -0.007057414168822716) val_MSE_mean: 1.0398868110236221 - (std: -0.12176257291946108) train_MSE_mean: 1.0398750482672072 - (std: -0.01354074583910719) val_RMSE_mean: 1.0180017820772593 - (std: 0.05965888627141756) train_RMSE_mean: 1.0197209977802941 - (std: 0.006643414270421584) val_R2_mean: -0.6192850555554466 - (std: 0.14799333040101653) train_R2_mean: -0.5986022943608599 - (std: 0.01598456942915052)  Un regresor que siempre predice la puntuación de calidad más frecuente (en nuestro caso, la puntuación 5) obtiene un RMSE = 1.01.\nProbemos ahora con un regresor dummy que predice la media de las puntuaciones de calidad:\nrg_dummy = DummyRegressor(strategy=\"mean\") # Mean prediction rg_dummy.fit(X_train, y_train) DummyRegressor()  rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.6842639509806605 - (std: -0.039939453843720794) train_MAE_mean: 0.6836374055181736 - (std: -0.004461928774514038) val_MSE_mean: 0.6515564887161005 - (std: -0.08938937463665708) train_MSE_mean: 0.6505431870574859 - (std: -0.009928873673332832) val_RMSE_mean: 0.8052590895459458 - (std: 0.05580580095057208) train_RMSE_mean: 0.8065390950374436 - (std: 0.006154285796714715) val_R2_mean: -0.007632943779434287 - (std: 0.010684535533448955) train_R2_mean: 0.0 - (std: 0.0)  Un regresor que predice siempre la puntuación media de calidad obtiene un RMSE = 0.80. Vamos a tomar la predicción de este regresor dummy como nuestra línea base.\nEntrenamiento de diversos modelos OK, ya estamos listos para entrenar varios modelos de forma rápida de diferente tipología y usando los parámetros estándar. Seleccionamos algunos modelos de regresión: Linear Regression, Lasso, ElasticNet, Ridge, Extre Trees, y RandomForest.\nmodels = [LinearRegression(), Lasso(alpha=0.1), ElasticNet(), Ridge(), ExtraTreesRegressor(), RandomForestRegressor()] model_names = [\"Lineal Regression\", \"Lasso\", \"ElasticNet\", \"Ridge\", \"Extra Tree\", \"Random Forest\"] mae = [] mse = [] rmse = [] r2 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") rg_scores = evaluate_model(models[model], X_train, y_train) mae.append(rg_scores[\"Val MAE\"]) mse.append(rg_scores[\"Val MSE\"]) rmse.append(rg_scores[\"Val RMSE\"]) r2.append(rg_scores[\"Val R2\"]) Paso 1 de 6 ...running Lineal Regression val_MAE_mean: 0.5054157041773433 - (std: -0.046264972549372924) train_MAE_mean: 0.49951141240221786 - (std: -0.005396834677886112) val_MSE_mean: 0.4363366846653876 - (std: -0.0713599197838867) train_MSE_mean: 0.423559916011364 - (std: -0.007783364048942027) val_RMSE_mean: 0.6578988186927084 - (std: 0.059210041615646476) train_RMSE_mean: 0.6507877560250832 - (std: 0.005934022177307515) val_R2_mean: 0.32302131635332426 - (std: 0.0972958323285871) train_R2_mean: 0.34888336017832816 - (std: 0.008988207786517072) Paso 2 de 6 ...running Lasso val_MAE_mean: 0.5542159398138832 - (std: -0.044044881537899525) train_MAE_mean: 0.551926769360105 - (std: -0.005222359881914205) val_MSE_mean: 0.5011613158962728 - (std: -0.07980261731926688) train_MSE_mean: 0.49648903729654775 - (std: -0.00886434349442919) val_RMSE_mean: 0.7054560563903938 - (std: 0.05910218607112876) train_RMSE_mean: 0.7045920060170998 - (std: 0.006256385006291075) val_R2_mean: 0.22550457016915199 - (std: 0.06858817248045986) train_R2_mean: 0.23679715721911138 - (std: 0.008061051196907644) Paso 3 de 6 ...running ElasticNet val_MAE_mean: 0.6484828644185054 - (std: -0.03858618665902155) train_MAE_mean: 0.6472074434172257 - (std: -0.004861676284701619) val_MSE_mean: 0.6260699925252777 - (std: -0.08837053843631361) train_MSE_mean: 0.6236958050351286 - (std: -0.009753039023728842) val_RMSE_mean: 0.7891968495348196 - (std: 0.056906284447264595) train_RMSE_mean: 0.7897200517246066 - (std: 0.0061680579774354895) val_R2_mean: 0.032300440343033296 - (std: 0.027013749786509673) train_R2_mean: 0.041268269123349036 - (std: 0.0034334107542665303) Paso 4 de 6 ...running Ridge val_MAE_mean: 0.5052017417711606 - (std: -0.04639189777979148) train_MAE_mean: 0.5000120146851917 - (std: -0.00538293390792397) val_MSE_mean: 0.4353611411950837 - (std: -0.07150445371257734) train_MSE_mean: 0.4243933932521361 - (std: -0.007774091981744382) val_RMSE_mean: 0.6571341500690723 - (std: 0.05946301378236467) train_RMSE_mean: 0.6514279204128516 - (std: 0.0059209592739344254) val_R2_mean: 0.32476443307512515 - (std: 0.09605257129964452) train_R2_mean: 0.3476024511130947 - (std: 0.0089301257345918) Paso 5 de 6 ...running Extra Tree val_MAE_mean: 0.3767233021653543 - (std: -0.048411131876621855) train_MAE_mean: -0.0 - (std: -0.0) val_MSE_mean: 0.33849758981299216 - (std: -0.07037684927470149) train_MSE_mean: -0.0 - (std: -0.0) val_RMSE_mean: 0.5784725891678845 - (std: 0.062185636560190514) train_RMSE_mean: 0.0 - (std: 0.0) val_R2_mean: 0.4753582472917177 - (std: 0.09435328966382882) train_R2_mean: 1.0 - (std: 0.0) Paso 6 de 6 ...running Random Forest val_MAE_mean: 0.421939406988189 - (std: -0.03848180259232641) train_MAE_mean: 0.15720688154624 - (std: -0.0024955091475250693) val_MSE_mean: 0.3536394728100393 - (std: -0.06315688035394738) train_MSE_mean: 0.049982221460505356 - (std: -0.0012897300801719821) val_RMSE_mean: 0.5921558699969636 - (std: 0.05468910712544724) train_RMSE_mean: 0.22354850027354933 - (std: 0.0028791467403151403) val_R2_mean: 0.450229047801475 - (std: 0.08970981370698214) train_R2_mean: 0.9231573754360927 - (std: 0.0020715859571618753)  Veamos cuál es el rendimiento de cada uno de ellos:\ndf_result = pd.DataFrame({\"Model\": model_names, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}) df_result  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model MAE MSE RMSE R2     0 Lineal Regression 0.505416 0.436337 0.657899 0.323021   1 Lasso 0.554216 0.501161 0.705456 0.225505   2 ElasticNet 0.648483 0.626070 0.789197 0.032300   3 Ridge 0.505202 0.435361 0.657134 0.324764   4 Extra Tree 0.375012 0.335985 0.576599 0.479648   5 Random Forest 0.422140 0.356897 0.594764 0.445618     df_result.sort_values(by=\"RMSE\", ascending=False).plot.barh(\"Model\", \"RMSE\"); df_result.sort_values(by=\"R2\").plot.barh(\"Model\", \"R2\"); Analizando los resultados vemos que extra trees es el modelo que mejores resultados obtiene. RMSE = 0.576599 and R2 = 0.479648. OK, este será nuestro modelo candidato. Vamos a realizar el ajuste fino.\nFine-Tune param_grid = [ {'n_estimators': range(10, 300, 10), 'max_features': [2, 3, 4, 5, 8, \"auto\"], 'bootstrap': [True, False]} ] xtree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1) grid_search = GridSearchCV(xtree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(X_train, y_train) GridSearchCV(cv=5, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=42), param_grid=[{'bootstrap': [True, False], 'max_features': [2, 3, 4, 5, 8, 'auto'], 'n_estimators': range(10, 300, 10)}], return_train_score=True, scoring='neg_mean_squared_error')  grid_search.best_params_ {'bootstrap': False, 'max_features': 5, 'n_estimators': 160}  ¡Es el momento de la verdad! Veamos su rendimiento en el conjunto de prueba:\nfinal_model = grid_search.best_estimator_ y_pred = final_model.predict(X_test) print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\") print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\") print(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\") print(f\"R2: {final_model.score(X_test, y_test)}\") MAE: 0.38298828124999995 MSE: 0.28038391113281247 RMSE: 0.5295128998738486 R2: 0.5709542506612473  Bueno, ¡un poco mejor! Obtenemos un error de +/- 0.5295.\nPodemos visualizar cómo han sido sus predicciones:\nplt.figure(figsize=(10,8)) plt.scatter(y_test, y_pred, alpha=0.1) plt.xlabel(\"Real\") plt.ylabel(\"Predicted\") plt.show() Se observa una mayor concentración de predicciones en las puntuaciones centrales (5 y 6), debido a un mayor número de instancias en el dataset respecto a las demás. También podemos comprobar que las predicciones sobre las puntuaciones extremas son pésimas. Las puntuaciones 5 y 6 son las que mejores resultados ofrecen.\n¿Cuáles son las características más relevantes?:\nfeature_importances = final_model.feature_importances_ feature_importances array([0.06242878, 0.12054219, 0.07478461, 0.06697772, 0.06670251, 0.05944129, 0.07925392, 0.07148382, 0.06178626, 0.12593217, 0.21066673])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.2106667292131454, 'alcohol'), (0.12593217102849735, 'sulphates'), (0.1205421943281732, 'volatile acidity'), (0.07925392046422035, 'total sulfur dioxide'), (0.07478461494308856, 'citric acid'), (0.07148382305429932, 'density'), (0.06697771630809285, 'residual sugar'), (0.06670250522733821, 'chlorides'), (0.062428775664599805, 'fixed acidity'), (0.061786258397281676, 'pH'), (0.05944129137126337, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances') La gráfica nos muestra que las características más importantes son: alcohol, sulphates y volatile acidity, algo que también nos anticipaba el análisis de correlaciones que vimos anteriormente.\nVeamos ahora cómo se distribuyen los errores:\ndf_resul = pd.DataFrame({\"Pred\": y_pred, \"Real\": y_test, \"error\": y_pred - y_test, \"error_abs\": abs(y_pred - y_test)}) df_resul[\"error\"].plot.hist(bins=40, density=True) plt.title(\"Error distribution\") plt.xlabel(\"Error\"); Parece que los errores siguen una distribución gaussiana.\n¿Cuál es el MAE que se produce en la puntuación de calidad 6?\ndf_resul[df_resul[\"Real\"].isin([6])][\"error\"].abs().mean() 0.3437013037105609  Más en general ¿Cuál es el MAE que se produce en cada puntuación de calidad?\ndf_resul.groupby(\"Real\")[\"error_abs\"].mean() Real 3 2.268966 4 1.286657 5 0.462774 6 0.343701 7 0.617315 8 1.597190 9 3.434483 Name: error_abs, dtype: float64  df_resul.groupby(\"Real\")[\"error_abs\"].mean().plot.bar() plt.title(\"MAE distribution\") plt.ylabel(\"MAE\") plt.xlabel(\"Quality\"); Se comprueba que en las puntuaciones de calidad extremas el error es elevado, sobre todo en la puntuación 8 y 3. Las puntuaciones 5 y 6 es donde menos error se produce.\nGuardado del modelo Como paso final, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model.joblib\", compress=True) ['final_model.joblib']  Conclusiones Después de probar diversos modelos, el que mejores resultados arroja es ExtraTrees. Tras un ajuste fino del mismo conseguimos una importante mejora:\n Nuestra línea base teníamos un MAE: 0.684263 y RMSE: 0.805259. El modelo de Extra Tree obtiene un MAE: 0.382988, RMSE: 0.529512 y R2:0.570954.  Sin embargo, la puntuación de R2 sigue siendo muy baja. Según dicho valor, nuestro modelo apenas puede explicar un 57% de la varianza. Es decir, el porcentaje de relación entre las variables que puede explicarse mediante nuestro modelo lineal es del 57,09%.\n Como sabemos, R2 varía entre 0 y 1. Es la proporción de la varianza en la variable dependiente (nuestra variable objetivo) que es predecible a partir de las variables independientes (nuestros predictores). Si la predicción fuera exactamente igual a lo real, R2 = 1 (es decir, 100%).\n El RMSE = 0,529. Es decir, tenemos un error típico de predicción de 0,529.\nSegún la gráfica de distribución de MAE podemos observar que nuestro modelo no es nada bueno para valores extremos de puntuación. De hecho no es capaz de predecir ninguna puntuación de 3 y apenas alguna de 8. Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nComo ejercicio final, podríamos probar a enfocar el modelado como un problema de clasificación, para evaluar si ofrece mejores resultados que un problema de regresión. Lo veremos en futuros posts.\n","description":"","tags":["regresión","ExtraTrees"],"title":"Calidad del vino - Un problema de regresión","uri":"/posts/wine-quality-un-problema-de-regresion/"},{"categories":["taxonomía"],"content":"Lee Sedol tenía solo 12 años cuando se convirtió en uno de los jugadores profesionales de Go más jóvenes de la historia. Cuando el 9 de marzo de 2016 cruzó las puertas del Hotel Four Seasons de Seúl tenía 33 años y era 18 veces campeón del mundo. Le esperaban cinco intensas partidas contra un duro contrincante. Ante el asombro general perdió 4-1. Ese día pasaría a la historia como el día en que el campeón del mundo de Go perdió contra AlphaGo, un programa informático perteneciente a la división DeepMind de Google. Lee Sedol también pasaría a la historia como el único humano que ha ganado una partida a AlphaGo (aunque posteriormente reconocería que fue debido a un error en su programa).\nGran parte de la magia negra de AlphaGo proviene del uso de técnicas y sistemas de Machine Learning e Inteligencia Artificial. Los sistemas de Machine learning (ML) o aprendizaje automático, están detrás de muchos de los productos de alta tecnología que nos rodean, de los motores de búsqueda de webs, del reconocimiento de habla de nuestros dispositivos, nos recomienda películas y series en nuestras plataformas de streaming favoritas, detecta el spam de nuestros correos, etc.\nPero ¿qué es machine Learning y qué significa que una máquina pueda aprender algo? Según la definición académica de Arthur Samuel, que popularizó dicho término en 1959, machine Learning es el campo de estudio que proporciona a los ordenadores la habilidad de aprender sin ser explícitamente programados. Como informáticos que somos, aquí va otra definición más “ingenieril”: Un programa de ordenador se dice que aprende de una experiencia E con respecto a alguna tarea T y alguna medida de la ejecución P, si su ejecución en T, medida por P, mejora con la experiencia E. (Tom Mitchell, 1997)\nEn vista de esto ¿si nos descargamos una copia de Wikipedia o la Hemeroteca Digital, nuestros ordenadores están aprendiendo algo? Evidentemente no. Dispondremos de una cantidad enorme de datos, pero de repente nuestras máquinas no serán mejores en ninguna tarea.\n¿Qué ventajas ofrece el uso de machine learning sobre otras técnicas de programación tradicionales? Utilizando el caso de uso del spam de correo que mencionamos anteriormente, con un enfoque tradicional haríamos lo siguiente:\n  Observaríamos que en los correos de spam aparecen palabras del tipo “para ti”, “gratis”, “increíble”, etc.\n  Codificaríamos un procedimiento que detectara estas palabras y etiquetaríamos como spam aquellos correos que contuvieran estos patrones.\n  Iteraríamos tantas veces por los dos pasos anteriores para codificar tantas reglas como patrones detectemos.\n  Un enfoque basado en técnicas de machine learning se centraría en aprender qué palabras o frases aparecen con mayor frecuencia en correos etiquetados como spam en comparación con correos “buenos”. Es lo que se denomina “entrenar” nuestro modelo, con el objetivo de que pueda clasificar los nuevos correos que nos lleguen.\nAdemás, supongamos que nuestro inteligente spammer compulsivo detecta que le bloqueamos aquellos correos donde aparece la palabra “gratis” y empieza a sustituirla por la palabra “gratuito”, y así sucesivamente cambiando las reglas. Un enfoque tradicional nos obligaría a estar constantemente cambiando nuestros patrones de detección y haciendo re-entregas. Un enfoque basado en ML detectaría automáticamente estos patrones inusualmente frecuentes en los correos marcados como spam y los marcaría en el futuro sin intervención humana.\nOtro campo donde realmente brilla machine learning es en el reconocimiento de escritura manual (o del habla). Podríamos escribir un programa que detectara determinados trazos o incluso el alfabeto completo, pero esto no escalaría a los miles de combinaciones escritas por millones de personas en el mundo. La mejor forma sería entrenar un modelo de ML proporcionándole muchos ejemplos de diferentes tipos de letras y patrones escritos a mano.\nComo vemos, machine learning es ideal para procesos donde tengamos mucho ajuste manual o un gran número de reglas, soluciones donde haya que adaptarse a nuevos datos, tratamiento de información no estructurada (sonidos, imágenes) y un largo etcétera de casos de uso.\nClasificación de los sistemas de machine learning Existen formas muy diversas de clasificar los sistemas de machine learning. Las más comunes serían las siguientes:\n  Si son entrenados con supervisión humana se pueden clasificar en: supervisados, no supervisados, semisupervisados y aprendizaje por reforzamiento.\n  Si pueden aprender incrementalmente al vuelo: aprendizaje online y aprendizaje por lotes.\n  Aprendizaje basado en instancia (donde los sistemas aprenden ejemplos “de memoria” y después generalizan a nuevos ejemplos usando medidas de similitud) vs aprendizaje basado en modelo (el sistema crea un modelo a partir de ejemplos de entrenamiento que usará posteriormente para realizar predicciones).\n  Esta tipología no es excluyente. Nuestro sistema de spam podría ser un ejemplo de aprendizaje supervisado online basado en modelo si lo entrenamos con una red neuronal.\nVeamos un poco más cerca nuestra primera categorización. Una mañana cualquiera nos acercamos a nuestro “banco amigo” a pedir un préstamo para montar nuestro soñado puesto de castañas. Después de rellenar varios formularios con datos de todo tipo, el director de la sucursal nos convoca para la semana siguiente, donde nos comunicará si nos concede dicho préstamo. ¿Cómo sabe el banco si devolveremos el préstamo? El banco tiene información de otros cientos de miles de operaciones similares a la nuestra y conoce si el cliente devolvió el préstamo o no (es decir, tiene datos etiquetados, aprendizaje supervisado). Con los datos que les hemos proporcionado y con sus modelos de clasificación, el banco puede predecir con un nivel de probabilidad en qué medida seremos capaces de devolver el préstamo. Queda a criterio del director de la sucursal si confiar ciegamente en lo que pronostican dichos modelos.\nEn el aprendizaje no supervisado no disponemos de datos etiquetados, por lo que el sistema debe aprender sin contar con un profesor. Los algoritmos no supervisados son muy útiles para detectar relaciones o agrupaciones entre los datos, algo que a una persona le resultaría muy difícil detectar. Por ejemplo, los modelos detrás de las empresas de venta online pueden detectar que las personas que compran un determinado producto X también suelen comprar el producto Z, por lo que nos los suelen sugerir (“Tal vez le interese…”, “Otros clientes también compraron…”, etc.) durante el proceso de compra. Este tipo de algoritmos no supervisados también se usan para la detección de anomalías (muy útil en la prevención del fraude bancario o en la detección de defectos de fabricación). El sistema está entrenado con ejemplos normales, por lo que es capaz de determinar si una nueva instancia es o no una anomalía.\nAlgunos sistemas de clasificación de imágenes serían un ejemplo de aprendizaje semisupervisado: son capaces de detectar personas y probablemente determinará que la persona X aparece en el siguiente grupo de imágenes. Tan solo hay que ayudarle indicándole quién es esa persona para que a la siguiente ocasión sepa etiquetarla correctamente.\nPor último, el aprendizaje por reforzamiento es un tipo muy diferente a los anteriores. El sistema obtiene recompensas o penalizaciones en función de sus acciones. Debe aprender a partir de ellas, eligiendo cuál sería la mejor estrategia (denominada política) para obtener la mayor recompensa a lo largo del tiempo. AlphaGo sería un ejemplo de aprendizaje por refuerzo. Aprendió su política ganadora estudiando millones de partidas. Durante su combate con el campeón del mundo aplicó las políticas que había aprendido.\nEn posteriores artículos hablaremos también de algunos de los lenguajes más idóneos y la combinación de herramientas que tenemos a nuestra disposición para trabajar de forma inmediata en machine learning: Python, Jupyter Notebook, Scikit-Learn, Tensor-Flow, Keras, etc.\nRevisaremos cuáles son las fases principales de un proyecto típico de machine learning con ejemplos prácticos.\n¡Bienvenidos al mundo de machine learning!\nPor cierto, apenas 3 años después de su derrota por AlphaGo, Lee Sedol se retiró de las competiciones oficiales. Si tenéis oportunidad no dejéis de ver el documental AlphaGo – The movie, que narra la apasionante crónica del combate entre ambas “mentes”.\n","description":"","tags":["aprendizaje automático","machine learning"],"title":"Breve Introducción a Machine Learning","uri":"/posts/breve-introduccion-machine-learning/"}]
