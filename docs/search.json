[{"categories":["series temporales"],"content":"Estacionalidad ¿Qué es la estacionalidad? Se dice que una serie temporal exhibe estacionalidad siempre que haya un cambio regular y periodico en la media de la serie. Generalmente, los cambios estacionales siguen el reloj y el calendario. Son comunes las repeticiones en un día, una semana o un año. La estacionalidad suele estar dirigida por los ciclos del mundo natural durante días y años o por convenciones de comportamiento social en torno a fechas y horas.\nVeremos que hay dos tipos de variables que modelan la estacionalidad. El primer tipo, los indicadores, es mejor para una estacionalidad con pocas observaciones, como la estacionalidad semanal de observaciones diarias. El segundo tipo, variables Fourier, es mejor para una estacionalidad con muchas observaciones, como la estacionalidad anual de observaciones diarias.\nGráficas de estacionalidad e indicadores de estacionalidad Al igual que usamos una gráfica de media móvil para descubrir la tendencia en una serie, podemos usar una gráfica estacional para descubrir patrones estacionales.\nUna gráfica estacional muestra segmentos de la serie temporal representados en un periodo común, siento el periodo la “estación” que queremos observar. La siguiente figura muestra una gráfica estacional de las vistas diarias del artículo de Wikipedia sobre Trigonometría: las vistas diarias del artículo dibujadas sobre un periodo semanal común. En ella se puede observar claramente un patrón estacional semanal, más grande entre semana y descendente hacia el fin de semana.\nIndicadores estacionales Los indicadores estacionales son features binarias que representan diferencias estacionales en el nivel de una serie temporal. Los indicadores estacionales son lo que obtenemos si tratamos un periodo estacional como una variable categórica y le aplicamos codificación one-hot.\nAl codificar one-hot los días de la semana, obtenemos indicadores estacionales semanales. La creación de indicadores semanales para la serie Trigonometría nos proporcionará seis nuevas features “dummy”. (La regresión lineal trabaja mejor si eliminamos unos de los indicadores; elegimos el lunes en la tabla siguiente.)\nAñadir los indicadores estacionales a los datos de entrenamiento ayuda a los modelos a distinguir medias dentro de un periodo estacional:\nLos indicadores actúan como interruptores encendido/apagado. En cualquier momento, como máximo uno de esos indicadores puede tener un valor de 1 (encendido). La regresión lineal aprende un valor de línea base de 2379 para Mon y luego lo ajusta según el valor de si el indicador está encendido para ese día; el resto son 0 y desaparecen.\nVariables Fourier y el Periodograma Este tipo de features es más apropiado para grandes estacionalidades en muchas observaciones, donde los indicadores serían impracticables. En lugar de crear una variable para cada fecha, las features Fourier intentar capturar la forma global de la curva estacional con solo unas pocas features.\nEchemos un vistazo al gráfico de la estacionalidad anual de Trigonometría. Observemos las repeticiones de varias frecuencias: un movimiento largo ascendente y descendente tres veces al año, movimientos cortos semanales 52 veces al año y tal vez otros.\nEstas son las frecuencias dentro de una estación que intentamos capturar con las features Fourier. La idea es incluir en nuestros datos de entrenamiento curvas periodicas que tengan las mismas frecuencias que la estación que estamos intentando modelar. Las curvas que utilizamos son las de las funciones seno y coseno.\nLas features Fourier son pares de curvas seno y coseno, un par para cada potencial frecuencia en la estación empezando con la más larga. Los pares de Fourier que modelan la estacionalidad anual tendrían frecuencias: una vez al año, dos veces por año, tres veces por año y así sucesivamente.\nEn la figura anterior se muestras los primeros dos pares Fourier para la estacionalidad anual. En la parte superior, frecuencia de una vez por año. En la parte inferior, frecuencia de dos veces por año.\nSi añadimos un conjunto de estas curvas seno/coseno a nuestros datos de entrenamiento, el algoritmo de regresión lineal averiguará los pesos que ajustarán el componente estacional en la serie objetivo. La siguiente figura ilustra cómo la regresión lineal usó cuatro pares Fourier para modelar la estacionalidad anual en la serie Trigonometría de la wiki.\nEn la parte superior, las curvas de cuatro pares, una suma de senos y cosenos con coeficientes de regresión. Cada curva modela una frecuencia diferente. En la parte inferior, la suma de esas curvas aproxima el patrón estacional.\nTengamos en cuenta que solo necesitamos ocho features (cuatro pares seno/coseno) para obtener una buena estimación de la estacionalidad anual. Comparemos esto con el método de indicadores estacionales, que habría requerido cientos de features (una por cada día del año). Al modelar solo el “efecto pincipal” de la estacionalidad con features Fourier, normalmente necesitaremos añadir muchas menos funciones a los datos de entrenamiento, lo que reduce el tiempo computacional y disminuye el riesgo de overfitting.\nElegir features Fourier con el Periodograma ¿Cuántos pares Fourier se deben añadir realmente en nuestro conjunto de features? Podemos responder esta pregunta con el periodograma. El periodograma nos dice la fortaleza de las frecuencias de un serie temporal. Específicamente, el valor del eje y del gráfico es (a ** 2 + b ** 2) / 2, donde a y b son los coeficientes del seno y coseno en esa frecuencia (como vimos en el grafico anterior).\nDe izquierda a derecha, el periodograma cae después de trimestral (quarterly), cuatro veces al año. Por eso elegimos cuatro pares Fourier para modelas la estacionalidad anual. La frecuencia semanal la ignoramos, dado que es mejor modelarla con indicadores.\nCalculo de las features Fourier Saber cómo se calculan las features Fourier no es esencial para saber usarlas, pero si se ven los detalles se pueden aclarar cosas. A continuación, se muestra cómo se pueden derivar un conjunto de features Fourier del índice de una serie temporal. Sin embargo, para nuestros ejemplos usaremos una función de statsmodels.\nimport numpy as np import pandas as pd def fourier_features(index, freq, order): time = np.arange(len(index), dtype=np.float32) k = 2 * np.pi * (1 / freq) * time features = {} for i in range(1, order + 1): features.update({ f\"sin_{freq}_{i}\": np.sin(i * k), f\"cos_{freq}_{i}\": np.cos(i * k), }) return pd.DataFrame(features, index=index) # Calcula features Fourier de 4º orden (8 nuevas features) para una # serie y con observaciones diarias y estacionalidad anual: # # fourier_features(y, freq=365.25, order=4) Ejemplo - Tráfico del Túnel Utilizaremos el dataset de Tráfico del Túnel. Vamos a definir 2 funciones: seasonal_plot y plot_periodogram.\nimport matplotlib.pyplot as plt import pandas as pd import seaborn as sns from sklearn.linear_model import LinearRegression from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess # Set Matplotlib defaults plt.style.use(\"seaborn-whitegrid\") plt.rc(\"figure\", autolayout=True, figsize=(11, 5)) plt.rc( \"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=16, titlepad=10, ) plot_params = dict( color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", legend=False, ) %config InlineBackend.figure_format = 'retina' def seasonal_plot(X, y, period, freq, ax=None): if ax is None: _, ax = plt.subplots() palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),) ax = sns.lineplot( x=freq, y=y, hue=period, data=X, ci=False, ax=ax, palette=palette, legend=False, ) ax.set_title(f\"Gráfico estacional ({period}/{freq})\") for line, name in zip(ax.lines, X[period].unique()): y_ = line.get_ydata()[-1] ax.annotate( name, xy=(1, y_), xytext=(6, 0), color=line.get_color(), xycoords=ax.get_yaxis_transform(), textcoords=\"offset points\", size=14, va=\"center\", ) return ax def plot_periodogram(ts, detrend='linear', ax=None): from scipy.signal import periodogram fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\") freqencies, spectrum = periodogram( ts, fs=fs, detrend=detrend, window=\"boxcar\", scaling='spectrum', ) if ax is None: _, ax = plt.subplots() ax.step(freqencies, spectrum, color=\"purple\") ax.set_xscale(\"log\") ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104]) ax.set_xticklabels( [ \"Annual (1)\", \"Semiannual (2)\", \"Quarterly (4)\", \"Bimonthly (6)\", \"Monthly (12)\", \"Biweekly (26)\", \"Weekly (52)\", \"Semiweekly (104)\", ], rotation=30, ) ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0)) ax.set_ylabel(\"Variance\") ax.set_title(\"Periodogram\") return ax tunnel = pd.read_csv(\"../data/tunnel.csv\", parse_dates=[\"Day\"]) tunnel = tunnel.set_index(\"Day\").to_period(\"D\") Echemos un vistazo al gráfico estacional durante una semana y durante un año.\nX = tunnel.copy() # días dentro de una semana X[\"day\"] = X.index.dayofweek # eje x (freq) X[\"week\"] = X.index.week # periodo estacional (period) # días dentro de un año X[\"dayofyear\"] = X.index.dayofyear X[\"year\"] = X.index.year fig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6)) seasonal_plot(X, y=\"NumVehicles\", period=\"week\", freq=\"day\", ax=ax0) seasonal_plot(X, y=\"NumVehicles\", period=\"year\", freq=\"dayofyear\", ax=ax1); Veamos ahora el periodograma:\nplot_periodogram(tunnel.NumVehicles); C:\\Users\\futit\\AppData\\Local\\Temp\\ipykernel_8772\\272637971.py:1: FutureWarning: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and will be removed in a future version. plot_periodogram(tunnel.NumVehicles);  El periodograma concuerda con el gráfico estacional anterior: una fuerte estacionalidad semanal y una estacionalidad anual más débil. La estacionalidad semanal la modelaremos con indicadores y la estacionalidad anual con features Fourier. De derecha a izquierda, el periodograma cae entre Semestral (6) y Mensual (12), así que usaremos 10 pares Fourier.\nCrearemos nuestras features estacionales usando DeterministicProcess, la misma utilidad usada para crear la features de tendencia. Para usar dos periodos estacionales (semanal y anual) necesitaremos instanciar uno de ellos como un “término adicional”:\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess fourier = CalendarFourier(freq=\"A\", order=10) # 10 pares seno/coseno para estacionalidad \"A\"nual dp = DeterministicProcess( index=tunnel.index, constant=True, # feature dummy para bias (y-intercept) order=1, # tendencia (orden 1 significa lineal) seasonal=True, # estacionalidad semanal (indicadores) additional_terms=[fourier], # estacionalidad anual (fourier) drop=True, # eliminamos términos para evitar colinealidad ) X = dp.in_sample() # crear features para fecha en tunnel.index X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  const trend s(2,7) s(3,7) s(4,7) s(5,7) s(6,7) s(7,7) sin(1,freq=A-DEC) cos(1,freq=A-DEC) ... sin(6,freq=A-DEC) cos(6,freq=A-DEC) sin(7,freq=A-DEC) cos(7,freq=A-DEC) sin(8,freq=A-DEC) cos(8,freq=A-DEC) sin(9,freq=A-DEC) cos(9,freq=A-DEC) sin(10,freq=A-DEC) cos(10,freq=A-DEC)   Day                          2003-11-01 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.867456 0.497513 ... -0.017213 0.999852 -0.875892 0.482508 -0.854322 -0.519744 0.025818 -0.999667 0.880012 -0.474951   2003-11-02 1.0 2.0 1.0 0.0 0.0 0.0 0.0 0.0 -0.858764 0.512371 ... 0.085965 0.996298 -0.811539 0.584298 -0.917584 -0.397543 -0.128748 -0.991677 0.785650 -0.618671   2003-11-03 1.0 3.0 0.0 1.0 0.0 0.0 0.0 0.0 -0.849817 0.527078 ... 0.188227 0.982126 -0.735417 0.677615 -0.963471 -0.267814 -0.280231 -0.959933 0.668064 -0.744104   2003-11-04 1.0 4.0 0.0 0.0 1.0 0.0 0.0 0.0 -0.840618 0.541628 ... 0.288482 0.957485 -0.648630 0.761104 -0.991114 -0.133015 -0.425000 -0.905193 0.530730 -0.847541   2003-11-05 1.0 5.0 0.0 0.0 0.0 1.0 0.0 0.0 -0.831171 0.556017 ... 0.385663 0.922640 -0.552435 0.833556 -0.999991 0.004304 -0.559589 -0.828770 0.377708 -0.925925    5 rows × 28 columns\n Con nuestro conjunto de features creado, estamos listos para entrenar el modelo y hacer predicciones. Añadiremos un pronóstico de 90 días para ver cómo extrapola nuestro modelo más allá de los datos de entrenamiento.\ny = tunnel[\"NumVehicles\"] model = LinearRegression(fit_intercept=False) _ = model.fit(X, y) y_pred = pd.Series(model.predict(X), index=y.index) X_fore = dp.out_of_sample(steps=90) y_fore = pd.Series(model.predict(X_fore), index=X_fore.index) ax = y.plot(color='0.25', style='.', title=\"Tráfico del Túnel - Pronóstico estacionalidad\") ax = y_pred.plot(ax=ax, label=\"Estacional\") ax = y_fore.plot(ax=ax, label=\"Pronóstico estacional\", color='C3') _ = ax.legend() Ejercicio Vamos a realizar un ejercicio para ampliar lo que acabamos de ver. Para ello cargaremos algunos datasets.\nstore_sales = pd.read_csv( \"../data/store_sales/train.csv\", usecols=['store_nbr', 'family', 'date', 'sales'], dtype={ 'store_nbr': 'category', 'family': 'category', 'sales': 'float32', }, parse_dates=['date'], infer_datetime_format=True, ) store_sales['date'] = store_sales.date.dt.to_period('D') store_sales = store_sales.set_index(['store_nbr', 'family', 'date']).sort_index() average_sales = ( store_sales .groupby('date').mean() .squeeze() .loc['2017'] ) average_sales.head() date 2017-01-01 6.780303 2017-01-02 786.928406 2017-01-03 619.740234 2017-01-04 555.608032 2017-01-05 436.375397 Freq: D, Name: sales, dtype: float32  Determinar la estacionalidad Examinemos el siguiente gráfico estacional:\nX = average_sales.to_frame() X[\"week\"] = X.index.week X[\"day\"] = X.index.dayofweek seasonal_plot(X, y='sales', period='week', freq='day'); y el periodograma:\nplot_periodogram(average_sales); C:\\Users\\futit\\AppData\\Local\\Temp\\ipykernel_8772\\69537304.py:1: FutureWarning: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and will be removed in a future version. plot_periodogram(average_sales);  ¿De qué tipo de estacionalidad se ve evidencia?\nAmbos gráficos sugieren una fuerte estacionalidad semanal. Del periodograma parece que también puede haber algunos componentes mensuales y bisemanales (quincenales) De hecho, las notas del dataset dicen que los salarios en el sector público se pagan quincenalmente, el 15 y último día de cada mes (un posible origen para esta estacionalidad).\nCrear features estacionales Usa DeterministicProcess y CalendarFourier para crear:\n  indicadores para estaciones semanales y\n  features Fourier de orden 4 para estaciones mensuales\n  y = average_sales.copy() # YOUR CODE HERE fourier = CalendarFourier(freq=\"M\", order=4) # 4 pares seno/coseno para estacionalidad \"M\"ensual dp = DeterministicProcess( index=y.index, constant=True, order=1, seasonal=True, additional_terms=[fourier], drop=True, ) X = dp.in_sample() Ahora vamos a entrenar el modelo estacional.\nmodel = LinearRegression().fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) ax = y.plot(**plot_params, alpha=0.5, title=\"Ventas promedio\", ylabel=\"artículos vendidos\") ax = y_pred.plot(ax=ax, label=\"Estacional\") ax.legend(); Comprobar la estacionalidad restante Eliminar de una serie su tendencia o estacionalidad se denomina “detrending” o “desestacionalizar” la serie. Observemos el periodograma de la serie desestacionalizada.\ny_deseason = y - y_pred fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7)) ax1 = plot_periodogram(y, ax=ax1) ax1.set_title(\"Componentes de Frecuenca de Venta de Productos\") ax2 = plot_periodogram(y_deseason, ax=ax2) ax2.set_title(\"Desestacionalizado\"); C:\\Users\\futit\\AppData\\Local\\Temp\\ipykernel_8772\\3088312804.py:4: FutureWarning: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and will be removed in a future version. ax1 = plot_periodogram(y, ax=ax1) C:\\Users\\futit\\AppData\\Local\\Temp\\ipykernel_8772\\3088312804.py:6: FutureWarning: Units 'M', 'Y' and 'y' do not represent unambiguous timedelta values and will be removed in a future version. ax2 = plot_periodogram(y_deseason, ax=ax2)  Basándonos en estos periodogramas, ¿cómo de efectivo parece que nuestro modelo capturó la estacionalidad? ¿El periodograma concuerda con la gráfica de tiempo de la serie desestacionalizada?\nEl periodograma de la serie desestacionalizada carece de valores grandes. Al compararlo con el periodograma de la serie original podemos ver que nuestro modelo fue capaz de capturar la variación estacional en el dataset.\nCrear features de vacaciones En el dataset de ventas se incluye una tabla de fiestas.\nholidays_events = pd.read_csv( \"../data/store_sales/holidays_events.csv\", dtype={ 'type': 'category', 'locale': 'category', 'locale_name': 'category', 'description': 'category', 'transferred': 'bool', }, parse_dates=[\"date\"], infer_datetime_format=True, ) holidays_events = holidays_events.set_index(\"date\").to_period(\"D\") holidays_events.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  type locale locale_name description transferred   date          2012-03-02 Holiday Local Manta Fundacion de Manta False   2012-04-01 Holiday Regional Cotopaxi Provincializacion de Cotopaxi False   2012-04-12 Holiday Local Cuenca Fundacion de Cuenca False   2012-04-14 Holiday Local Libertad Cantonizacion de Libertad False   2012-04-21 Holiday Local Riobamba Cantonizacion de Riobamba False     # Vacaciones nacionales y regionales en el conjunto de entrenamiento holidays = ( holidays_events .query(\"locale in ['National', 'Regional']\") .loc['2017':'2017-08-15', ['description']] .assign(description=lambda x: x.description.cat.remove_unused_categories()) ) holidays  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  description   date      2017-01-01 Primer dia del ano   2017-01-02 Traslado Primer dia del ano   2017-02-27 Carnaval   2017-02-28 Carnaval   2017-04-01 Provincializacion de Cotopaxi   2017-04-14 Viernes Santo   2017-05-01 Dia del Trabajo   2017-05-13 Dia de la Madre-1   2017-05-14 Dia de la Madre   2017-05-24 Batalla de Pichincha   2017-05-26 Traslado Batalla de Pichincha   2017-06-25 Provincializacion de Imbabura   2017-08-10 Primer Grito de Independencia   2017-08-11 Traslado Primer Grito de Independencia     A partir de un gráfico de ventas desestacionalizado, parece que estas vacaciones podrían tener algún poder predictivo.\nax = y_deseason.plot(**plot_params) plt.plot_date(holidays.index, y_deseason[holidays.index], color='C3') ax.set_title('Vacaciones nacionales y regionales'); ¿Qué tipo de features podríamos crear para ayudar a nuestro modelo a hacer uso de esta información? Scikit-learn y Pandas tienen utilidades que permitirían hacer esto de forma fácil.\n# solución Scikit-learn from sklearn.preprocessing import OneHotEncoder one_hot = OneHotEncoder(sparse=False) X_holidays = pd.DataFrame( one_hot.fit_transform(holidays), index=holidays.index, columns=holidays.description.unique(), ) X_holidays.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Primer dia del ano Traslado Primer dia del ano Carnaval Provincializacion de Cotopaxi Viernes Santo Dia del Trabajo Dia de la Madre-1 Dia de la Madre Batalla de Pichincha Traslado Batalla de Pichincha Provincializacion de Imbabura Primer Grito de Independencia Traslado Primer Grito de Independencia   date                  2017-01-01 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-01-02 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0   2017-02-27 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-02-28 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-04-01 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0     # solución Pandas X_holidays = pd.get_dummies(holidays) X_holidays.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  description_Batalla de Pichincha description_Carnaval description_Dia de la Madre description_Dia de la Madre-1 description_Dia del Trabajo description_Primer Grito de Independencia description_Primer dia del ano description_Provincializacion de Cotopaxi description_Provincializacion de Imbabura description_Traslado Batalla de Pichincha description_Traslado Primer Grito de Independencia description_Traslado Primer dia del ano description_Viernes Santo   date                  2017-01-01 0 0 0 0 0 0 1 0 0 0 0 0 0   2017-01-02 0 0 0 0 0 0 0 0 0 0 0 1 0   2017-02-27 0 1 0 0 0 0 0 0 0 0 0 0 0   2017-02-28 0 1 0 0 0 0 0 0 0 0 0 0 0   2017-04-01 0 0 0 0 0 0 0 1 0 0 0 0 0     # Unión a los datos de entrenamiento X2 = X.join(X_holidays, on='date').fillna(0.0) X2.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  const trend s(2,7) s(3,7) s(4,7) s(5,7) s(6,7) s(7,7) sin(1,freq=M) cos(1,freq=M) ... description_Dia de la Madre-1 description_Dia del Trabajo description_Primer Grito de Independencia description_Primer dia del ano description_Provincializacion de Cotopaxi description_Provincializacion de Imbabura description_Traslado Batalla de Pichincha description_Traslado Primer Grito de Independencia description_Traslado Primer dia del ano description_Viernes Santo   date                          2017-01-01 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.000000 1.000000 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-01-02 1.0 2.0 1.0 0.0 0.0 0.0 0.0 0.0 0.201299 0.979530 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0   2017-01-03 1.0 3.0 0.0 1.0 0.0 0.0 0.0 0.0 0.394356 0.918958 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-01-04 1.0 4.0 0.0 0.0 1.0 0.0 0.0 0.0 0.571268 0.820763 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   2017-01-05 1.0 5.0 0.0 0.0 0.0 1.0 0.0 0.0 0.724793 0.688967 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0    5 rows × 29 columns\n Vamos a entrenar el modelo estacional con las features de vacaciones añadidas. ¿Los valores ajustados parecen haber mejorado?\nmodel = LinearRegression().fit(X2, y) y_pred = pd.Series(model.predict(X2), index=X2.index) ax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\") ax = y_pred.plot(ax=ax, label=\"Seasonal\") ax.legend(); El modelo ha mejorado las predicciones en los días festivos.\n","description":"","tags":["series temporales","estacionalidad","indicadores estacionales","Fourier","Periodograma"],"title":"Series temporales: (3) Estacionalidad","uri":"/posts/time-series-seasonality/"},{"categories":["series temporales"],"content":"Tendencia ¿Qué es una tendencia? El componente tendencia de una serie temporal representa un cambio persistente a largo plazo en la media de la serie. La tendencia es la parte más lenta del movimiento de una serie, la parte que representa la mayor escala de tiempo de importancia. En una serie temporal de venta de productos, una tendencia creciente podría ser el efecto de una expansión del mercado a medida que más personas conocen el producto año tras año.\nAquí nos centraremos en tendencias en la media. Sin embargo, de manera más general, cualquier cambio persistente y de movimiento lento de una serie podría constituir una tendencia. Normalmente, las series temporales tienen tendencias en su variación, por ejemplo.\nGráficos de media móviles Para ver qué tipo de tendencia podría tener una serie temporal podemos usar un gráfico de média móvil. Para calcular una media móvil de una serie temporal, calculamos la media de los valores dentro de una ventana deslizante de un ancho definido. Cada punto del gráfico representa la media de todos los valores de la serie que se encuentran dentro de la ventada de cada lado. La idea es suavizar cualquier fluctuación a corto plazo en la serie para que solo permanezcan los cambios a largo plazo.\nEsta figura muestra una media móvil ilustrando una tendencia lineal. Cada punto de la curva (azul) es la media de los puntos (rojo) dentro de una ventana de tamaño 12.\nObservemos cómo la serie Mauna Loa tiene un movimiento ascendente y descendente año tras año, un cambio estacional a corto plazo. Para que un cambio sea parte de la tendencia debe ocurrir durante un periodo más largo que cualquier cambio estacional. Para visualizar una tendencia, por tanto, tomaremos un promedio durante un periodo más largo que cualquier periodo estacional de la serie. Para la serie Mauna Loa, elegimos una ventana de 12 para suavizar la estacionalidad dentro de cada año.\nIngeniería de tendencias Una vez que hemos identificado la forma de la tendencia, podemos intentar modelarla usando una variable de paso de tiempo. Ya vimos cómo el uso de una time dummy en sí misma modelará una tendencia lineal:\nobjetivo = a * time + b\nPodemos ajustar muchos otros tipos de tendencias a través de transformaciones de la time dummy. Si la tendencia parece ser cuadrática (una parábola) solo necesitamos añadir el cuadrado de la time dummy a la feature, lo que nos daría:\nobjetivo = a * time ** 2 + b * time + c\nLa regresión lineal aprenderá los coeficientes a, b y c.\nLas curvas de tendencia de la siguiente figura se han entrenado usando estos tipos de features y LinearRegression de scikit-learn:\nLa serie superior tendría una tendencia lineal. La serie inferior tendría una tendencia cuadrática.\nLa regresión lineal puede adaptarse a curvas que no sean líneas. La idea es que si podemos proporcionar curvas de la forma apropiada como features, entonces la regresión lineal puede aprender a combinarlas de la forma que mejor se adapte al objetivo.\nEjemplo - Tráfico del Túnel En este ejemplo, crearemos un modelo de tendencia para el dataset de Tráfico del Túnel.\nimport matplotlib.pyplot as plt # Set Matplotlib defaults plt.style.use(\"seaborn-whitegrid\") plt.rc(\"figure\", autolayout=True, figsize=(11, 5)) plt.rc( \"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10, ) plot_params = dict( color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", legend=False, ) %config InlineBackend.figure_format = 'retina' import pandas as pd tunnel = pd.read_csv(\"../data/tunnel.csv\", parse_dates=[\"Day\"]) tunnel = tunnel.set_index(\"Day\").to_period() tunnel.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  NumVehicles   Day      2003-11-01 103536   2003-11-02 92051   2003-11-03 100795   2003-11-04 102352   2003-11-05 106569     Hagamos una gráfica de media móvil para ver qué tipo de tendencia tiene esta serie. Dado que esta serie tiene observaciones diarias, elegiremos una ventada de 365 días para suavizar cualquier cambio a corto plazo dentro del año.\nPara crar un media móvil, primero usaremos el método rolling para empezar el cálculo en la ventana.\nmoving_average = tunnel.rolling( window=365, # ventana de 365 días center=True, # pone la media en el dentro de la ventana min_periods=183, # selecciona la mitad del tamaño de la ventana ).mean() # calcula la media (también podría hacerse la mediana, desviación, mínimo, máximo, ...) ax = tunnel.plot(style=\".\", color=\"0.5\") moving_average.plot( ax=ax, linewidth=3, title=\"Tráfico del Túnel - Media móvil 365 días\", legend=False, ); Como vemos, la tendencia parece ser lineal.\nAunque podemos crear directmente en Pandas nuestra time dummy, usaremos una función de la librería statsmodels llamada DeterministicProcess. Usar esta función nos ayudará a evitar algunos casos de fallos complicados que pueden surgir a veces con las series temporales y la regresión lineal. El argumento order se refiere al orden polinomial: 1 para lineal, 2 para cuadrático, 3 cúbico y así sucesivamente.\nfrom statsmodels.tsa.deterministic import DeterministicProcess dp = DeterministicProcess( index=tunnel.index, # fechas de los datos de entrenamiento constant=True, # variable dummy para el bias (y_intercept) order=1, # la time dummy (tendencia) drop=True, # elimina términos si es necesario para evitar colinearidad ) # crea features para las fechas dadas en el argumento `index` X = dp.in_sample() X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  const trend   Day       2003-11-01 1.0 1.0   2003-11-02 1.0 2.0   2003-11-03 1.0 3.0   2003-11-04 1.0 4.0   2003-11-05 1.0 5.0     Un proceso determinista es un término técnico para una serie temporal que no es aleatoria o está completamente determinada, como son const y trend. Las features derivadas del índice de tiempo serán generalmente deterministas.\nCreamos nuestro modelo de tendencia básicamente como antes, aunque tengamos en cuenta la adición del argumento fit_intercept=False.\nfrom sklearn.linear_model import LinearRegression y = tunnel[\"NumVehicles\"] # objetivo # intercept es lo mismo que la variable `const` de DeterministicProcess. # LinearRegression se comporta mal con features duplicadas, # así que necesitamos asegurarnos de excluirlas aquí. model = LinearRegression(fit_intercept=False) model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) La tendencia descubierta por nuestro modelo LinearRegression es casi idéntica a la gráfica de media móvil, lo que sugiere que una tendencia lineal fue la decisión correcta en este caso.\nax = tunnel.plot(style=\".\", color=\"0.5\", title=\"Tráfico del Túnel - Tendencia Lineal\") _ = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\") Para hacer un pronóstico, aplicamos nuestro modelo a features “externas”. En este caso, “externas” se refiere a fechas fuera del periodo de observación de los datos de entrenamiento. A continuación se muestra cómo sería una predicción a 30 días:\nX = dp.out_of_sample(steps=30) y_fore = pd.Series(model.predict(X), index=X.index) y_fore.head() 2005-11-17 114981.801146 2005-11-18 115004.298595 2005-11-19 115026.796045 2005-11-20 115049.293494 2005-11-21 115071.790944 Freq: D, dtype: float64  Dibujemos una porción de la serie para ver la tendencia del pronóstico para los siguientes 30 días:\nax = tunnel[\"2005-05\":].plot(title=\"Tráfico del Túnel - Pronóstico de tendencia lineal\", **plot_params) ax = y_pred[\"2005-05\":].plot(ax=ax, linewidth=3, label=\"Tendencia\") ax = y_fore.plot(ax=ax, linewidth=3, label=\"Tendencia pronóstico\", color=\"C3\") _ = ax.legend() Los modelos de tendencias que hemos visto son útiles por numerosas razones. Además de actuar como línea base o punto de partida para modelos más sofisticados, también podemos usarlos como un componente en un “modelo híbrido” con algoritmos que nos son capaces de aprender tendencias (como XGBoost y random forest).\nEjercicio Vamos a realizar un ejercicio para ampliar lo que acabamos de ver. Para ello cargaremos algunos datasets.\nEl dataset US Retail Sales contiene datos de ventas mensuales para una serie de industrias minoristas de EEUU.\nretail_sales = pd.read_csv( \"../data/us-retail-sales.csv\", parse_dates=[\"Month\"], index_col=\"Month\", ).to_period(\"D\") food_sales = retail_sales.loc[:, \"FoodAndBeverage\"] auto_sales = retail_sales.loc[:, \"Automobiles\"] food_sales.head() Month 1992-01-01 29589 1992-02-01 28570 1992-03-01 29682 1992-04-01 30228 1992-05-01 31677 Freq: D, Name: FoodAndBeverage, dtype: int64  Vamos a dibujar la serie de Alimentos y Bebidas.\nDeterminar la tendencia con un gráfico de media móvil ax = food_sales.plot(**plot_params) ax.set(title=\"Ventas de alimentos y bebidas en EEUU\", ylabel=\"Millones de dólares\"); Ahora haremos un gráfico de media móvil para estimar la tendencia de esta serie.\ntrend = food_sales.rolling( window=12, # ventana de 12 meses center=True, # pone la media en el dentro de la ventana min_periods=6, # selecciona la mitad del tamaño de la ventana ).mean() # Make a plot ax = food_sales.plot(**plot_params, alpha=0.5) ax = trend.plot(ax=ax, linewidth=3) ax.set(title=\"Ventas de alimentos y bebidas en EEUU - Medía móvil 12 meses\", ylabel=\"Millones de dólares\"); Identificar la tendencia ¿Qué orden polinomial de la tendencia podría ser adecuado para la serie Alimentos y Bebidas? ¿Podríamos pensar en una curva no polinomial que funcionara mejor?\nLa curva ascendente de la tendencia sugiere que un polinomio de orden 2 sería el adecuado.\nSi se ha trabajado anteriormente con series temporales financieras, se puede suponer que la tasa de crecimiento de la serie Alimentos y Bebidas se expresaría mejor como un cambio porcentual. El cambio porcentual se suele modelar usando una curva exponencial.\nCrear una tendencia futura dtype = { 'store_nbr': 'category', 'family': 'category', 'sales': 'float32', 'onpromotion': 'uint64', } store_sales = pd.read_csv( \"../data/store_sales/train.csv\", dtype=dtype, parse_dates=[\"date\"], infer_datetime_format=True, ) store_sales = store_sales.set_index(\"date\").to_period(\"D\") store_sales = store_sales.set_index([\"store_nbr\", \"family\"], append=True) average_sales = store_sales.groupby(\"date\").mean()[\"sales\"] Vamos a dibujar una gráfica de media móvil de average_sales estimando la tendencia.\ntrend = average_sales.rolling( window=365, center=True, min_periods=183, ).mean() ax = average_sales.plot(**plot_params, alpha=0.5) ax = trend.plot(ax=ax, linewidth=3) Usa DeterministicProcess para crear un conjunto de features para un modelo de tendencia cúbico. Crea también features para un pronóstico de 90 días\nfrom statsmodels.tsa.deterministic import DeterministicProcess y = average_sales.copy() # el objetivo dp = DeterministicProcess( index=y.index, # fechas de los datos de entrenamiento order=3, # la time dummy (tendencia), cúbica ) X = dp.in_sample() X_fore = dp.out_of_sample(90) model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) y_fore = pd.Series(model.predict(X_fore), index=X_fore.index) ax = y.plot(**plot_params, alpha=0.5, title=\"Promedio de ventas\", ylabel=\"venta artículos\") ax = y_pred.plot(ax=ax, linewidth=3, label=\"Tendencia\", color='C0') ax = y_fore.plot(ax=ax, linewidth=3, label=\"Tendencia pronóticada\", color='C3') ax.legend(); Comprender el riesgo de pronosticar con polinomios de orden alto Una forma de adaptarse a tendencias más complicadas es incrementar el orden del polinomio usado. Para obtener un mejor ajuste a la tendencia algo complicada de este dataset, podríamos usar un polinomio de orden 11.\ndp = DeterministicProcess(index=y.index, order=11) X = dp.in_sample() model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) ax = y.plot(**plot_params, alpha=0.5, title=\"Promedio de ventas\", ylabel=\"venta artículos\") ax = y_pred.plot(ax=ax, linewidth=3, label=\"Tendencia\", color='C0') ax.legend(); Generalmente, los polinomios de orden alto no se adaptan bien a los pronósticos. Un polinomio de orden 11 incluirá términos como t ** 11. Términos como éste tienden a divergir rápidamente fuera del periodo de entrenamiento haciendo que los pronósticos sean poco confiables.\nPodemos confirmar esta intuición usando un polinomio de grado 11 para el mismo pronóstico que hicimos anteriormente de 90 días.\ny = average_sales.copy() # el objetivo dp = DeterministicProcess( index=y.index, # fechas de los datos de entrenamiento order=11, # la time dummy (tendencia), cúbica ) X = dp.in_sample() X_fore = dp.out_of_sample(90) model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) y_fore = pd.Series(model.predict(X_fore), index=X_fore.index) ax = y.plot(**plot_params, alpha=0.5, title=\"Promedio de ventas\", ylabel=\"venta artículos\") ax = y_pred.plot(ax=ax, linewidth=3, label=\"Tendencia\", color='C0') ax = y_fore.plot(ax=ax, linewidth=3, label=\"Tendencia pronóticada\", color='C3') ax.legend(); ","description":"","tags":["series temporales","tendencia","media móvil"],"title":"Series temporales: (2) Tendencia","uri":"/posts/time-series-trend/"},{"categories":["series temporales"],"content":"Regresión lineal con series temporales En este tipo de problemas, el objetivo básico de la predicción son las series temporales, que son un conjunto de observaciones registradas a lo largo del tiempo. En las aplicaciones de pronóstico, las observaciones se registran con una frecuencia regular, como puede ser diaria o mensualmente.\nimport pandas as pd df = pd.read_csv( \"../data/book_sales.csv\", index_col=\"Date\", parse_dates=[\"Date\"], ).drop(\"Paperback\", axis=1) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Hardcover   Date      2000-04-01 139   2000-04-02 128   2000-04-03 172   2000-04-04 139   2000-04-05 191     Esta serie registra el número de ventas de libros en una tienda durante 30 días. Por simplicidad, tiene una única columna de observaciones, Hardcover con un índice de tiempo Date.\nUsaremos el algoritmo de regresión lineal para construir modelos predictivos. Estos algoritmos aprenden cómo hacer una suma ponderada a partir de sus variables de entrada. Para dos variables tendríamos:\nobjetivo = peso_1 * feature_1 + peso_2 + feature_2 + bias\nDurante el entrenamiento, el algoritmo de regresión aprende los valores para los parámetros peso_1, peso_2 y bias que mejor se ajustan al objetivo. A este algoritmo se le suele llamar mínimos cuadrados ordinarios ya que elige valores que minimizan el error cuadrático entre el objetivo y las predicciones. Los pesos también se denominan coeficientes de regresión y al bias también se le llama intercept porque nos dice dónde cruza el eje y la grafica de esta función.\nFeatures de paso de tiempo Existen dos tipo de features únicas y distintivas de las series temporales: las variables de paso de tiempo (time-step) y las variables de lag.\nLas features de paso de tiempo son variables que se pueden derivar directamente del índice de tiempo. La feature de paso de tiempo más básica es la dummy (time dummy), que cuenta el número de pasos de tiempo en las series desde el principio al final.\nimport numpy as np df[\"Time\"] = np.arange(len(df.index)) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Hardcover Time   Date       2000-04-01 139 0   2000-04-02 128 1   2000-04-03 172 2   2000-04-04 139 3   2000-04-05 191 4     La regresión lineal con la time dummy produce el siguiente modelo:\nobjetivo = peso * time + bias\nimport matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(11, 4)) ax.plot(\"Time\", \"Hardcover\", data=df, color=\"0.75\") ax = sns.regplot(x=\"Time\", y=\"Hardcover\", data=df, ci=None, scatter_kws=dict(color=\"0.25\")) ax.set_title(\"Ventas de libros\"); Las features de paso de tiempo nos permiten modelar la dependencia del tiempo. Una serie es dependiente del tiempo si sus valores se pueden predecir desde el momento en que ocurrieron. En las series de nuestro ejemplo, podemos predecir que las ventas al final de mes son generalmente más altas que las ventas al principio del mes.\nFeatures de lag Para hacer una variable de lag deslizamos las observaciones de las series del objetivo para que parezcan haber ocurrido más tarde en el tiempo. Aquí hemos creado una variable lag de 1-paso, aunque también es posible desplazar varios pasos.\ndf[\"Lag_1\"] = df[\"Hardcover\"].shift(1) df = df.reindex(columns=[\"Hardcover\", \"Lag_1\"]) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Hardcover Lag_1   Date       2000-04-01 139 NaN   2000-04-02 128 139.0   2000-04-03 172 128.0   2000-04-04 139 172.0   2000-04-05 191 139.0     La regresión lineal con la variable lag produce el siguiente modelo:\nobjetivo = peso * lag + bias\nEntonces, las variables de lag nos permiten dibujar gráficas donde cada observación en una serie se dibuja contra la observación anterior.\nfig, ax = plt.subplots() ax = sns.regplot(x=\"Lag_1\", y=\"Hardcover\", data=df, ci=None, scatter_kws=dict(color=\"0.25\")) ax.set_aspect(\"equal\") ax.set_title(\"Gráfico lag de Ventas\"); Podemos ver en el gráfico de lag que las ventas de un día (Hardcover) están correlacionadas con las ventas del día anterior (Lag_1). Cuando vemos una relación como ésta sabemos que una variable de lag será útil.\nDe forma más genérica, las features de lag nos permiten modelar la dependencia en serie o serial. Una serie temporal tiene dependencia serial cuando una observación se puede predecir a partir de las observaciones previas. En nuestro ejemplo, podemos predecir que ventas altas en un día, generalmente significan ventas altas en el siguiente día.\nLa adaptación de los algoritmos de machine learning a los problemas de series temporales se trata en gran medida con la ingeniería de features del índice de tiempo y los lags. Aunque estamos usando regresión lineal, estas variables serán útiles independientemente del algoritmo que seleccionemos para nuestras predicciones.\nEjemplo - Tráfico túnel El tráfico de túnel es una serie temporal que describe el número de vehículos que viajan a través del Túnel de Baregg en Suiza cada día desde noviembre 2002 a noviembre 2005. En este ejemplo, practicaremos aplicando regresión lineal a variables de paso de tiempo y variables lag.\ntunnel = pd.read_csv( \"../data/tunnel.csv\", index_col=\"Day\", parse_dates=[\"Day\"]) tunnel.to_period() tunnel.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  NumVehicles   Day      2003-11-01 103536   2003-11-02 92051   2003-11-03 100795   2003-11-04 102352   2003-11-05 106569     Por defecto, Pandas crea un DatetimeIndex cuyo tipo es Timestamp, equivalente a np.datetime64, representando una serie temporal como una secuencia de medidas tomadas en un determinado momento. Un PeriodIndex, por otro lado, representa una serie temporal como una secuencia de cuantiles acumulados en periodos de tiempo. Los periodos suelen ser más fáciles de trabajar con ellos.\nVariable de paso de tiempo Siempre que a la serie temporal no le falten fechas, podemos crear una time dummy contando la longitud de las series.\ndf = tunnel.copy() df[\"Time\"] = np.arange(len(tunnel.index)) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  NumVehicles Time   Day       2003-11-01 103536 0   2003-11-02 92051 1   2003-11-03 100795 2   2003-11-04 102352 3   2003-11-05 106569 4     Vamos a entrenar un modelo de regresión lineal.\nfrom sklearn.linear_model import LinearRegression X = df.loc[:, [\"Time\"]] y = df.loc[:, \"NumVehicles\"] model = LinearRegression() model.fit(X, y) # Almacena las predicciones como una serie temporal con el mismo # índice de tiempo que los datos de entrenamiento y_pred = pd.Series(model.predict(X), index=X.index) y_pred Day 2003-11-01 98176.206344 2003-11-02 98198.703794 2003-11-03 98221.201243 2003-11-04 98243.698693 2003-11-05 98266.196142 ... 2005-11-12 114869.313898 2005-11-13 114891.811347 2005-11-14 114914.308797 2005-11-15 114936.806247 2005-11-16 114959.303696 Length: 747, dtype: float64  Veamos cuáles son los coeficientes e intercept obtenidos:\nmodel.coef_, model.intercept_ (array([22.49744953]), 98176.20634409295)  Por tanto, el modelo creado realmente es, aproximadamente: Vehicles = 22.5 * Time + 98176. Al dibujar los valores obtenidos a lo largo del tiempo se muestra cómo la regresión lineal ajustada a la time dummy crea la línea de tendencia para esta ecuación.\n# Set Matplotlib defaults plt.style.use(\"seaborn-whitegrid\") plt.rc(\"figure\", autolayout=True, figsize=(11, 4)) plt.rc( \"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10, ) plot_params = dict( color=\"0.75\", style=\".-\", markeredgecolor=\"0.25\", markerfacecolor=\"0.25\", legend=False, ) ax = y.plot(**plot_params) ax = y_pred.plot(ax=ax, linewidth=3) ax.set_title('Time Plot del Tráfico del Túnel'); Variable lag Pandas proporciona un método simple para “lagear” una serie, el método shift.\ndf[\"Lag_1\"] = df[\"NumVehicles\"].shift(1) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  NumVehicles Time Lag_1   Day        2003-11-01 103536 0 NaN   2003-11-02 92051 1 103536.0   2003-11-03 100795 2 92051.0   2003-11-04 102352 3 100795.0   2003-11-05 106569 4 102352.0     Cuando creamos variables lag, necesitamos decidir qué hacer con los valores faltantes que se generan. Una opción es rellenarlos, quizas con 0.0 o con el primer valor conocido. En lugar de esto vamos a eliminar los valores faltantes, asegurándonos también de eliminar los valores del objetivo en las fechas correspondientes.\nfrom sklearn.linear_model import LinearRegression X = df.loc[:, [\"Lag_1\"]] X.dropna(inplace=True) y = df.loc[:, \"NumVehicles\"] y, X = y.align(X, join=\"inner\") model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) El diagrama de lag nos muestra cómo de bien somos capaces de ajustar la relación entre el número de vehículos de un día y el número del día anterior.\nfig, ax = plt.subplots() ax.plot(X['Lag_1'], y, '.', color='0.25') ax.plot(X['Lag_1'], y_pred) ax.set_aspect('equal') ax.set_ylabel('NumVehicles') ax.set_xlabel('Lag_1') ax.set_title('Diagrama de lag del Tráfico del Túnel'); ¿Qué significa esta predicción de la variable lag sobre cómo de bien puede predecir las series a lo largo del tiempo? El siguiente gráfico temporal nos muestra cómo nuestros pronósticos de ahora responden al comportamiento de las series del pasado reciente.\nax = y.plot(**plot_params) ax = y_pred.plot() Los mejores modelos de series temporales normalmente incluirán alguna combinación entre variables de paso de tiempo y variables lag.\nEjercicio Vamos a realizar un ejercicio para ampliar lo que acabamos de ver. Para ello cargaremos algunos datasets.\nbook_sales = pd.read_csv( \"../data/book_sales.csv\", index_col=\"Date\", parse_dates=[\"Date\"], ).drop(\"Paperback\", axis=1) book_sales[\"Time\"] = np.arange(len(book_sales.index)) book_sales[\"Lag_1\"] = book_sales[\"Hardcover\"].shift(1) book_sales = book_sales.reindex(columns=[\"Hardcover\", \"Time\", \"Lag_1\"]) book_sales.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Hardcover Time Lag_1   Date        2000-04-01 139 0 NaN   2000-04-02 128 1 139.0   2000-04-03 172 2 128.0   2000-04-04 139 3 172.0   2000-04-05 191 4 139.0     Una de las ventajas que tiene la regresión lineal sobre algoritmos más complicados es que los modelos que genera son interpretables, es decir, es fácil interpretar la contribución que hace cada feature a las predicciones. En el modelo objetivo = peso * feature + bias, el peso nos dice cuánto cambia el objetivo de media por cada unidad de cambio de la feature.\nVamos a ver la regresión lineal de las ventas de libros:\nfig, ax = plt.subplots() ax.plot('Time', 'Hardcover', data=book_sales, color='0.75') ax = sns.regplot(x='Time', y='Hardcover', data=book_sales, ci=None, scatter_kws=dict(color='0.25')) ax.set_title('Gráfico temporal de venta de libros'); Interpretar la regresión lineal con time dummy Digamos que la regresión lineal tiene una ecuación aproximada de: Hardcover = 3.33 * Time + 150.5. Al cabo de 6 días, ¿cuánto se esperaría que cambiaran las ventas de libros?\nSi aplicamos la fórmula, entonces 3.33 * 6 + 150.5 = 19.98. Luego se esperaría que las ventas sean de 19.98 libros. De acuerdo a este modelo, dado que la pendiente es 3.33, la venta de libros Hardcover cambiará de media 3.33 unidades por cada paso que cambie Time.\nInterpretar la regresión lineal con una variable lag Interpretar los coeficientes de regresión puede ayudarnos a reconocer dependencias seriales en un gráfico temporal. Consideremos el modelo objetivo = peso * lag_1 + error, donde error es ruido aleatorio y peso es un número entre -1 y 1. En este caso, el peso nos dice cómo es de probable que el siguiente paso de tiempo tenga el mismo signo que el paso de tiempo anterior: un peso cercano a 1 significa que el objetivo probablemente tendrá el mismo signo que el paso previo, mientras que un peso cercano a -1 significa que el objetivo probablemente tendrá el signo opuesto.\nTenemos las siguientes dos series temporales:\nar = pd.read_csv(\"../data/ar.csv\") ar.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ar1 ar2     0 0.541286 -1.234475   1 -1.692950 3.532498   2 -1.730106 -3.915508   3 -0.783524 2.820841   4 -1.796207 -1.084120     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 5.5), sharex=True) ax1.plot(ar['ar1']) ax1.set_title('Series 1') ax2.plot(ar['ar2']) ax2.set_title('Series 2'); Una de estas series tiene la ecuación: objetivo = 0.95 * lag_1 + error y la otra tiene la ecuación objetivo = -0.95 * lag_1 + error, diferenciándose únicamente por el signo de la variable lag. ¿Qué ecuación correspondería a cada serie?\nLa Serie 1 estaría generada por la ecuación objetivo = 0.95 * lag_1 + error y la Serie 2 estaría generada por la ecuación objetivo = -0.95 * lag_1 + error. Como explicamos anteriormente, la serie con el peso 0.95 (signo positivo) tenderá a tener valores con signos que permanecen iguales. La serie con el peso -0.95 (signo negativo) tenderá a tener valores con signos que van y vienen. `\nEntrenar una variable de paso de tiempo Vamos a cargar el dataset de la competición de Pronóstico de series temporales de ventas de almacén. El dataset completo contiene casi 1800 series registrando las ventas de una amplia variedad de familias de productos desde 2013 a 2017. En principio solo trabajaremos con una única serie (average_sales) de las ventas medias por día.\ndtype = { 'store_nbr': 'category', 'family': 'category', 'sales': 'float32', 'onpromotion': 'uint64', } store_sales = pd.read_csv( \"../data/store_sales/train.csv\", dtype=dtype, parse_dates=['date'], infer_datetime_format=True, ) store_sales.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id date store_nbr family sales onpromotion     0 0 2013-01-01 1 AUTOMOTIVE 0.0 0   1 1 2013-01-01 1 BABY CARE 0.0 0   2 2 2013-01-01 1 BEAUTY 0.0 0   3 3 2013-01-01 1 BEVERAGES 0.0 0   4 4 2013-01-01 1 BOOKS 0.0 0     store_sales = store_sales.set_index('date').to_period('D') store_sales = store_sales.set_index(['store_nbr', 'family'], append=True) store_sales.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n    id sales onpromotion   date store_nbr family        2013-01-01 1 AUTOMOTIVE 0 0.0 0   BABY CARE 1 0.0 0   BEAUTY 2 0.0 0   BEVERAGES 3 0.0 0   BOOKS 4 0.0 0     average_sales = store_sales.groupby('date').mean()['sales'] average_sales.head() date 2013-01-01 1.409438 2013-01-02 278.390808 2013-01-03 202.840195 2013-01-04 198.911148 2013-01-05 267.873230 Freq: D, Name: sales, dtype: float32  Vamos a crear un modelo de regresión lineal con una variable de paso de tiempo en la serie de promedio de ventas de producto. El objetivo es la columna sales.\nfrom sklearn.linear_model import LinearRegression df = average_sales.to_frame() # Crea time dummy time = np.arange(len(df.index)) df['time'] = time # Crea los datos de entrenamiento X = df.loc[:, [\"time\"]] # features y = df.loc[:, \"sales\"] # objetivo # Entrena el modelo model = LinearRegression() model.fit(X, y) y_pred = pd.Series(model.predict(X), index=X.index) Vamos a dibujar la gráfica con el resultado:\nax = y.plot(**plot_params, alpha=0.5) ax = y_pred.plot(ax=ax, linewidth=3) ax.set_title('Gráfica temporal de Ventas totales'); Entrenar una variable lag Vamos a crear un modelo de regresión lineal con una variable lag en la serie de promedio de ventas de producto. El objetivo es la columna sales.\ndf = average_sales.to_frame() # Crea la variable lag lag_1 = df[\"sales\"].shift(1) df['lag_1'] = lag_1 X = df.loc[:, ['lag_1']].dropna() # features y = df.loc[:, 'sales'] # target y, X = y.align(X, join='inner') model = LinearRegression() model.fit(X,y) y_pred = pd.Series(model.predict(X), index=X.index) Vamos a dibujar la gráfica con el resultado:\nfig, ax = plt.subplots() ax.plot(X['lag_1'], y, '.', color='0.25') ax.plot(X['lag_1'], y_pred) ax.set(aspect='equal', ylabel='sales', xlabel='lag_1', title='Diagrama de lag de Ventas promedio'); ","description":"","tags":["series temporales","regresión lineal","time dummy","lag feature","time-step feature"],"title":"Series temporales: (1) Regresión lineal","uri":"/posts/time-series-linear-regression/"},{"categories":["tutoriales"],"content":"En anteriores posts vimos el framework general de validación cruzada y su uso para evaluar el rendimiento de modelos. Sin embargo, es importante tener en cuenta que algunos elementos de la validación cruzada deben decidirse en función de la naturaleza del problema: (i) la estrategia de validación cruzada y (ii) las métricas de evaluación. Además, siempre es bueno comparar el rendimiento de los modelos respecto de alguna línea base.\nEn este post presentaremos ambos aspectos y ofreceremos intuiciones e ideas de cuándo usar una estrategia de validación cruzada y métrica específicas. Además, también daremos algunas ideas sobre cómo comparar un modelo con alguna línea base.\nComo objetivos generales intentaremos:\n comprender la necesidad de usar una estrategia adecuada de validación cruzada dependiendo de los datos; obtener las intuiciones y principios que hay detrás del uso de la validación cruzada anidada cuando el modelo necesita ser evaluado y optimizado; comprender las diferencias entre las métricas de regresión y clasificación; comprender las diferencias entre métricas.  Comparación del rendimiento del modelo con una línea base simple Vamos a ver cómo comparar el rendimiento de generalización de un modelo con una mínima linea base. En regresión, podemos usar la clase DummyRegressor para predecir el valor medio del objetivo observado en el conjunto de entrenamiento sin usar las variables de entrada.\nDemostraremos cómo calcular la puntuación de un modelo de regresión y compararlo con una línea base en el dataset de viviendas de California.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 # reescala el objetivo en k$ En todas las evaluaciones usaremos un divisor de validación cruzada ShuffleSplit con el 20% de los datos reservados para validación.\nfrom sklearn.model_selection import ShuffleSplit cv = ShuffleSplit(n_splits=30, test_size=0.2, random_state=0) Empezaremos ejecutando la validación cruzada para un simple árbol de decisión regresor, que es nuestro modelo de interés. Además, almacenaremos el error de prueba en un objeto Series de pandas para hacer más sencillo dibujar los resultados.\nimport pandas as pd from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import cross_validate regressor = DecisionTreeRegressor() cv_results_tree_regressor = cross_validate( regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) errors_tree_regressor = pd.Series( -cv_results_tree_regressor[\"test_score\"], name=\"Arbol decisión regresor\" ) errors_tree_regressor.describe() count 30.000000 mean 45.641306 std 1.249005 min 43.111065 25% 44.691150 50% 45.586332 75% 46.640596 max 47.711138 Name: Arbol decisión regresor, dtype: float64  Luego, evaluamos nuestra línea base. Esta línea base se denomina regresor dummy. Este regresor dummy siempre predecirá la media del objetivo calculada en la variable objetivo de entrenamiento. Por lo tanto, el regresor dummy no usa ninguna información de las variables de entrada almacenadas en el dataframe llamado X.\nfrom sklearn.dummy import DummyRegressor dummy = DummyRegressor(strategy=\"mean\") result_dummy = cross_validate( dummy, X, y, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) errors_dummy_regressor = pd.Series( -result_dummy[\"test_score\"], name=\"Regresor dummy\" ) errors_dummy_regressor.describe() count 30.000000 mean 91.140009 std 0.821140 min 89.757566 25% 90.543652 50% 91.034555 75% 91.979007 max 92.477244 Name: Regresor dummy, dtype: float64  Ahora dibujaremos los errores de prueba de la validación cruzada para la línea base usando la media del objetivo y el actual árbol de decisión regresor.\nall_errors = pd.concat( [errors_tree_regressor, errors_dummy_regressor], axis=1, ) all_errors  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Arbol decisión regresor Regresor dummy     0 47.156293 90.713153   1 46.491834 90.539353   2 43.960410 91.941912   3 43.343595 90.213912   4 47.711138 92.015862   5 44.960189 90.542490   6 44.180467 89.757566   7 44.498127 92.477244   8 45.463726 90.947952   9 45.048634 91.991373   10 46.661503 92.023571   11 46.020580 90.556965   12 45.755225 91.539567   13 45.130518 91.185225   14 47.388774 92.298971   15 44.601470 91.084639   16 45.564095 90.984471   17 47.202268 89.981744   18 44.568353 90.547140   19 46.764385 89.820219   20 43.111065 91.768721   21 45.608568 92.305556   22 45.263799 90.503017   23 46.884297 92.147974   24 46.215357 91.386320   25 45.934370 90.815660   26 44.374564 92.216574   27 46.577874 90.107460   28 45.182111 90.620318   29 47.615573 91.165331     import matplotlib.pyplot as plt import numpy as np bins = np.linspace(start=0, stop=100, num=80) all_errors.plot.hist(bins=bins, edgecolor=\"black\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.xlabel(\"Error absoluto medio (k$)\") _ = plt.title(\"Errores de prueba de validación cruzada\") Vemos que el rendimiento de generalización de nuestro ábol de decisión está lejos de ser perfecto: las predicciones del precio tiene aproximadamente un márgen de error de 45000 dólares de media. Sin embargo, es mucho mejor que el margen de error de la línea base. Por tanto, esto confirma que es posible predecir mucho mejor el precio de la vivienda usando un modelo que tenga en cuenta los valores de las variables de entrada (localización de la propiedad, tamaño, ingresos del vecindario, etc.). Dicho modelo hace predicciones más formadas y, aproximadamente, divide la tasa de error por la mitad comparado con la línea base que ignora las variables de entrada.\nObservemos que hemos usado la media del precio como predicción de línea base. Podríamos haber usado en su lugar la mediana. Véase la documentación online de la clase sklearn.dummy.DummyRegressor para otras opciones. Para este ejemplo en particular, no existe mucha diferencia entre usar la media en lugar de la mediana, pero este podría ser el caso de un dataset con valores atípicos extremos.\nEjercicio Vamos a poner en práctica lo aprendido hasta ahora. Definiremos una línea base con un clasificador dummy y lo usaremos como referencia para evaluar el rendimiento predictivo relativo de un modelo de interés dado.\nIlustraremos dicha línea base con la ayuda del dataset del censo de adultos, usando únicamente las variables numéricas, por simplicidad.\nadult_census = pd.read_csv(\"../data/adult-census-numeric-all.csv\") X, y = adult_census.drop(columns=\"class\"), adult_census[\"class\"] Primero definiremos una estrategia de validación cruzada con ShuffleSplit tomando la mitad de las muestras como prueba en cada ciclo. Usaremos 10 ciclos de validación cruzada.\ncv = ShuffleSplit(n_splits=10, test_size=0.5, random_state=0) Lo siguiente es crear un pipeline de machine learning compuesto por un transformador para estandarizar los datos seguido por un clasificador de regresión logística.\nfrom sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline classifier = make_pipeline(StandardScaler(), LogisticRegression()) Calculemos ahora las puntuaciones de prueba de validación cruzada para el clasificador en este dataset y almacenaremos los resultados en una Series de panda.\nfrom sklearn.model_selection import cross_validate cv_results_logistic_regression = cross_validate( classifier, X, y, cv=cv, n_jobs=-1) test_score_logistic_regression = pd.Series( cv_results_logistic_regression[\"test_score\"], name=\"Regresión logística\" ) test_score_logistic_regression 0 0.815937 1 0.813849 2 0.815036 3 0.815569 4 0.810982 5 0.814709 6 0.813112 7 0.810327 8 0.812416 9 0.816388 Name: Regresión logística, dtype: float64  Ahora calcularemos las puntuaciones de validación cruzada de un clasificador dummy que prediga constantemente la clase más frecuente observada en el conjunto de entrenamiento. Almacenaremos los resultados en un Series de panda.\nfrom sklearn.dummy import DummyClassifier dummy_most_frequent = DummyClassifier(strategy=\"most_frequent\") result_dummy = cross_validate( dummy_most_frequent, X, y, cv=cv, n_jobs=-1 ) test_dummy_most_frequent = pd.Series( result_dummy[\"test_score\"], name=\"Most-frequent dummy\" ) test_dummy_most_frequent 0 0.760329 1 0.756808 2 0.759142 3 0.760739 4 0.761681 5 0.761885 6 0.757463 7 0.757176 8 0.761885 9 0.763114 Name: Most-frequent dummy, dtype: float64  Ahora que hemos recopilado los resultados tanto de la línea base como del modelo, vamos a concatenar las puntuaciones de prueba como columnas en un dataframe de pandas.\nall_scores = pd.concat( [test_score_logistic_regression, test_dummy_most_frequent], axis=1, ) all_scores  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Regresión logística Most-frequent dummy     0 0.815937 0.760329   1 0.813849 0.756808   2 0.815036 0.759142   3 0.815569 0.760739   4 0.810982 0.761681   5 0.814709 0.761885   6 0.813112 0.757463   7 0.810327 0.757176   8 0.812416 0.761885   9 0.816388 0.763114     Lo siguiente que haremos será dibujar el histograma de las puntuaciones de prueba de validación cruzada para ambos modelos con la ayuda de la función de dibujado incorporada en pandas. ¿Qué conclusiones obtenemos de los resultados?\nimport matplotlib.pyplot as plt import numpy as np bins = np.linspace(start=0.5, stop=1.0, num=100) all_scores.plot.hist(bins=bins, edgecolor=\"black\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.xlabel(\"Accuracy (%)\") _ = plt.title(\"Puntuaciones de prueba de validación cruzada\") Observamos que los dos histogramas están bien separados. Por lo tanto el clasificador dummy con la estrategia most_frequent tiene mucha menos precisión que el clasificador de regresión logística. Podemos concluir que el modelo de regresión logística puede encontrar satisfactoriamente información predictiva en las variables de entrada para mejorar la línea base.\nPor último, vamos a cambiar la estrategia del clasificador dummy a \"stratified\" y calcularemos los resultados. De igual forma, calcularemos las puntuaciones para la strategy=\"uniform\" y después dibujaremos la distribución conjuntamente con los otros resultados.\n¿Estas nuevas líneas base son mejores que la previa? ¿Por qué es este el caso?\nPodemos consultar la documentación de sklearn.dummy.DummyClassifier para conocer el significado de estas estrategias.\ndummy_stratified = DummyClassifier(strategy=\"stratified\") result_dummy_stratified = cross_validate( dummy_stratified, X, y, cv=cv, n_jobs=-1 ) test_dummy_stratified = pd.Series( result_dummy_stratified[\"test_score\"], name=\"Stratified dummy\" ) test_dummy_stratified 0 0.638590 1 0.640596 2 0.636133 3 0.634085 4 0.634085 5 0.634126 6 0.632652 7 0.638016 8 0.639327 9 0.635027 Name: Stratified dummy, dtype: float64  dummy_uniform = DummyClassifier(strategy=\"uniform\") result_dummy_uniform = cross_validate( dummy_uniform, X, y, cv=cv, n_jobs=-1 ) test_dummy_uniform = pd.Series( result_dummy_uniform[\"test_score\"], name=\"Uniform dummy\" ) test_dummy_uniform 0 0.502477 1 0.505426 2 0.505549 3 0.501003 4 0.503911 5 0.497318 6 0.501413 7 0.499775 8 0.500143 9 0.496171 Name: Uniform dummy, dtype: float64  all_scores = pd.concat( [test_score_logistic_regression, test_dummy_most_frequent, test_dummy_stratified, test_dummy_uniform,], axis=1, ) all_scores  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Regresión logística Most-frequent dummy Stratified dummy Uniform dummy     0 0.815937 0.760329 0.638590 0.502477   1 0.813849 0.756808 0.640596 0.505426   2 0.815036 0.759142 0.636133 0.505549   3 0.815569 0.760739 0.634085 0.501003   4 0.810982 0.761681 0.634085 0.503911   5 0.814709 0.761885 0.634126 0.497318   6 0.813112 0.757463 0.632652 0.501413   7 0.810327 0.757176 0.638016 0.499775   8 0.812416 0.761885 0.639327 0.500143   9 0.816388 0.763114 0.635027 0.496171     bins = np.linspace(start=0.5, stop=1.0, num=100) all_scores.plot.hist(bins=bins, edgecolor=\"black\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.xlabel(\"Accuracy (%)\") _ = plt.title(\"Puntuaciones de prueba de validación cruzada\") Vemos que al usar strategy=\"stratified\" los resultados son mucho peores que con la estrategia most_frequent. Dado que las clases están desbalanceadas, predecir la más frecuente implica que acertaremos en la proporción de esa clase (aproximadamente el 75% de las muestras), es decir, el 75% de las veces. Sin embargo, la estrategia \"stratified\" generará predicciones aleatoriamente respetando la distribución de las clases del conjunto de entrenamiento, lo que dará como resultado algunas predicciones incorrectas, incluso para la clase más frecuente, por lo tanto obtenemos una precisión menor.\nEsto es aún más cierto para strategy=\"uniform\": esta estrategia asigna etiquetas de clase uniformemente al azar. Por lo tanto, en un problema de clasificación binaria, la precisión de validación cruzada es del 50% de media, por lo cual es la más débil de las tres líneas base dummy.\nNota: se podría argumentar que las estrategias \"uniform\" y \"stratified\" son maneras válidas de definir un “nivel de aleatoriedad” de la precisión de una línea base para el problema de clasificación, debido a que hacen predicciones “al azar”.\nOtra forma de definir un nivel de azar podría ser usando la utilidad de scikit-learn sklearn.model_selection.permutation_test_score. En lugar de usar un clasificador dummy, esta función compara la precisión de validación cruzada de un modelo de interés con la precisión de validación cruzada de este mismo modelo pero entrenado en etiquetas de clase permutadas aleatoriamente. Por lo tanto, permutation_test_score define un nivel de azar que depende de la elección de la clase y los hiperparámetros del estimador de interés. Cuando entrenamos en tales etiquetas permutadas aleatoriamente, muchos estimadores de machine learning terminan comportándose aproximadamente como DummyClassifier(strategy=\"most_frequent\"), prediciendo siempre la clase mayoritaria, independientemente de las variables de entrada. Como resultado, esta línea base most_frequent se llama algunas veces “nivel de azar” para problemas de clasificación desbalanceados, aunque sus predicciones son completamente deterministas y no involucran mucho azar.\nDefinir el nivel de azar usando permutation_test_score es bastante costoso computacionalmente, debido a que requiere entrenar muchos modelos no dummys en permutaciones aleatorias de los datos. Usar clasificadores dummys como líneas base suele ser suficiente para fines prácticos. Para problemas de clasificacion desbalanceados, la estrategia most_frequent es la más fuerte de las tres líneas bases y por tanto la que debemos usar.\nElección de validación cruzada Estratificación Generalmente hemos usado por defecto una estrategia de validación cruzada KFold o ShuffleSplit para dividir de forma iterativa nuestro dataset. Sin embargo, no debemos asumir que estos enfoques son siempre la mejor opción: otras estrategias de validación cruzada podrían adaptarse mejor a nuestro problema.\nComencemos con el concepto de estratificación, dando un ejemplo donde podemos tener problemas si no somos cuidadosos. Carguemos el dataset iris.\nfrom sklearn.datasets import load_iris X, y = load_iris(as_frame=True, return_X_y=True) Vamos a crear un modelo de machine learning básico: una regresión logística. Esperamos que este modelo funcione bastante bien en el dataset iris, ya que es un dataset bastante simple.\nfrom sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline model = make_pipeline(StandardScaler(), LogisticRegression()) Una vez que hemos creado nuestro modelo, usaremos validación cruzada para evaluarlo. Usaremos la estrategia KFold. Definiremos un dataset con nueve muestras y repetiremos la validación cruzada tres veces (es decir, n_splits).\nimport numpy as np from sklearn.model_selection import KFold X_random = np.random.randn(9, 1) cv = KFold(n_splits=3) for train_index, test_index in cv.split(X_random): print(f\"ENTRENAMIENTO: {train_index} PRUEBA: {test_index}\") ENTRENAMIENTO: [3 4 5 6 7 8] PRUEBA: [0 1 2] ENTRENAMIENTO: [0 1 2 6 7 8] PRUEBA: [3 4 5] ENTRENAMIENTO: [0 1 2 3 4 5] PRUEBA: [6 7 8]  Al definir tres divisiones, usaremos tres muestras para prueba y seis para entrenamiento cada vez. KFold por defecto no baraja. Lo que significa que seleccionará las tres primeras muestras para el conjunto de prueba en la primera división, luego las tres siguientes muestras para la segunda división y las siguientes tres para la última división. Al final, todas las muestras se habrán usado en la prueba al menos una vez entre las diferentes divisiones.\nAhora vamos a aplicar esta estrategia para verificar el rendimiento de generalización de nuestro modelo.\nfrom sklearn.model_selection import cross_validate cv = KFold(n_splits=3) results = cross_validate(model, X, y, cv=cv) test_score = results[\"test_score\"] print(f\"La precisión media es: \" f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\") La precisión media es: 0.000 +/- 0.000  Es una sorpresa real que nuestro modelo no pueda clasificar correctamente ninguna muestra en ninguna división de validación cruzada. Comprobemos nuestros valores de objetivo para comprender el problema.\nimport matplotlib.pyplot as plt y.plot() plt.xlabel(\"Indice muestra\") plt.ylabel(\"Clase\") plt.yticks(y.unique()) _ = plt.title(\"Valor de la clase en el objetivo y\") Vemos que el vector objetivo y está ordenado. Tendrá algunas consecuencias inesperadas cuando usemos la validación cruzada KFold. Para ilustrar las consecuencias, mostraremos el recuento de clases en cada partición de la validación cruzada en los conjuntos de entrenamiento y prueba y dibujaremos esta información en un barplot.\nimport pandas as pd n_splits = 3 cv = KFold(n_splits=n_splits) train_cv_counts = [] test_cv_counts = [] for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)): y_train, y_test = y.iloc[train_idx], y.iloc[test_idx] train_cv_counts.append(y_train.value_counts()) test_cv_counts.append(y_test.value_counts()) train_cv_counts = pd.concat(train_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) train_cv_counts.index.name = \"Etiqueta clase\" train_cv_counts  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Partición #0 Partición #1 Partición #2   Etiqueta clase        1 50.0 NaN 50.0   2 50.0 50.0 NaN   0 NaN 50.0 50.0     test_cv_counts = pd.concat(test_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) test_cv_counts.index.name = \"Etiqueta clase\" test_cv_counts  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Partición #0 Partición #1 Partición #2   Etiqueta clase        0 50.0 NaN NaN   1 NaN 50.0 NaN   2 NaN NaN 50.0     train_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto entrenamiento\") test_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto prueba\") Podemos confirmar que en cada partición del conjunto de entrenamiento solo están presentes dos de las tres clases y todas las muestras de la clase restante se usan como conjunto de prueba. Por tanto, nuestro modelo es incapaz de predecir esta clase que no ha sido visto nunca durante la fase de entrenamiento.\nUna posibilidad para resolver este problema es barajar los datos antes de dividirlos en los tres grupos.\ncv = KFold(n_splits=3, shuffle=True, random_state=0) results = cross_validate(model, X, y, cv=cv) test_score = results[\"test_score\"] print(f\"La precisión media es: \" f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\") La precisión media es: 0.953 +/- 0.009  Obtenemos un resultado que está más cercano a lo se podría esperar con una precisión por encima del 90%. Ahora que hemos resulto nuestro primer problema, podría ser interesante comprobar si la frecuencia de clases en el conjunto de entrenamiento y prueba son iguales que las de nuestro dataset original. Aseguraría que estamos entrenando y probando nuestro modelo con una distribución de clases que encontaremos en producción.\ntrain_cv_counts = [] test_cv_counts = [] for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)): y_train, y_test = y.iloc[train_idx], y.iloc[test_idx] train_cv_counts.append(y_train.value_counts()) test_cv_counts.append(y_test.value_counts()) train_cv_counts = pd.concat(train_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) test_cv_counts = pd.concat(test_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) train_cv_counts.index.name = \"Etiqueta clase\" test_cv_counts.index.name = \"Etiqueta clase\" train_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto entrenamiento\") test_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto prueba\") Vemos que ni el conjunto de entrenamiento ni el de prueba tienen las mismas frecuencias de clase que nuestro dataset original debido a que el recuento de cada clase varía un poco.\nSin embargo, podríamos querer dividir nuestros datos preservando las frecuencias de clase originales: queremos estratificar nuestros datos por clase. En scikit-learn, algunas estrategias de validación cruzada implementan la estratificación; contienen Stratified en sus nombres.\nfrom sklearn.model_selection import StratifiedKFold cv = StratifiedKFold(n_splits=3) results = cross_validate(model, X, y, cv=cv) test_score = results[\"test_score\"] print(f\"La precisión media es: \" f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\") La precisión media es: 0.960 +/- 0.016  train_cv_counts = [] test_cv_counts = [] for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)): y_train, y_test = y.iloc[train_idx], y.iloc[test_idx] train_cv_counts.append(y_train.value_counts()) test_cv_counts.append(y_test.value_counts()) train_cv_counts = pd.concat(train_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) test_cv_counts = pd.concat(test_cv_counts, axis=1, keys=[f\"Partición #{idx}\" for idx in range(n_splits)]) train_cv_counts.index.name = \"Etiqueta clase\" test_cv_counts.index.name = \"Etiqueta clase\" train_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto entrenamiento\") test_cv_counts.plot.bar() plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") plt.ylabel(\"Recuento\") _ = plt.title(\"Conjunto prueba\") En este caso, observamos que el recuento de clases es muy parecido entre el conjunto de entrenamiento y el conjunto de prueba. La diferencia es debida al pequeño número de muestras del dataset iris.\nEn conclusión, es una buena práctica usar estratificación dentro de la validación cruzada cuando se trata de un problema de clasificación.\nAgrupación de muestras Vamos a detenernos en el concepto de grupos de muestras. Usaremos el dataset de dígitos escritos a mano.\nfrom sklearn.datasets import load_digits digits = load_digits() X, y = digits.data, digits.target Vamos a recrear el mismo modelo que vimos más arriba: un clasificador de regresión logística con preprocesamiento para escalar los datos.\nfrom sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline model = make_pipeline(MinMaxScaler(), LogisticRegression(max_iter=1_000)) Usaremos el mismo modelo de línea base. Usaremos una validación cruzada KFold sin mezclar los datos al principio.\nfrom sklearn.model_selection import cross_val_score, KFold cv = KFold(shuffle=False) test_score_no_shuffling = cross_val_score(model, X, y, cv=cv, n_jobs=-1) print(f\"La precisión media es: \" f\"{test_score_no_shuffling.mean():.3f} +/- \" f\"{test_score_no_shuffling.std():.3f}\") La precisión media es: 0.931 +/- 0.026  Ahora repitamos lo mismo mezclando los datos dentro de la validación cruzada.\ncv = KFold(shuffle=True) test_score_with_shuffling = cross_val_score(model, X, y, cv=cv, n_jobs=-1) print(f\"La precisión media es: \" f\"{test_score_with_shuffling.mean():.3f} +/- \" f\"{test_score_with_shuffling.std():.3f}\") La precisión media es: 0.967 +/- 0.008  Observamos que mezclar los datos mejora la precisión media. Podríamos ir un poco más allá y dibujar la distribución de la puntuación de prueba. Primero concatenemos las puntuaciones de prueba.\nimport pandas as pd all_scores = pd.DataFrame( [test_score_no_shuffling, test_score_with_shuffling], index=[\"KFold sin mezclado\", \"KFold con mezclado\"], ).T import matplotlib.pyplot as plt all_scores.plot.hist(bins=10, edgecolor=\"black\", alpha=0.7) plt.xlim([0.8, 1.0]) plt.xlabel(\"Puntuación de precisión\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Distribución de las puntuaciones de prueba\") El error de prueba de validación cruzada que usa el mezclado tiene menos varianza que la que no impone ningún mezclado. Lo que significa que, en este caso, alguna partición específica conduce a una puntuación baja.\nprint(test_score_no_shuffling) [0.94166667 0.89722222 0.94986072 0.9637883 0.90250696]  Por lo tanto, existe una estructura subyacente en los datos que al mezclarlos se rompe y se obtienen mejores resultados. Para tener una mejor comprensión, podríamos leer la documentación que acompaña al dataset.\nprint(digits.DESCR) .. _digits_dataset: Optical recognition of handwritten digits dataset -------------------------------------------------- **Data Set Characteristics:** :Number of Instances: 1797 :Number of Attributes: 64 :Attribute Information: 8x8 image of integer pixels in the range 0..16. :Missing Attribute Values: None :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr) :Date: July; 1998 This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits The data set contains images of hand-written digits: 10 classes where each class refers to a digit. Preprocessing programs made available by NIST were used to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions. For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G. T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C. L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469, 1994. .. topic:: References - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their Applications to Handwritten Digit Recognition, MSc Thesis, Institute of Graduate Studies in Science and Engineering, Bogazici University. - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika. - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin. Linear dimensionalityreduction using relevance weighted LDA. School of Electrical and Electronic Engineering Nanyang Technological University. 2005. - Claudio Gentile. A New Approximate Maximal Margin Classification Algorithm. NIPS. 2000.  Si leemos cuidadosamente, 13 personas escribieron los dígitos para nuestro dataset, lo que supone un total de 1797 muestras. Por tanto, una persona escribió varias veces el mismo número. Supongamos que las muestras de la persona están agrupadas. En consecuencia, no mezclar los datos mantendrá todas las muestras de la persona juntas, ya sea en el conjunto de entrenamiento como en el de prueba. Mezclar los datos romperá esta estructura y, por tanto, dígitos escritos por la misma persona estarán disponibles tanto en conjunto de entrenamiento como en el de prueba.\nAdemás, por norma general, una persona tenderá a escribir los dígitos de la misma manera. Así, nuestro modelo aprenderá a identificar los patrones de una persona para cada dígito en lugar de reconocer el dígito en sí mismo.\nPodemos resolver este problema asegurando que los datos asociados a una persona pertenezcan al conjunto de entrenamiento o al de prueba. Por tanto, queremos agrupar muestras para cada persona. De hecho, podemos recuperar los grupos mirando la variable objetivo.\ny[:200] array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4, 6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6, 4, 9])  Puede que no sea obvio al principio, pero existe una estructura en el objetivo: existe un patrón repetitivo que siempre empieza por una serie ordenada de dígitos del 0 al 9 seguidos de dígitos aleatorios en un cierto punto. Si miramos en detalle, vemos que existen 14 de tales patrones, siempre con alrededor de 130 muestras cada uno.\nIncluso si no corresponden exactamente a las 13 personas que menciona la documentación (quizás una persona escribió dos series de dígitos), podemos hipotetizar que cada uno de esos patrones corresponde a una persona diferente y, por tanto, a un grupo diferente.\nfrom itertools import count import numpy as np # define los límites inferior y superior de cada índice de muestras # para cada escritor writer_boundaries = [0, 130, 256, 386, 516, 646, 776, 915, 1029, 1157, 1287, 1415, 1545, 1667, 1797] groups = np.zeros_like(y) lower_bounds = writer_boundaries[:-1] upper_bounds = writer_boundaries[1:] for group_id, lb, up in zip(count(), lower_bounds, upper_bounds): groups[lb:up] = group_id Podemos verificar los grupos dibujando los índices asociados a los id’s de escritor.\nplt.plot(groups) plt.yticks(np.unique(groups)) plt.xticks(writer_boundaries, rotation=90) plt.xlabel(\"Índice del objetivo\") plt.ylabel(\"Índice del escritor\") _ = plt.title(\"Grupos de escritores subyacentes existentes en el objetivo\") Una vez que agrupamos los dígitos por escritor, podemos usar validación cruzada para tener esta información en cuenta: la clase conteniendo Group debe ser usada.\nfrom sklearn.model_selection import GroupKFold cv = GroupKFold() test_score = cross_val_score(model, X, y, groups=groups, cv=cv, n_jobs=2) print(f\"La precisión media es \" f\"{test_score.mean():.3f} +/- \" f\"{test_score.std():.3f}\") La precisión media es 0.920 +/- 0.021  Vemos que esta estrategia es menos optimista en lo que respecta al rendimiento de generalización del modelo. Sin embargo, este es más confiable si nuestro propósito es hacer reconocimiento de dígitos manuscritos por escritores independientes. Además, podemos ver que la desviación estándar se ha reducido.\nall_scores = pd.DataFrame( [test_score_no_shuffling, test_score_with_shuffling, test_score], index=[\"KFold sin mezclado\", \"KFold con mezclado\", \"KFold con grupos\"], ).T all_scores.plot.hist(bins=10, edgecolor=\"black\", alpha=0.7) plt.xlim([0.8, 1.0]) plt.xlabel(\"Puntuación de precisión\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Distribución de las puntuaciones de prueba\") Como conclusión, cuando evaluamos un modelo es realmente importante tener en cuenta cualquier patrón de agrupamiento de las muestras. De lo contario, los resultados obtenidos podrían ser demasiado optimistas respecto a la realidad.\nSin datos i.i.d En machine learning es bastante común asumir que los datos son i.i.d. (acrónimo inglés correspondiente a “independent and identically distributed”, independientes e idénticamente distribuidos), lo que significa que el proceso generativo no tiene ninguna memoria de muestras pasadas para generar nuevas muestras.\nEsta asunción es violada normalmente cuando tratamos con series temporales. Una muestra depende de información pasada.\nVeremos un ejemplo para destacar estos problemas con datos no i.i.d en las estrategias de validación cruzada presentadas anteriormente. Vamos a cargar cotizaciones financieras de algunas empresas de energía.\nimport pandas as pd symbols = {\"TOT\": \"Total\", \"XOM\": \"Exxon\", \"CVX\": \"Chevron\", \"COP\": \"ConocoPhillips\", \"VLO\": \"Valero Energy\"} template_name = \"../data/financial-data/{}.csv\" quotes = {} for symbol in symbols: data = pd.read_csv( template_name.format(symbol), index_col=0, parse_dates=True ) quotes[symbols[symbol]] = data[\"open\"] quotes = pd.DataFrame(quotes) Vamos a empezar dibujando las diferentes cotizaciones financieras.\nimport matplotlib.pyplot as plt quotes.plot() plt.ylabel(\"Valor de cotización\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Evolución del valor de las acciones\") Vamos a repetir el experimento anterior. En lugar de usar datos aleatorios, esta vez usaremos cotizaciones reales. Aunque es obvio que un modelo predictivo no funciona en la práctica en datos aleatorios, esto es lo mismo con estos datos reales. Aquí queremos predecir la cotización de Chevron usando las cotizaciones de las otras compañías de energía.\nPara hacer gráficos explicativos, usaremos una única división además de la validación cruzada que ya usamos anteriormente.\nfrom sklearn.model_selection import train_test_split X, y = quotes.drop(columns=[\"Chevron\"]), quotes[\"Chevron\"] X_train, X_test, y_train, y_test = train_test_split( X, y, shuffle=True, random_state=0) Usaremos un árbol de decisión regresor que esperamos que tenga overfitting y, por tanto, no generalizará a datos no vistos. Usaremos una validación cruzada ShuffleSplit para comprobar el rendimiento de generalización de nuestro modelo.\nfrom sklearn.tree import DecisionTreeRegressor regressor = DecisionTreeRegressor() from sklearn.model_selection import ShuffleSplit cv = ShuffleSplit(random_state=0) from sklearn.model_selection import cross_val_score test_score = cross_val_score(regressor, X_train, y_train, cv=cv, n_jobs=-1) print(f\"La medida de R2 es: \" f\"{test_score.mean():.2f} +/- {test_score.std():.2f}\") La medida de R2 es: 0.95 +/- 0.07  Sorprendentemente, hemos obtenido un rendimiento de generalización excepcional. Investigaremos y buscaremos la razón de tan buenos resultados con un modelo que se espera que falle. Anteriormente, hemos mencionado que ShuffleSplit es un esquema de validación cruzada iterativo que mezcla y divide datos. Simplificaremos este procedimiento con una única división y dibujaremos la predicción. Para este propósito podemos usar train_test_split.\nregressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) # Afecta el índice de `y_pred` para facilitar el dibujado y_pred = pd.Series(y_pred, index=y_test.index) Vamos a comprobar el rendimiento de generalización de nuestro modelo en esta división.\nfrom sklearn.metrics import r2_score test_score = r2_score(y_test, y_pred) print(f\"R2 en esta única división es: {test_score:.2f}\") R2 en esta única división es: 0.83  De forma similar, obtenemos buenos resultados en términos de $R^2$. Dibujaremos las muestras de entrenamiento, prueba y predicción.\ny_train.plot(label=\"Entrenamiento\") y_test.plot(label=\"Prueba\") y_pred.plot(label=\"Predicción\") plt.ylabel(\"Valor cotización\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Predicción del modelo usando estrategia ShuffleSplit\") Entonces, en este contexto, parece que las predicciones del modelo siguen las pruebas. Pero también podemos ver que las muestras de prueba están al lado de alguna muestra de entrenamiento. Y con estas series temporales vemos una relación entre una muestra en el momento t y una muestra en t+1. En este caso estamos violando la asuncion i.i.d. La idea a obtener es la siguiente: un modelo puede generar su conjunto de entrenamiento en el momento t para una muestra de prueba en el momento t+1. Esta predicción estaría cerca del valor real, incluso aunque nuestro modelo no aprendiera nada, solo memorizando el dataset de entrenamiento.\nUna manera sencilla de verificar esta hipótesis es no mezclar los datos cuando hacemos la división. En este caso, usaremos el primer 75% de los datos para entrenar y los datos restantes para prueba.\nX_train, X_test, y_train, y_test = train_test_split( X, y, shuffle=False, random_state=0, ) regressor.fit(X_train, y_train) y_pred = regressor.predict(X_test) y_pred = pd.Series(y_pred, index=y_test.index) test_score = r2_score(y_test, y_pred) print(f\"R2 en esta única división es: {test_score:.2f}\") R2 en esta única división es: -2.16  En este caso vemos que nuestro modelo ya no es mágico. De hecho, su rendimiento es peor que sólo predecir la media del objetivo. Podemos comprobar visualmente qué estamos prediciendo.\ny_train.plot(label=\"Entrenamiento\") y_test.plot(label=\"Prueba\") y_pred.plot(label=\"Predicción\") plt.ylabel(\"Valor cotización\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Predicción del modelo usando una división sin shuffling\") Vemos que nuestro modelo no puede predecir nada porque no tiene muestras alrededor de la muestra de prueba. Comprobemos cómo podríamos haber hecho un esquema adecuado de validación cruzada para obtener una estimación razonable del rendimiento de generalización.\nUna solución podría ser agrupar los ejemplos en bloques de tiempo, por ejemplo por cuatrimestres, y predecir la información de cada grupo usando información de los otros grupos. Para este propósito, podemos usar la validación cruzada LeaveOneGroupOut.\nfrom sklearn.model_selection import LeaveOneGroupOut groups = quotes.index.to_period(\"Q\") cv = LeaveOneGroupOut() test_score = cross_val_score(regressor, X, y, cv=cv, groups=groups, n_jobs=-1) print(f\"La R2 media es: \" f\"{test_score.mean():.2f} +/- {test_score.std():.2f}\") La R2 media es: -0.74 +/- 1.72  En este caso, vemos que no podemos hacer buenas predicciones, lo que es menos sorprendente que nuestra resultados originales.\nOtra cosa a considerar es la aplicación real de nuestra solución. Si nuestro modelo tiene como objetivo la previsión (es decir, predicción de datos futuros a partir de datos pasados), no debemos usar datos de entrenamiento que sean posteriores a los datos de prueba. En este caso, podemos usar la validación TimeSeriesSplit para forzar este comportamiento.\nfrom sklearn.model_selection import TimeSeriesSplit cv = TimeSeriesSplit(n_splits=groups.nunique()) test_score = cross_val_score(regressor, X, y, cv=cv, groups=groups, n_jobs=2) print(f\"La R2 media es: \" f\"{test_score.mean():.2f} +/- {test_score.std():.2f}\") La R2 media es: -2.27 +/- 3.42  En conclusión, es importante no usar una estrategia de validación cruzada que no respete algunas asunciones, como tener datos i.i.d. Podría conducir a obtener resultados absurdos que podrían hacer pensar que un modelo predictivo podría funcionar.\nValidación cruzada anidada La validación cruzada se puede usar tanto para el ajuste de hiperparámetros como para la estimación del rendimiento de generalización de un modelo. Sin embargo, usarla para ambos propósitos al mismo tiempo puede ser problemático, ya que la evaluación resultante puede subestimar algún overfitting que resulta del procedimiento de ajuste de hiperparámetros en sí mismo.\nDesde un punto de vista filosófico, el ajuste de hiperparámetros es una forma de machine learning en sí misma y, por tanto, necesita otro bucle externo de validación cruzada para evaluar apropiadamente el rendimiento de generalización del procedimiento de modelado global.\nVamos a destacar la validación cruzada anidada y su impacto en el rendimiento de generalización estimado comparado con usar simplemente un único nivel de validación cruzada, tanto para el ajuste de hiperparámetros como para la evaluación del rendimiento de generalización.\nIlustraremos esta diferencia usando el dataset de cáncer de mama.\nfrom sklearn.datasets import load_breast_cancer X, y = load_breast_cancer(return_X_y=True) En primer lugar, usaremos GridSearchCV para encontrar los mejores hiperparámetros a través de validación cruzada.\nfrom sklearn.model_selection import GridSearchCV from sklearn.svm import SVC param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [.01, .1]} model_to_tune = SVC() search = GridSearchCV( estimator=model_to_tune, param_grid=param_grid, n_jobs=-1 ) search.fit(X, y) GridSearchCV(estimator=SVC(), n_jobs=-1, param_grid={'C': [0.1, 1, 10], 'gamma': [0.01, 0.1]})  Recordemos que, internamente, GridSearchCV entrena varios modelos para cada conjunto de entrenamiento submuestreado y los evalúa cada uno de ellos en los conjuntos de prueba correspondientes usando validación cruzada. Este procedimiento de evaluación se controla a través del parámetro cv. El proceso se repite para todas las posibles combinaciones de parámetros dados en param_grid.\nEl atributo best_params_ proporciona el mejor conjunto de parámetros que maximizan la puntuación media en los conjuntos de prueba internos.\nprint(f\"Los mejores parámetros encontrados son: {search.best_params_}\") Los mejores parámetros encontrados son: {'C': 0.1, 'gamma': 0.01}  También podemos mostrar la puntuación media obtenida usando los parámetros best_params_.\nprint(f\"La media de puntuación CV del mejor modelo es: {search.best_score_:.3f}\") La media de puntuación CV del mejor modelo es: 0.627  En esta fase, debemos ser extremadamente cuidadosos al usar esta puntuación. La malinterpretación podría ser la siguiente: dado que esta puntuación media se ha calculado usando conjuntos de prueba de validación cruzada, podemos usarla para evaluar el rendimiento de generalización del modelo entrenado con los mejores hiperparámetros.\nSin embargo, no debemos olvidar que usamos esta puntuación para seleccionar el mejor modelo. Lo que significa que usamos el conocimiento de los conjuntos de prueba (es decir, las puntuaciones de prueba) para seleccionar los hiperparámetros del modelo en sí mismos.\nPor lo tanto, esta puntuación media no es una estimación justa de nuestro error de prueba. De hecho, puede ser demasiado optimista, en particular cuando ejecutamos una búsqueda de parámetros de una lista grande con muchos hiperparámetros y muchos posibles valores de los mismos. Una forma de evitar este escollo es usar una validación cruzada anidada.\nA continuación, usaremos una validación cruzada interna correspondiente al procedimiento anterior para optimizar únicamente los hiperparámetros. También incluiremos este procedimiento de tunning dentro de una validación cruzada externa, la cual se dedicará a estimar el error de prueba de nuestro modelo tuneado.\nEn este caso, nuestra validación cruzada interna siempre obtiene el conjunto de entrenamiento de la validación cruzada externa, lo que hace posible calcular siempre las puntuaciones de prueba finales en conjuntos de muestras completamente independientes.\nfrom sklearn.model_selection import cross_val_score, KFold # Declara las estrategias de validación cruzada interna y externa inner_cv = KFold(n_splits=5, shuffle=True, random_state=0) outer_cv = KFold(n_splits=3, shuffle=True, random_state=0) # Validación cruzada interna pra la búsqueda de parámetros model = GridSearchCV( estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1 ) # Validación cruzada externa para calcular la puntuación de prueba test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=-1) print(f\"La puntuación media usando CV anidada es: \" f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\") La puntuación media usando CV anidada es: 0.627 +/- 0.014  La puntuación reportada es más confiable y deber estar cerca del rendimiento de generalización esperado en producción. Hay que tener en cuenta que, en este caso, las dos puntuaciones son muy parecidas para este primer intento.\nNos gustaría mejorar la evaluación de la diferencia entre las puntuaciones de validación cruzada anidada y no anidada para mostrar que esta última puede ser demasiado optimista en la práctica. Para hacer esto, repetimos el experimento varias veces y mezclamos los datos de forma diferente para asegurar que nuestras conclusiones no dependen de un muestreo particular de los datos.\ntest_score_not_nested = [] test_score_nested = [] N_TRIALS = 20 for i in range(N_TRIALS): # Para cada intento, usamos divisiones de validación cruzada en # datos mezclados aleatoriamente de forma independiente pasando # distintos valores al parámetro random_state inner_cv = KFold(n_splits=5, shuffle=True, random_state=i) outer_cv = KFold(n_splits=3, shuffle=True, random_state=i) # Búsqueda de parámetros no anidada y puntuación model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1) model.fit(X, y) test_score_not_nested.append(model.best_score_) # Validación cruzada anidada con optimización de parámetros test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=-1) test_score_nested.append(test_score.mean()) Podemos unir los datos juntos y hacer un box plot de las dos estrategias.\nimport pandas as pd all_scores = { \"Not nested CV\": test_score_not_nested, \"Nested CV\": test_score_nested, } all_scores = pd.DataFrame(all_scores) import matplotlib.pyplot as plt color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"} all_scores.plot.box(color=color, vert=False) plt.xlabel(\"Precisión\") _ = plt.title(\"Comparación de precisión media obtenida en el conjunto de prueba con\\n\" \"y sin validación cruzada anidada\") Observamos que el rendimiento de generalización estimado sin usar CV anidada es mayor que el que obtenemos con CV anidada. La razón es que el procedimiento de tuneado en sí mismo selecciona el modelo con la mayor puntuación de CV interna. Si hay muchas combinaciones de hiperparámetros y si las puntuaciones de CV interna tienen comparativamente grandes desviaciones estándar, tomar el valor máximo puede atraer al científico de datos novato a sobreestimar el verdadero rendimiento de generalización del resultado del procedimiento de aprendizaje completo. Usar un procedimiento de validación cruzada externo proporciona una estimación más confiable del rendimiento de generalización en el procedimiento de aprendizaje completo, incluido el efecto de ajuste de hiperparámetros.\nComo conclusión, cuando se optimizan partes de un pipeline de machine learning (por ejemplo, hiperparámetros, transformadores, etc.), se necesita usar validación cruzada anidada para evaluar el rendimiento de generalización del modelo predictivo. De lo contrario, los resultados obtenidos sin validación cruzada anidada suelen ser demasiado optimistas.\nMétricas de clasificación Los modelos de machine learning se basan en optimizar una función objetivo, buscando su mínimo o máximo. Es importante comprender que esta función objetivo suele estar desacoplada de la métrica de evaluación que queremos optimizar en la práctica. La función objetivo sirve como un proxy de la métrica de evaluación. Por tanto, vamos a presentar las diferentes métricas de evaluación usadas en machine learning.\nAhora vamos a proporcionar una panorámica de las métricas de clasificación que se pueden usar para evaluar el rendimiento de generalización de un modelo predictivo. Recordemos que en un problema de clasificación, el vector objetivo es categórico, en lugar de continuo.\nCargaremos el dataset de trasfusiones de sangre.\nimport pandas as pd blood_transfusion = pd.read_csv(\"../data/blood_transfusion.csv\") X = blood_transfusion.drop(columns=\"Class\") y = blood_transfusion[\"Class\"] Comencemos comprobando las clases presentes en el vector objetivo y.\nimport matplotlib.pyplot as plt y.value_counts().plot.barh() plt.xlabel(\"Número de muestras\") _ = plt.title(\"Número de muestras por clases presentes\\nen el objetivo\") Podemos ver que el vector y contiene dos clases correspondientes a si una persona donó sangre. Usaremos un clasificador de regresión logística para predecir este resultado.\nPara centrarnos en la presentación de métricas, solo usaremos una única división en lugar de validación cruzada.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, shuffle=True, random_state=0, test_size=0.5) Usaremos una regresión logística como modelo de base. Entrenaremos el modelo en el conjunto de entrenamiento y, después, usaremos el conjunto de prueba para calcular las diferentes métricas de clasificación.\nfrom sklearn.linear_model import LogisticRegression classifier = LogisticRegression() classifier.fit(X_train, y_train) LogisticRegression()  Predicciones del clasificador Antes de entrar en detalles respecto a las métricas, recordemos qué tipo de predicciones puede proporcionar un clasificador.\nPor esta razón, crearemos una muestra sintética para un nuevo donante potencial: él/ella donó sangre dos veces en el pasado (1000 cc cada vez). La última vez fue hace 6 meses y la primera hace 20 meses.\nnew_donor = pd.DataFrame( { \"Recency\": [6], \"Frequency\": [2], \"Monetary\": [1000], \"Time\": [20], } ) Podemos obtener la clase predicha por el clasificador llamando al método predict.\nclassifier.predict(new_donor) array(['not donated'], dtype=object)  Con esta información, nuestro clasificador predice que este sujeto sintético es más probable que no done sangre otra vez.\nSin embargo, no podemos comprobar que la predicción es correcta (no sabemos el valor objetivo verdadero). Este es el propósito del conjunto de prueba. Primero, predecimos si un sujeto donará sangre con la ayuda del clasificador entrenado.\ny_pred = classifier.predict(X_test) y_pred[:5] array(['not donated', 'not donated', 'not donated', 'not donated', 'donated'], dtype=object)  Precisión como línea base Ahora que tenemos estas predicciones, podemos compararlas con las predicciones reales (también denominadas verdaderas) que no usamos hasta ahora.\ny_test == y_pred 258 True 521 False 14 False 31 False 505 True ... 665 True 100 False 422 True 615 True 743 True Name: Class, Length: 374, dtype: bool  En la comparación anterior, True significa que el valor predicho por nuestra clasificador es idéntico al valor real, mientras que False significa que nuestro clasificador ha cometido un error. Una forma de obtener una tasa general que represente el rendimiento de generalización de nuestro clasificador podría ser calcular cuántas veces es correcto nuestro clasificador dividido por el número de muestras de nuestro conjunto.\nimport numpy as np print(f\"{np.mean(y_test == y_pred):.3f}\") 0.778  Esta medida se denomina precisión (accuracy). En este caso, nuestro clasificador tiene un 78% de precisión al clasificar si un sujeto donará sangre. scikit-learn provee una función que calcula esta métrica en el módulo sklearn.metrics.\nfrom sklearn.metrics import accuracy_score accuracy = accuracy_score(y_test, y_pred) print(f\"Accuracy: {accuracy:.3f}\") Accuracy: 0.778  LogisticRegression también tiene un método denominado score (que es parte de la API estándar de scikit-learn) que calcula la puntuación de accuracy.\naccuracy = classifier.score(X_test, y_test) print(f\"Accuracy: {accuracy:.3f}\") Accuracy: 0.778  Matriz de confusión y métricas asociadas La comparación que hicimos anteriormente y la precisión que calculamos no tienen en cuenta el tipo de error que nuestro clasificador está cometiendo. La accuracy es una agregación de los errores cometidos por el clasificador. Es posible que nos interese una granularidad, saber de forma independiente si los errores son por los siguientes casos:\n predecimos que una persona donará sangre pero no lo hace; predecimos que una persona no donará sangre pero lo hace.  from sklearn.metrics import ConfusionMatrixDisplay _ = ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test) Los números en diagonal se refieren a las predicciones que fueron correctas, mientras que los números fuera de la diagonal se refieren a las predicciones incorrectas (mal clasisificadas). Ahora conocemos los cuatro tipos de predicciones correctas y erróneas:\n la esquina superior izquierda son los verdaderos positivos (TP - true positives) y corresponden a las personas que donaron sangre y se predijeron como tal por el clasificador; la esquina inferior derecha son los verdaderos negativos (TN - true negatives) y corresponden a las personas que no donaron sangre y se predijeron como tal por el clasificador; la esquina superior derecha se corresponde con los falsos negativos (FN - false negatives) y corresponden a las personas que donaron sangre pero se predijo que no habían donado sangre; la esquina inferior izquierda son los falsos positivos (FP - false positives) y corresponden a las personas que no donaron sangre pero se predijo que sí lo hicieron.  Una vez tenemos dividida esta información, podemos calcular métricas para destacar el rendimiento de generalización de nuestro clasificador en una configuración particular. Por ejemplo, podríamos estar interesados en la fracción de personas que realmente donaron sangre cuando el clasificador lo predijo así o en la fracción de personas que se predijo que habrían donado sangre de la población total que realmente lo hizo.\nLa métrica anterior, conocida como precisión (precision), se define como TP / (TP + FP) y representa la probabilidad de que la persona realmente haya donado sangre cuando el clasificador predijo que lo haría. El último, conocido como sensibilidad (recall), se define como TP / (TP + FN) y evalúa cómo de bien el clasificador es capaz de identificar correctamente a las personas que donaron sangre. Al igual que con la accuracy, podríamos calcular estos valores. Sin embargo, scikit-learn proporciona funciones para calcular estos estadísticos.\nfrom sklearn.metrics import precision_score, recall_score precision = precision_score(y_test, y_pred, pos_label=\"donated\") recall = recall_score(y_test, y_pred, pos_label=\"donated\") print(f\"Puntuación precisión: {precision:.3f}\") print(f\"Puntuación sensibilidad: {recall:.3f}\") Puntuación precisión: 0.688 Puntuación sensibilidad: 0.124  Estos resultados están en línea con que hemos visto en la matriz de confusión. Mirando en la columna de la izquierda, más de la mitad de las predicciones “donated” fueron correctas, lo que nos lleva a una precisión superior al 0.5. Sin embargo, nuestro clasificador etiquetó erroneamente a muchas personas que donaron sangre como “not donated”, lo que nos lleva a una baja sensibilidad de aproximadamente 0.1.\nEl problema del desequilibrio de clases En esta fase, podríamos plantearnos una pregunta razonable. Si bien la accuracy no parecía mala (es decir, 77%), la sensibilidad es relativamente baja (es decir, 12%). Como mencionábamos, la precisión y la sensibilidad solo se centran en muestras predichas como positivas. Además, no observamos la proporción de clases. Podríamos comprobar esta proporción en el conjunto de entrenamiento.\ny_train.value_counts(normalize=True).plot.barh() plt.xlabel(\"Frecuencia de clase\") _ = plt.title(\"Frecuencia de clase en el conjunto de entrenamiento\") Observamos que la clase positiva, donated, comprende únicamente el 24% de las muestras. La buena accuracy de nuestro clasificador esta ligada entonces a su habilidad de predecir correctamente la clase negativa not donated, que puede o no ser relevante, dependiendo de la aplicación. Podemos ilustrar este problema usando un clasificado dummy como línea base.\nfrom sklearn.dummy import DummyClassifier dummy_classifier = DummyClassifier(strategy=\"most_frequent\") dummy_classifier.fit(X_train, y_train) print(f\"Accuracy del clasificador dummy: \" f\"{dummy_classifier.score(X_test, y_test):.3f}\") Accuracy del clasificador dummy: 0.762  Con el clasificador dummy, que siempre predice la clase más frecuente, en nuestro caso la clase not donated, obtenemos un accuracy del 76%. Por tanto, significa que este clasificador, sin aprender nada de los datos X, es capaz de predecir más precisamente que nuestro modelo de regresión logística.\nEste problema es también conocido como el problema de desequilibrio de clases. Cuando las clases están desbalanceadas, no se debe usar accuracy. En este caso, debemos usar la precisión y la sensibilidad como presentamos anteriormente o la puntuación accuracy equilibrada en lugar de la accuracy.\nfrom sklearn.metrics import balanced_accuracy_score balanced_accuracy = balanced_accuracy_score(y_test, y_pred) print(f\"Accuracy balanceada: {balanced_accuracy:.3f}\") Accuracy balanceada: 0.553  La accuracy balanceada es equivalente a la accuracy en un contexto de clases equilibradas. Se define como la sensibilidad media obtenida en cada clase.\nEvaluación y diferentes umbrales de probabilidad Todas las estadísticas que hemos presentado hasta ahora se basan en classifier.predict que devuelve la etiqueta más probable. No hemos hecho uso de la probabilidad asociada con esta predicción, la cual proporciona la confianza del clasificador en esta predicción. Por defecto, la predicción de un clasificador corresponde a un umbral de 0.5 de probabilidad en un problema de clasificación binaria. Podemos comprobar rápidamente esta relación con el clasificador que entrenamos.\ntarget_proba_predicted = pd.DataFrame(classifier.predict_proba(X_test), columns=classifier.classes_) target_proba_predicted[:5]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  donated not donated     0 0.271820 0.728180   1 0.451764 0.548236   2 0.445211 0.554789   3 0.441577 0.558423   4 0.870583 0.129417     y_pred = classifier.predict(X_test) y_pred[:5] array(['not donated', 'not donated', 'not donated', 'not donated', 'donated'], dtype=object)  Dado que las probabilidades suman 1, podemos obtener la clase con la mayor probabilidad sin usar el umbral de 0.5.\nequivalence_pred_proba = ( target_proba_predicted.idxmax(axis=1).to_numpy() == y_pred) np.all(equivalence_pred_proba) True  El umbral de decisión predeterminado (0.5) puede no ser el mejor umbral que conduce al rendimiento de generalización óptimo de nuestro clasificador. En este caso, podemos variar el umbral de decisión y, por lo tanto, la predicción subsiguiente y calcular las mismas estadísticas presentadas anteriormente. Normalmente, las dos métricas, sensibilidad y precisión, son calculadas y dibujadas en un gráfico. Cada métrica dibujada en un eje del gráfico y cada punto del gráfico corresponde a un umbral de decisión específico. Empecemos calculando la curva precision-recall.\nfrom sklearn.metrics import PrecisionRecallDisplay disp = PrecisionRecallDisplay.from_estimator( classifier, X_test, y_test, pos_label='donated', marker=\"+\" ) plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = disp.ax_.set_title(\"Curva precision-recall\") En esta curva, cada cruz azul corresponde a un nivel de probabilidad que usamos como umbral de decisión. Podemos ver que variando este umbral de decisión obtenemos diferentes valores de precisión vs sensibilidad.\nUn clasificador perfecto tendría una precisión de 1 para todos los valores de sensibilidad. Una métrica que caracteriza la curva está referida al área bajo la curva (AUC - area under the curve) y se denomina precisión media (AP). Con un clasificador ideal, la precisión media sería 1.\nLas métricas de precisión y sensibilidad se centran en la clase positiva, sin embargo, podríamos estar interesados en el compromiso entre discriminar con precisión la clase positiva y discriminar con precisión las clases negativas. Las estadísticas usadas para esto son la sensibilidad y la especificidad. La especificidad mide la proporción de muestras clasificadas correctamente en la clase negativa y se define como: TN / (TN + FP). De forma similar a la curva precisión-sensibilidad, la sensibilidad y la especificidad se dibujan generalmente con una curva denominada ROC (receiver operating characteristic). Esta sería la curva ROC:\nfrom sklearn.metrics import RocCurveDisplay disp = RocCurveDisplay.from_estimator( classifier, X_test, y_test, pos_label='donated', marker=\"+\") disp = RocCurveDisplay.from_estimator( dummy_classifier, X_test, y_test, pos_label='donated', color=\"tab:orange\", linestyle=\"--\", ax=disp.ax_) plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = disp.ax_.set_title(\"Curva ROC AUC\") Esta curva se construyó usando el mismo principio que la curva precisión-sensibilidad: variamos el umbral de probabilidad para determinar la predicción “dura” y calculamos las métricas. Al igual que la curva precisión-sensibilidad, podemos calcular el área bajo la ROC (ROC-AUC) para caracterizar el rendimiento de generalización de nuestro clasificador. Sin embargo, es importante observar que el límite inferior de ROC-AUC es 0.5. De hecho, mostramos el rendimiento de generalización de un clasificador dummy (la linea discontinua naranja) para mostrar que incluso el peor rendimiento de generalización obtenido estará por encima de esta línea.\nEjercicio métricas de clasificación Anteriormente hemos presentado diferentes métricas de clasificación pero no las usamos con validación cruzada. En este ejercicio practicaremos e implementaremos validación cruzada.\nVolveremos a usar el dataset de transfusiones de sangre.\nimport pandas as pd blood_transfusion = pd.read_csv(\"../data/blood_transfusion.csv\") X = blood_transfusion.drop(columns=\"Class\") y = blood_transfusion[\"Class\"] En primer lugar, vamos a crear un árbol de decisión clasificador.\nfrom sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier() Vamos a crear un objeto de validación cruzada StratifiedKFold. Después lo usaremos dentro de la función cross_val_score para evaluar el árbol de decisión. Primero usaremos la accuracy para evaluar el árbol de decisión. Usaremos explícitamente el parámetro scoring de cross_val_score para calcula la accuracy (aunque sea ésta la puntuación por defecto). Compruebe su documentación para aprender cómo hacerlo.\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score cv = StratifiedKFold(n_splits=10) scores = cross_val_score(tree, X, y, scoring=\"accuracy\", cv=cv, n_jobs=-1) print(f\"Puntuación accuracy: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación accuracy: 0.631 +/- 0.141  Repetiremos el experimento calculando la balanced_accuracy.\nscores_balanced_accuracy = cross_val_score(tree, X, y, scoring=\"balanced_accuracy\", cv=cv, n_jobs=-1) print(f\"Puntuación accuracy balanceada: \" f\"{scores_balanced_accuracy.mean():.3f} +/- {scores_balanced_accuracy.std():.3f}\") Puntuación accuracy balanceada: 0.503 +/- 0.114  Ahora añadiremos un poco de complejidad. Queremos calcular la precisión de nuestro modelo. Sin embargo, anteriormente vimos que necesitamos mencionar la etiqueta positiva, que en nuestro caso consideramos que es la clase donated.\nMostraremos que calcular la precisión sin suministrar la clase positiva no es soportado por scikit-learn porque, de hecho, es ambigüo.\ntry: scores = cross_val_score(tree, X, y, cv=10, scoring=\"precision\") except ValueError as exc: print(exc) C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score scores = scorer(estimator, X_test, y_test) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__ score = scorer._score(cached_call, estimator, *args, **kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 264, in _score return self._sign * self._score_func(y_true, y_pred, **self._kwargs) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1757, in precision_score p, _, _, _ = precision_recall_fscore_support( File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1544, in precision_recall_fscore_support labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) File \"C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1356, in _check_set_wise_labels raise ValueError( ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated'] warnings.warn(  Obtenemos una excepción, porque el scorer por defecto tiene su etiqueta positiva marcada como uno (pos_label=1), que no es nuestro caso (nuestra etiqueta positiva es donated). En este caso, necesitamos crear un scorer usando la función scoring y la función helper make_scorer.\nPor tanto, importamos sklearn.metrics.make_scorer y sklearn.metrics.precision_score. Consulta la documentación para más información. Finalmente, creamos un scorer llamando a make_scorer usando la función de puntuación precision_score y pasándole el parámetro extra pos_label=\"donated\".\nfrom sklearn.metrics import make_scorer, precision_score precision = make_scorer(precision_score, pos_label=\"donated\") Ahora, en lugar de suministrar la cadena \"precision\" al parámetro scoring en la llamada cross_val_score, le pasamos el scorer que acabamos de crear.\nscores_precision = cross_val_score(tree, X, y, cv=10, scoring=precision) print(f\"Puntuación precisión: \" f\"{scores_precision.mean():.3f} +/- {scores_precision.std():.3f}\") Puntuación precisión: 0.246 +/- 0.170  cross_val_score solo calculará un única puntuación proporcionada al parámetro scoring. La función cross_validate permite el cálculo de múltiples puntuaciones pasándole una lista de cadenas o scorer al parámetro scoring, lo que podría ser útil.\nImportaremos sklearn.model_selection.cross_validate y calcularemos la accuracy y la accuracy balanceada a través de validación cruzada. Dibujaremos la puntuación de validación cruzada para ambas métricas usando un box plot.\nfrom sklearn.model_selection import cross_validate scores = cross_validate(tree, X, y, scoring=[\"accuracy\", \"balanced_accuracy\"], cv=cv, n_jobs=-1) scores {'fit_time': array([0.00250149, 0.00250244, 0.00300288, 0.00250196, 0.00250196, 0.00250149, 0.00350356, 0.00350428, 0.0035038 , 0.00400329]), 'score_time': array([0.00200152, 0.00250149, 0.00450349, 0.00200224, 0.00250244, 0.00250363, 0.00300264, 0.00200105, 0.00250173, 0.002002 ]), 'test_accuracy': array([0.29333333, 0.53333333, 0.77333333, 0.56 , 0.58666667, 0.68 , 0.68 , 0.78666667, 0.64864865, 0.74324324]), 'test_balanced_accuracy': array([0.42105263, 0.48391813, 0.66081871, 0.40643275, 0.42397661, 0.44736842, 0.54239766, 0.74561404, 0.4623323 , 0.50309598])}  print(f\"Puntuación accuracy: \" f\"{scores['test_accuracy'].mean():.3f} +/- {scores['test_accuracy'].std():.3f}\") Puntuación accuracy: 0.629 +/- 0.139  print(f\"Puntuación accuracy balanceada: \" f\"{scores['test_balanced_accuracy'].mean():.3f} +/- {scores['test_balanced_accuracy'].std():.3f}\") Puntuación accuracy balanceada: 0.510 +/- 0.106  all_scores = { \"Accuracy\": scores['test_accuracy'], \"Accucary balanceada\": scores['test_balanced_accuracy'], } all_scores = pd.DataFrame(all_scores) color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"} all_scores.plot.box(color=color, vert=False) plt.xlabel(\"Puntuación\") _ = plt.title(\"Comparación accuracy vs accuracy balanceada\") Métricas de regresión Vamos a presentar las métricas que pueden usarse en regresión. Existen un conjunto de métricas dedicadas a la regresión. De hecho, las métricas de clasificación no pueden usarse para evaluar el rendimiento de generalización de modelos de regresión porque hay una diferencia fundamental entre sus tipos de objetivos: es una variable continua en el caso de la regresión, mientras que en el caso de la clasificación es una variable discreta.\nUsaremos el dataset de viviendas de Ames. El objetivo es predecir el precio de las propiedades en la ciudad de Ames, Iowa. Al igual que con la clasificación, solo usaremos una única división entrenamiento-prueba para enfocarnos únicamente en las métricas de regresión.\nimport pandas as pd import numpy as np ames_housing = pd.read_csv(\"../data/house_prices.csv\") X = ames_housing.drop(columns=\"SalePrice\") y = ames_housing[\"SalePrice\"] X = X.select_dtypes(np.number) y /= 1000 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, shuffle=True, random_state=0 ) Algunos modelos de machine learning están diseñados para resolverse como un problema de optimización: minimizan un error (también conocida como función de pérdida) usando un conjunto de entrenamiento. Una función de pérdida básica que se usa en regresión es el error cuadrático medio (MSE). Por lo tanto, esta métrica se usa a veces para evaluar el modelo dado que ya está optimizada por este modelo.\nVeamos un ejemplo usando un modelo de regresión lineal.\nfrom sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error regressor = LinearRegression() regressor.fit(X_train, y_train) y_pred = regressor.predict(X_train) print(f\"MSE en el conjunto de entrenamiento: \" f\"{mean_squared_error(y_train, y_pred):.3f}\") MSE en el conjunto de entrenamiento: 996.902  Nuestro modelo de regresión lineal está minimizando el error cuadrático medio en el conjunto de entrenamiento. Significa que no existe otro conjunto de coeficientes que reduzcan más el error.\nVamos a calcular el MSE en el conjunto de prueba.\ny_pred = regressor.predict(X_test) print(f\"MSE en el conjunto de prueba: \" f\"{mean_squared_error(y_test, y_pred):.3f}\") MSE en el conjunto de prueba: 2064.736  El MSE en bruto puede ser difícil de interpretar. Una forma es reescalar el MSE por la varianza del objetivo. Esta puntuación se conoce como $R^2$, también conocida como coeficiente de determinación. De hecho, esta es la puntuación por defecto que usa scikit-learn cuando se llama al método score.\nregressor.score(X_test, y_test) print(f\"R2 en el conjunto de prueba: \" f\"{regressor.score(X_test, y_test):.3f}\") R2 en el conjunto de prueba: 0.687  La puntuación $R^2$ representa la proporción de varianza del objetivo que es explicada por las variables independientes del modelo. La mejor puntuación posible es 1 pero no existe límite inferior. Sin embargo, un modelo que predice el valor esperado del objetivo obtendría una puntación de 0.\nfrom sklearn.dummy import DummyRegressor dummy_regressor = DummyRegressor(strategy=\"mean\") dummy_regressor.fit(X_train, y_train) print(f\"R2 para un regresor que predice la media:\" f\"{dummy_regressor.score(X_test, y_test):.3f}\") R2 para un regresor que predice la media:-0.000  La puntuación $R^2$ nos da ideas de la calidad del ajuste del modelo. Sin embargo, esta puntuación no se puede comparar de un dataset a otro y el valor obtenido no tiene una interpretación significativa en relación a la unidad original del objetivo. Si buscamos obtener una puntuación interpretable, nos interesaría la mediana o el error absoluto medio.\nfrom sklearn.metrics import mean_absolute_error y_pred = regressor.predict(X_test) print(f\"MAE: \" f\"{mean_absolute_error(y_test, y_pred):.3f} k$\") MAE: 22.608 k$  Al calcular el error absoluto medio, podemos interpretar que nuestro modelo está prediciendo en promedio con un márgen de 22.6 k$ respecto al valor verdadero de la propiedad. Una desventaja de esta métrica es que la media se puede ver impactada por errores grandes. Para algunas aplicaciones, es posible que no queramos que estos grandes errores tengan una gran influencia en nuestra métrica. En este caso, podemos usar el error absoluto mediano.\nfrom sklearn.metrics import median_absolute_error print(f\"Error absoluto mediano: \" f\"{median_absolute_error(y_test, y_pred):.3f} k$\") Error absoluto mediano: 14.137 k$  El error absoluto medio (o el error absoluto mediano) aún sigue teniendo una limitación conocida: cometer un error de 50 k dólares en una casa valorada en 50 k dólares tiene el mismo impacto que cometerlo en una casa de 500 k dólares. De hecho, el error absoluto medio no es relativo.\nEl error porcentual absoluto medio introduce este escalado relativo.\nfrom sklearn.metrics import mean_absolute_percentage_error print(f\"Error porcentual absoluto medio: \" f\"{mean_absolute_percentage_error(y_test, y_pred) * 100:.3f} %\") Error porcentual absoluto medio: 13.574 %  Además de métricas, podemos representar visualmente los resultados dibujando los valores predichos vs los valores reales.\npredicted_actual = { \"Valores reales (k$)\": y_test, \"Valores predichos (k$)\": y_pred} predicted_actual = pd.DataFrame(predicted_actual) import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(data=predicted_actual, x=\"Valores reales (k$)\", y=\"Valores predichos (k$)\", color=\"black\", alpha=0.5) plt.axline((0, 0), slope=1, label=\"Perfect fit\") plt.axis('square') plt.legend() _ = plt.title(\"Regresión usando un modelo sin \\ntransformación de objetivo\") En este gráfico, las predicciones correctas estarían en la línea diagonal. Este gráfico permite detectar si el modelo comete errores de forma consistente, es decir, si tiene algún sesgo (bias).\nEn este gráfico vemos que para los valores reales altos, nuestro modelo tiende a subestimar el precio de la propiedad. Normalmente, este problema surge cuando el objetivo a predecir no sigue una distribución normal. En este caso, el modelo se podría beneficiar de la trasformación de objetivo.\nfrom sklearn.preprocessing import QuantileTransformer from sklearn.compose import TransformedTargetRegressor transformer = QuantileTransformer( n_quantiles=900, output_distribution=\"normal\") model_transformed_target = TransformedTargetRegressor( regressor=regressor, transformer=transformer) model_transformed_target.fit(X_train, y_train) y_pred = model_transformed_target.predict(X_test) predicted_actual = { \"Valores reales (k$)\": y_test, \"Valores predichos (k$)\": y_pred} predicted_actual = pd.DataFrame(predicted_actual) sns.scatterplot(data=predicted_actual, x=\"Valores reales (k$)\", y=\"Valores predichos (k$)\", color=\"black\", alpha=0.5) plt.axline((0, 0), slope=1, label=\"Perfect fit\") plt.axis('square') plt.legend() _ = plt.title(\"Regresión usando un modelo que \\ntransforma el objetivo antes de entrenar\") Así, vemos que una vez trasformado el objetivo, vemos que corregimos algunos de los valores altos.\nEjercicio de métricas de regresión Vamos a evaluar las métricas de regresión dentro de una validación cruzada para familiarizarnos con la sintaxis. Usaremos el dataset de viviendas de Ames.\names_housing = pd.read_csv(\"../data/house_prices.csv\") X = ames_housing.drop(columns=\"SalePrice\") y = ames_housing[\"SalePrice\"] X = X.select_dtypes(np.number) y /= 1000 El primer paso es crear un modelo de regresión lineal.\nfrom sklearn.linear_model import LinearRegression model = LinearRegression() Después, usaremos cross_val_score para estimar el rendimiento de generalización del modelo. Usaremos una validación cruzada KFold con 10 particiones. Haremos uso de la puntuación $R^2$ asignando explícitamente el parámetro scoring (a pesar de que es la puntuación por defecto).\nfrom sklearn.model_selection import cross_val_score, KFold cv = KFold(n_splits=10) scores = cross_val_score(model, X, y, scoring=\"r2\", cv=cv, n_jobs=-1) print(f\"R2: {scores.mean():.3f} +/- {scores.std():.3f}\") R2: 0.794 +/- 0.103  Ahora, en lugar de usar la puntuación $R^2$, usaremos el error absoluto medio.\nscores = cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, n_jobs=-1) print(f\"MAE: {-scores.mean():.3f} +/- {-scores.std():.3f}\") MAE: 21.892 +/- -2.225  En scikit-learn, el parámetro scoring espera puntuaciones. Esto significa que cuanto mayor sean los valores y menores sean los errores, mejor será el modelo. Por lo tanto, el error debe multiplicarse por -1. Esta es la causa del prefijo neg_ de la cadena del scoring cuando tratamos con métricas que son errores.\nPor último, usaremos la función cross_validate y calcularemos múltiples puntuaciones/errores a la vez pasándole una lista de marcadores al parámetros scoring. Calcularemos la puntuación $R^2$ y el error absoluto medio.\nfrom sklearn.model_selection import cross_validate scores = cross_validate(model, X, y, scoring=[\"r2\", \"neg_mean_absolute_error\"], cv=cv, n_jobs=-1) print(f\"R2: {scores['test_r2'].mean():.3f} +/- {scores['test_r2'].std():.3f}\") print(f\"MAE: {-scores['test_neg_mean_absolute_error'].mean():.3f} +/- \" f\"{-scores['test_neg_mean_absolute_error'].std():.3f}\") R2: 0.794 +/- 0.103 MAE: 21.892 +/- -2.225  scores {'fit_time': array([0.00350285, 0.00300288, 0.00350332, 0.00350285, 0.00350308, 0.00350261, 0.00400424, 0.00350285, 0.00300217, 0.00300312]), 'score_time': array([0.00200248, 0.0025022 , 0.00200105, 0.00250173, 0.00200129, 0.00200224, 0.00200129, 0.00200176, 0.00150156, 0.00150156]), 'test_r2': array([0.84390289, 0.85497435, 0.88752303, 0.74951104, 0.81698014, 0.82013355, 0.81554085, 0.81452472, 0.50115778, 0.83330693]), 'test_neg_mean_absolute_error': array([-20.48049905, -21.38003105, -21.26831487, -22.86887664, -24.79955736, -18.95827641, -20.11793792, -20.5040172 , -26.76774564, -21.77871056])}  Ejercicio Vamos a poner en práctica lo aprendido en este post con un ejercicio. Para ello usaremos el dataset de bicis.\nimport pandas as pd cycling = pd.read_csv(\"../data/bike_rides.csv\", index_col=0, parse_dates=True) cycling.index.name = \"\" target_name = \"power\" X, y = cycling.drop(columns=target_name), cycling[target_name] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  heart-rate cadence speed acceleration slope             2020-08-18 14:43:19 102.0 64.0 4.325 0.0880 -0.033870   2020-08-18 14:43:20 103.0 64.0 4.336 0.0842 -0.033571   2020-08-18 14:43:21 105.0 66.0 4.409 0.0234 -0.033223   2020-08-18 14:43:22 106.0 66.0 4.445 0.0016 -0.032908   2020-08-18 14:43:23 106.0 67.0 4.441 0.1144 0.000000     En este dataset, el problema es intentar predecir la potencia de un ciclista usando sensores baratos (GPS, monitores de frecuencia cardíaca, etc.). De hecho, la potencia se puede registrar a través de medidores ciclistas de potencia, pero dichos dispositivos suelen ser bastante caros.\nEn lugar de usar machine learning a ciegas, primero presentaremos un poco de mecánica clásica: la segunda ley de Newton.\n$P_{meca} = (\\frac{1}{2} \\rho . SC_x . V_{a}^{2} + C_r . mg . \\cos \\alpha + mg . \\sin \\alpha + ma) V_d$\ndonde $\\rho$ es la densidad del aire en kg.m$^{-3}$, $S$ es la superficie frontal del ciclista en m$^{2}$, $C_x$ es el coeficiente de resistencia, $V_a$ es la velocidad del aire en m.s$^{-1}$, $C_r$ es la resistencia a la rodadura, $m$ es la masa del ciclista y la bicicleta en kg, $g$ es la aceleración estándar debida a la gravedad, que es igual a 9.81 m.s$^{-2}$, $\\alpha$ es la pendiente en radianes, $V_d$ es la velocidad del ciclista m.s$^{-1}$, y $a$ es la aceleración del ciclista en m.s$^{-2}$.\nAl principio, esta ecuación podría parecer un poco compleja pero podemos explicar con palabras lo que significan los diferéntes términos dentro del paréntesis:\n el primer término es la potencia que se requiere que produzca un ciclista para luchar contra el viento el segundo término es la potencia que se requiere que produzca un ciclista para luchar contrar la resistencia a la rodadura creada por los neumáticos en la pista el tercer término es la potencia que se requiere que produzca un ciclista para subir una colina si la pendiente es positiva. Si la pendiente es negativa, el ciclista no necesita producir ninguna potencia para avanzar el cuarto y último término es la potencia que se requiere que produzca un ciclista para cambiar su velocidad (es decir, aceleración).  Podemos simplificar el modelo anterior usando los datos que tenemos a mano. Se vería como lo siguiente:\n$P_{meca} = \\beta_{1} V_{d}^{3} + \\beta_{2} V_{d} + \\beta_{3} \\sin(\\alpha) V_{d} + \\beta_{4} a V_{d}$\nEste modelo está más cerca de lo que vimos anteriormente: es un modelo lineal entrenado en una transformación de features no lineal. Construiremos, entrenaremos y evaluaremos un modelo de este tipo como parte del ejercicio. Por tanto, necesitaremos:\n crear una nueva matriz de datos conteniendo el cubo de la velocidad, la velocidad, la velocidad multiplicada por el seno del ángulo de la pendiente y la velocidad multiplicada por la aceleración. Para calcular el ángulo de la pendiente, necesitamos tomar el arcotangente de la pendiente (alpha = np.arctan(slope)). Además, podemos limitarnos a la aceleración positiva solo recortando a 0 los valores de aceleración negativa (correspondería a alguna potencia creada por el frenado que no estamos modelando aquí). usando la nueva matriz de datos, crear un modelo predictivo lineal basado en un sklearn.preprocessing.StandardScaler y sklearn.linear_model.RidgeCV. usar un estrategia de validación cruzada sklearn.model_selection.ShuffleSplit con solo 4 particiones (n_splits=4) para evaluar el rendimiento de generalización del modelo. Usaremos el error absoluto medio (MAE) como métrica del rendimiento de generalización. Además, pasaremos el parámetro return_estimator=True y return_train_score=True. Tengamos en cuenta que la estrategia ShuffleSplit es un estrategia ingenua y simple e investigaremos las consecuencias de esta elección más adelante.  X[\"speed_3\"] = X[\"speed\"] ** 3 X[\"speed_x_sin_slope\"] = X[\"speed\"] * np.sin(np.arctan(X[\"slope\"])) X[\"speed_x_accel\"] = X[\"speed\"] * X[\"acceleration\"].clip(lower=0) X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  heart-rate cadence speed acceleration slope speed_3 speed_x_sin_slope speed_x_accel                2020-08-18 14:43:19 102.0 64.0 4.325 0.0880 -0.033870 80.901828 -0.146402 0.380600   2020-08-18 14:43:20 103.0 64.0 4.336 0.0842 -0.033571 81.520685 -0.145482 0.365091   2020-08-18 14:43:21 105.0 66.0 4.409 0.0234 -0.033223 85.707790 -0.146398 0.103171   2020-08-18 14:43:22 106.0 66.0 4.445 0.0016 -0.032908 87.824421 -0.146198 0.007112   2020-08-18 14:43:23 106.0 67.0 4.441 0.1144 0.000000 87.587538 0.000000 0.508050     print(f\"{X['speed_x_sin_slope'].mean():.3f}\") -0.003  features = [\"speed_3\", \"speed\", \"speed_x_sin_slope\", \"speed_x_accel\"] X_linear_model = X[features].copy() X_linear_model.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  speed_3 speed speed_x_sin_slope speed_x_accel            2020-08-18 14:43:19 80.901828 4.325 -0.146402 0.380600   2020-08-18 14:43:20 81.520685 4.336 -0.145482 0.365091   2020-08-18 14:43:21 85.707790 4.409 -0.146398 0.103171   2020-08-18 14:43:22 87.824421 4.445 -0.146198 0.007112   2020-08-18 14:43:23 87.587538 4.441 0.000000 0.508050     from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.model_selection import ShuffleSplit, cross_validate from sklearn.linear_model import RidgeCV model = make_pipeline(StandardScaler(), RidgeCV()) cv = ShuffleSplit(n_splits=4, random_state=0) cv_scores = cross_validate(model, X_linear_model, y, scoring=\"neg_mean_absolute_error\", return_estimator=True, return_train_score=True, cv=cv, n_jobs=-1) cv_scores {'fit_time': array([0.02552152, 0.02652311, 0.02452087, 0.02452159]), 'score_time': array([0.00150347, 0.002002 , 0.00200343, 0.00150061]), 'estimator': [Pipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV(alphas=array([ 0.1, 1. , 10. ])))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV(alphas=array([ 0.1, 1. , 10. ])))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV(alphas=array([ 0.1, 1. , 10. ])))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('ridgecv', RidgeCV(alphas=array([ 0.1, 1. , 10. ])))])], 'test_score': array([-73.23006461, -72.1311734 , -72.89061823, -71.2370263 ]), 'train_score': array([-72.35634493, -72.51703894, -72.42974777, -72.6121094 ])}  ¿De media, cuál es el error absoluto medio en el conjunto de prueba obtenido en la validación cruzada?\nprint(f\"MAE es: {-cv_scores['test_score'].mean():.2f} +/- \" f\"{cv_scores['test_score'].std():.2f}\") MAE es: 72.37 +/- 0.77  Muestra los coeficientes del modelo lineal resultado de la validación cruzada\nfor estimator in cv_scores[\"estimator\"]: print(estimator[-1].coef_) [ 5.66427806 32.84904152 80.08105928 10.85618779] [ 5.68897463 32.83434375 80.99005594 11.34425 ] [ 6.28736152 32.19112942 80.92397865 11.23297157] [ 6.18278112 32.4035743 80.61344415 11.27427489]  import matplotlib.pyplot as plt coefs = [ estimator[-1].coef_ for estimator in cv_scores[\"estimator\"]] coefs = pd.DataFrame(coefs, columns=features) ax = coefs.plot.box(vert=False) ax.axvline(0, -1, 1, linestyle=\"--\") _ = plt.title(\"Distribución de pesos del modelo lineal\") Todos los pesos son mayores que 0. No es una sorpresa dado que los coeficientes están relacionados con productos de cantidades físicas positivas tales como la masa del ciclista y la bicicleta, gravedad, densidad del aire, etc. De hecho, juntando la primera ecuación y el valor de $\\beta_{S}$ esperaríamos una relación que podría ser cercana a:\n $\\beta_{1} \\frac{1}{2} \\rho . SC_x$ $\\beta_{2} C_r . mg$ $\\beta_{3} mg$ $\\beta_{4} ma$  Esta relación también explicaría por qué podríamos esperar que $\\beta_{1} \u003c \\beta_{2} \u003c \\beta_{3}$. De hecho, $C_r$ es una constante pequeña, por lo que podríamos esperar que $\\beta_{2} \u003c \\beta_{3} . \\rho . SC_x$ son valores muy pequeños en comparación con $ C_r$ o $ mg $ y uno podría esperar $\\beta_{1} \u003c \\beta_{2}$.\nAhora crearemos un modelo predictivo que use todos los datos, incluyendo las medidas de los sensores disponibles, tales como la cadencia (velocidad a la que un ciclista gira los pedales medida en rotaciones por minuto) y frecuencia cardíaca (número de pulsaciones por minuto del ciclista mientras durante el ejercicio). También, usaremos un regresor no lineal, un sklearn.ensemble.HistGradientBoostingRegressor. Estableceremos el número máximo de iteraciones en 1000 (max_iter=1_000) y activaremos la parada temprana (early_stopping=True). Repetiremos la evaluación anterior usando este regresor.\nDe media, ¿cuál es el error absoluto medio en el conjunto de prueba obtenido a través de la validación cruzada?\nX = cycling.drop(columns=target_name) X[\"speed_3\"] = X[\"speed\"] ** 3 X[\"speed_x_sin_slope\"] = X[\"speed\"] * np.sin(np.arctan(X[\"slope\"])) X[\"speed_x_accel\"] = X[\"speed\"] * X[\"acceleration\"].clip(lower=0) X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  heart-rate cadence speed acceleration slope speed_3 speed_x_sin_slope speed_x_accel                2020-08-18 14:43:19 102.0 64.0 4.325 0.0880 -0.033870 80.901828 -0.146402 0.380600   2020-08-18 14:43:20 103.0 64.0 4.336 0.0842 -0.033571 81.520685 -0.145482 0.365091   2020-08-18 14:43:21 105.0 66.0 4.409 0.0234 -0.033223 85.707790 -0.146398 0.103171   2020-08-18 14:43:22 106.0 66.0 4.445 0.0016 -0.032908 87.824421 -0.146198 0.007112   2020-08-18 14:43:23 106.0 67.0 4.441 0.1144 0.000000 87.587538 0.000000 0.508050     from sklearn.ensemble import HistGradientBoostingRegressor model = make_pipeline(StandardScaler(), HistGradientBoostingRegressor(max_iter=1_000, early_stopping=True)) cv = ShuffleSplit(n_splits=4, random_state=0) cv_scores = cross_validate(model, X, y, scoring=\"neg_mean_absolute_error\", return_estimator=True, return_train_score=True, cv=cv, n_jobs=-1) cv_scores {'fit_time': array([1.1509881 , 1.16600084, 1.37017584, 0.85473323]), 'score_time': array([0.04653955, 0.04804134, 0.05254531, 0.03503013]), 'estimator': [Pipeline(steps=[('standardscaler', StandardScaler()), ('histgradientboostingregressor', HistGradientBoostingRegressor(early_stopping=True, max_iter=1000))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('histgradientboostingregressor', HistGradientBoostingRegressor(early_stopping=True, max_iter=1000))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('histgradientboostingregressor', HistGradientBoostingRegressor(early_stopping=True, max_iter=1000))]), Pipeline(steps=[('standardscaler', StandardScaler()), ('histgradientboostingregressor', HistGradientBoostingRegressor(early_stopping=True, max_iter=1000))])], 'test_score': array([-44.6157647 , -43.98685225, -43.78464147, -43.55871723]), 'train_score': array([-40.40936232, -40.42975609, -39.87099469, -41.43133008])}  print(f\"MAE es: {-cv_scores['test_score'].mean():.2f} +/- \" f\"{cv_scores['test_score'].std():.2f}\") MAE es: 43.99 +/- 0.39  De media, el MAE de este modelo en el conjunto de prueba es de ~44 vatios. Por lo tanto, parece que las features adicionales y el cambio de regresor tienen un impacto positivo en el rendimiento de generalización.\nComparando el modelo lineal y el modelo histogram gradient boosting y teniendo en cuenta el MAE de entrenamiento y prueba obtenidos a través de validación cruzada, ¿qué podemos concluir?\nRevisando las puntuaciones de entrenamiento y prueba de cada uno de los modelos, podemos concluir que:\n el rendimiento de generalización del histogram gradient boosting está limitado por su overfitting. El error de prueba es mayor que el error de entrenamiento. Esto es síntoma de un modelo con overfitting. el rendimiento de generalización del modelo lineal está limitado por su underfitting. El error de entrenamiento y prueba son muy parecidos. Sin embargo, los errores son mucho mayores que los del histogram gradient boosting. El modelo linean tiene claramente underfitting.  En la validación cruzada anterior, optamos por la opción de usar una estrategia ShuffleSplit de validación cruzada. Lo que significa que las muestras seleccionadas aleatoriamente se seleccionaron como conjunto de prueba, ignorando cualquier dependencia entre las líneas del dataframe.\nNos gustaría tener una estrategia de validación cruzada que evalúe la capacidad del nuestro modelo para predecir un recorrido completamente nuevo en bicicleta: las muestras del conjunto de validación sólo deben provenir de recorridos no presentes en el conjunto de entrenamiento.\n¿Cuántos recorridos en bicicleta están almacenados en el dataframe? Ayuda: Podemos comprobar los días únicos en DatetimeIndex (el índice del dataframe). De hecho, podemos asumir que en un día dado, el ciclista salió como máximo una vez al día. También podemos acceder a la fecha y hora de un DatatimeIndex usando df.index.date y df.index.time, respectivamente.\nprint(f\"Nº de recorridos: {len(set(X.index.date))}\") Nº de recorridos: 4  # otra forma print(f\"Nº de recorridos: {len(np.unique(X.index.date))}\") Nº de recorridos: 4  En lugar de usar una estrategia simple SuffleSplit, usaremos una estrategia que tenga en cuenta los grupos definidos por cada fecha individual. Corresponde a un recorrido en bicicleta. Nos gustaría tener una estrategia de validación cruzada que evalúe la capacidad de nuestro modelo para predecir en un recorrido completamente nuevo: las muestras en el conjunto de validación solo deben provenir de recorrides no presentes en el conjunto de entrenamiento. Por lo tanto, podemos usar una estrategia LeaveOneGroupOut: en cada iteración de validación cruzada mantendremos un recorrido para la evaluación y usaremos los otros recorridos para entrenar nuestro modelo.\nPor tanto, necesitamos:\n crear una variable llamda group que es un array unidimensional de numpy que contiene los índices de cada recorrido presente en el dataframe. Por tanto, la longitud de group será igual al número de muestras del dataframe. Si tuviéramos 2 recoridos, esperaríamos los índices 0 y 1 en group para diferenciar los recorridos. Podemos usar pd.factorize para codificar cualquier tipo de Python en índices enteros. crear un objeto de validación cruzada llamado cv usando la estrategia sklearn.model_selection.LeaveOneGroupOut. evaluar tanto el modelo lineal como el histogram gradient boosting con esta estrategia.  Usando esta evaluación y observando los errores de entrenamiento y prueba de ambos modelos, ¿qué podemos concluir?\nfrom sklearn.model_selection import LeaveOneGroupOut #groups = pd.factorize(np.unique(X.index.date)) groups, _ = pd.factorize(X.index.date) cv = LeaveOneGroupOut() model_linear = RidgeCV() cv_scores_lineal_regression = cross_validate( model_linear, X_linear_model, y, scoring=\"neg_mean_absolute_error\", groups=groups, return_estimator=True, return_train_score=True, cv=cv, n_jobs=-1 ) model_hgbr = HistGradientBoostingRegressor(max_iter=1_000, early_stopping=True) cv_scores_hgbr = cross_validate( model_hgbr, X, y, scoring=\"neg_mean_absolute_error\", groups=groups, return_estimator=True, return_train_score=True, cv=cv, n_jobs=-1 ) -cv_scores_lineal_regression[\"train_score\"], -cv_scores_lineal_regression[\"test_score\"] (array([72.43262484, 72.28760557, 68.96581013, 75.31143178]), array([72.44057575, 73.32215829, 81.30511116, 64.9905063 ]))  -cv_scores_hgbr[\"train_score\"], -cv_scores_hgbr[\"test_score\"] (array([39.46262019, 40.26826242, 38.58251339, 40.43820514]), array([47.74637053, 47.94907269, 53.96158726, 47.46269937]))  Revisando las puntuaciones de entrenamiento y prueba de cada uno de los modelos, podemos concluir que observamos el mismo comporamiento que con la estrategia ShuffleSplit, es decir:\n el rendimiento de generalización del histogram gradient boosting está limitado por su overfitting. el rendimiento de generalización del modelo lineal está limitado por su underfitting.  También observamos que histogram gradient boosting tiene claramente más overfitting con la estrategia de validación cruzada LeaveOneGroupOut, ya que la diferencia en las puntaciones de entrenamiento y prueba es aún mayor que la medida con la estrategia ShuffleSplit.\nPor lo tanto, incluso si el modelo lineal está modelando el problema físico real, es probable que adolezca de deficiencias impuestas por las mediciones faltantes (por ejemplo, velocidad del viento) y por la incertidumbre de algunas medidas (por ejemplo, errores de GPS). Como resultado, el modelo lineal no es tan preciso como el regresor histogram gradient boosting que tiene acceso a mediciones externas. La información adicional, tales como la frecuencia cardíaca y la cadencia, atenúan las anteriores deficiencias.\nEn este caso, no podemos comparar las puntuaciones de validación cruzada partición a partición, ya que las particiones no están alineadas (no han sido generadas con la misma estrategia). En su lugar, comparemos la media de los errores de prueba de validación cruzadda en las evaluaciones del model lineal. ¿Qué podemos concluir respecto al error de prueba medio cuando usamos ShuffleSplit en comparación con LeaveOneGroupOut?\nprint(f\"MAE es: {-cv_scores_lineal_regression['test_score'].mean():.2f} +/- \" f\"{cv_scores_lineal_regression['test_score'].std():.2f}\") MAE es: 73.01 +/- 5.78  El MAE del modelo lineal con estrategia ShuffleSplit era de 72.37, luego la diferencia es mínima, es decir, ambas estrategias de validación cruzada son equivalentes.\nComparemos la media de los errores de prueba de validación cruzada en las evaluaciones del modelo gradient boosting. ¿Qué podemos concluir respecto al error de prueba medio cuando usamos ShuffleSplit en comparación con LeaveOneGroupOut?\nprint(f\"MAE es: {-cv_scores_hgbr['test_score'].mean():.2f} +/- \" f\"{cv_scores_hgbr['test_score'].std():.2f}\") MAE es: 49.28 +/- 2.71  El MAE del modelo gradient boosting con estrategia ShuffleSplit era de 44.22, luego la diferencia es menor por más de 3 vatios, es decir, ShuffleSplit proporciona resultados demasiado optimistas. La estrategia LeaveOneGroupOut está más próxima a la configuración real encontrada cuando se pone el modelo en producción, por lo que podemos concluir que el rendimiento de generalización proporcionado por la estrategia de validación cruzada ShuffleSplit es demasiado optimista al estimar un error de prueba medio menor que el que observaríamos en recorridos futuros.\nEs interesante notar que no vemos este comportamiento en el modelo lineal. La razón es que nuestro modelo lineal tiene underfitting.\nAhora entraremos en más detalle seleccionando un único recorrido para la prueba y analizando las predicciones del modelo para este recorrido de prueba. Para hacer esto, podemos reutilizar el objeto de validación cruzada LeaveOneGroupOut de la siguiente forma:\ncv = LeaveOneGroupOut() train_indices, test_indices = list(cv.split(X, y, groups=groups))[0] X_linear_model_train = X_linear_model.iloc[train_indices] X_linear_model_test = X_linear_model.iloc[test_indices] X_train = X.iloc[train_indices] X_test = X.iloc[test_indices] y_train = y.iloc[train_indices] y_test = y.iloc[test_indices] Ahora, ajustaremos tanto el modelo lineal como el modelo de histogram gradient boosting en los datos de entrenamiento y recopilaremos las predicciones en los datos de prueba. Haremos un scatter plot donde en el eje x dibujaremos las potencias medidas (objetivo real) y en el eje y dibujaremos las potencias predichas (objetivo predicho). Haremos dos gráficos separados para cada modelo. ¿Qué conclusiones podemos obtener?\nfrom sklearn.metrics import mean_absolute_error model_linear.fit(X_train, y_train) y_pred = model_linear.predict(X_test) print(f\"MAE: \" f\"{mean_absolute_error(y_test, y_pred):.3f} k$\") MAE: 61.469 k$  predicted_actual = { \"Potencias reales (Vatios)\": y_test, \"Potencias predichas (Vatios)\": y_pred} predicted_actual = pd.DataFrame(predicted_actual) plt.figure(figsize=(6, 6)) sns.scatterplot(data=predicted_actual, x=\"Potencias reales (Vatios)\", y=\"Potencias predichas (Vatios)\", color=\"black\", alpha=0.5) #plt.axline((0, 0), slope=1, label=\"Perfect fit\") plt.plot([0, 800], [0, 800], color=\"black\", linestyle=\"--\", label=\"Perfect fit\") plt.axis('square') plt.xlim(-300, 1000) plt.ylim(-300, 1000) plt.legend() _ = plt.title(\"Regresión modelo lineal\") from sklearn.metrics import mean_absolute_error model_hgbr.fit(X_train, y_train) y_pred = model_hgbr.predict(X_test) print(f\"MAE: \" f\"{mean_absolute_error(y_test, y_pred):.3f} k$\") MAE: 47.833 k$  predicted_actual = { \"Potencias reales (Vatios)\": y_test, \"Potencias predichas (Vatios)\": y_pred} predicted_actual = pd.DataFrame(predicted_actual) plt.figure(figsize=(6, 6)) sns.scatterplot(data=predicted_actual, x=\"Potencias reales (Vatios)\", y=\"Potencias predichas (Vatios)\", color=\"black\", alpha=0.5) #plt.axline((0, 0), slope=1, label=\"Perfect fit\") plt.plot([0, 800], [0, 800], color=\"black\", linestyle=\"--\", label=\"Perfect fit\") plt.axis('square') plt.xlim(-300, 1000) plt.ylim(-300, 1000) plt.legend() _ = plt.title(\"Regresión model histogram gradient boosting\") Comencemos mirando las muestras con potencias altas. Vemos que tanto el modelo lineal como el regresor histogram gradient boosting predicen siempre potencias por debajo de las mediciones reales. Las muestras de altas potencias corresponden con esfuerzos en sprints. Al medir la velocidad y la aceleración, existe una especie de retardo para observar este cambio mientras que medir directamente la potencia aplicada en los pedales no se ve afectado por este problema.\nPor otro lado, vemos que el modelo lineal predice una potencia negativa catastrófica para muestras con una potencia de 0 vatios. Este se debe a nuestro modelado. De hecho, la potencia basada en el cambio de energía cinética (potencia necesaria para acelerar o desacelerar) está produciendo tales artefactos. No estamos modelando la pérdida de potencia introducida por la disipación de calor cuando los frenos reducen la velocidad de la bicicleta (para tomar una curva, por ejemplo) y, por tanto, obtenemos valores sin sentido para muestras con baja potencia. El regresor histogram gradient boosting usa en su lugar la cadencia, ya que 0 rpm (revoluciones por minuto) se corresponderá con 0 vatios producidos.\nAhora seleccionaremos una porción de los datos de prueba usando el siguiente código:\ntime_slice = slice(\"2020-08-18 17:00:00\", \"2020-08-18 17:05:00\") X_test_linear_model_subset = X_linear_model_test[time_slice] X_test_subset = X_test[time_slice] y_test_subset = y_test[time_slice] Permite seleccionar datos desde la 5.00 pm hasta las 5.05 pm. Usaremos los anteriores modelos ya entrenados (lineal y grandient boosting) para predecir en esta porción de los datos de prueba. Dibujaremos en el mismo gráfico los datos reales y las predicciones de cada modelo. ¿Qué conclusiones podemos obtener?\ny_pred_linear_model = model_linear.predict(X_test_subset) print(f\"MAE modelo lineal: \" f\"{mean_absolute_error(y_test_subset, y_pred_linear_model):.3f} k$\") y_pred_hgbr = model_hgbr.predict(X_test_subset) print(f\"MAE hgbr: \" f\"{mean_absolute_error(y_test_subset, y_pred_hgbr):.3f} k$\") MAE modelo lineal: 61.359 k$ MAE hgbr: 52.386 k$  ax = y_test_subset.plot(label=\"Objetivo real\", figsize=(10, 6)) ax.plot(y_test_subset.index, y_pred_linear_model, label=\"Modelo lineal\") ax.plot(y_test_subset.index, y_pred_hgbr, label=\"Modelo HGBR\") ax.set_ylabel(\"Potencia (Vatios)\") plt.legend() _ = plt.title(\"comparación del objetivo real y predicciones de ambos modelos\") Vemos que el modelo lineal tiene predicciones que están más alejadas del objetivo real que las predicciones del regresor histogram gradient boosting. Mientras que histogram gradient boosting es capaz de hacer cambios abruptos de potencia, el modelo lineal es incapaz de predecir dichos cambios y necesita tiempo para generar el nivel de potencia real.\nUna vez mas, el rendimiento comparativamente malo del modelo de regresión lineal entrenado en las features físicamente significativas no se deriva necesariamente de errores en la ingeniería de características sino que podría de errores de medición que impiden una estimación precisa de cambios pequeños en la velocidad y aceleración.\nEl modelo gradient boosting sería capar de solucionar estas limitaciones de los errores de GPS a través de mediciones más precisas de frecuencia cardíaca y cadencia.\n","description":"","tags":["dummy","baseline","kfold","stratification","estratificación","ShuffleSplit","accuracy","precision","recall","AUC","ROC AUC","sensibilidad","especificidad","R2"],"title":"Evaluación del rendimiento de modelos","uri":"/posts/evaluacion-modelos/"},{"categories":["tutoriales"],"content":"En este post veremos en detalle los algoritmos que combinan varios modelos juntos, también llamados conjunto de modelos. Presentaremos dos familias de estas técnicas: (i) basados en bootstrapping y (ii) basados en boosting. Presentaremos bagging y árboles aleatorios como pertenecientes a la primera estrategia y AdaBoost y árboles de decisión gradient boosting que pertenecen a la última estrategia. Finalmente, hablaremos sobre los hiperparámetros que permiten afinar estos modelos y compararlos entre modelos.\nEjemplo introductorio a los modelos de conjunto En este ejemplo queremos enfatizar los beneficios de los métodos de conjunto sobre los modelos simples (por ejemplo, árbol de decisión, modelo lineal, etc.). Combinar modelos simples produce modelos más poderosos y robustos con menos problemas.\nEmpezaremos cargando el dataset de propiedades de California. El objetivo de este dataset es prededir el valor medio de la vivienda en algunos distritos de California basándonos en datos geográficos y demográficos.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(as_frame=True, return_X_y=True) y *= 100 # reescala el objetivo en k$ Vamos a comprobar el rendimiento de generalización de un árbol de decisión regresor con los parámetros por defecto.\nfrom sklearn.model_selection import cross_validate from sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(random_state=0) cv_results = cross_validate(tree, X, y, n_jobs=-1) scores = cv_results[\"test_score\"] print(f\"Puntuación R2 obtenida por validación cruzada: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación R2 obtenida por validación cruzada: 0.354 +/- 0.087  Obtenemos resultados justos. Sin embargo, ya sabemos que este modelo necesita ser afinado. De hecho, los parámetros por defecto no conducen necesariamente a un árbol de decisión óptimo. En lugar de usar los valores por defecto, debemos buscar a través de validación cruzada el valor óptimo de parámetros importantes, como son max_depth, min_samples_split o min_samples_leaf.\nRecordemos que necesitamos ajustar esos parámetros, ya que los árboles de decisión adolecen de overfitting si aumenta la profundidad del árbol, pero no existen reglas de cómo se debe configurar cada parámetro. Por tanto, no realizar una búsqueda nos llevaría a tener un modelo con underfitting u overfitting.\nAhora vamos a hacer un grid search para afinar los hiperparámetros que hemos mencionado anteriormente.\n%%time from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeRegressor param_grid = { \"max_depth\": [5, 8, None], \"min_samples_split\": [2, 10, 30, 50], \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]} cv = 3 tree = GridSearchCV(DecisionTreeRegressor(random_state=0), param_grid=param_grid, cv=cv, n_jobs=-1) cv_results = cross_validate(tree, X, y, n_jobs=-1, return_estimator=True) scores = cv_results[\"test_score\"] print(f\"Puntuación R2 obtenida por validación cruzada: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación R2 obtenida por validación cruzada: 0.523 +/- 0.107 CPU times: total: 15.6 ms Wall time: 4.01 s  Vemos que optimizar los hiperparámetros tiene un efecto positivo en el rendimiento de generalización. Sin embargo, conlleva un alto coste computacional.\nPodemos crear un dataframe almacenando la información importante recopilada durante el ajuste de hiperparámetros e investigar los resultados.\nAhora usaremos un metodo de conjunto llamado bagging. Brevemente, este método usará un regresor de base (por ejemplo, árboles de decisión regresores) y entrenará varios de ellos en una versión ligeramente modificada del conjunto de entrenamiento. Luego, las predicciones de todos esos regresores base se combinarán promediando.\nAquí, usaremos 20 árboles de decisión y comprobaremos el tiempo de entrenamiento, así como el rendimiento de generalización en los datos de prueba externos. Es importante notar que no vamos a ajustar ningún parámetro del árbol de decisión.\n%%time from sklearn.ensemble import BaggingRegressor base_estimator = DecisionTreeRegressor(random_state=0) bagging_regressor = BaggingRegressor( base_estimator=base_estimator, n_estimators=20, random_state=0) cv_results = cross_validate(bagging_regressor, X, y, n_jobs=-1) scores = cv_results[\"test_score\"] print(f\"Puntuación R2 obtenida por validación cruzada: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación R2 obtenida por validación cruzada: 0.642 +/- 0.083 CPU times: total: 31.2 ms Wall time: 3.32 s  Sin realizar la búsqueda de los hiperparámetros óptimos, el rendimiento de generalización global del regresor bagging es mucho mejor que un único árbol de decisión. Además, el coste computacional se reduce en comparación a la búsqueda de hiperparámetros óptimos.\nEsto muestra la motivación existente detrás del uso de métodos de conjunto: proporcionan una relativamente buena baseline con un rendimiento de generalización decente sin ningún ajuste de hiperparámetros.\nA continuación, veremos en detalle dos familias de conjunto: bagging y boosting:\n conjunto usando bootstrap (por ejemplo, bagging y bosques aleatorios); conjunto usando boosting (por ejemplo, boosting adaptativo y álboles de decisión gradient-boosting).  Métodos de conjuntos usando bootstrapping Bagging Vamos a introducir una estrategia muy natural de construir conjuntos de modelos de machine learning llamado “bagging”.\n“Bagging” proviene de Bootstrap AGGregatING. Utiliza remuestreo bootstrap (muestreo aleatorio con reemplazo) para aprender varios modelos en variaciones aleatorias del conjunto de entrenamiento. A la hora de predecir, se agregan las predicciones de cada aprendiz para obtener las predicciones finales.\nEn primer lugar, vamos a generar un dataset sintético simple para obtener información de bootstraping.\nimport pandas as pd import numpy as np rng = np.random.RandomState(1) def generate_data(n_samples=30): x_min, x_max = -3, 3 x = rng.uniform(x_min, x_max, size=n_samples) noise = 4.0 * rng.randn(n_samples) y = x ** 3 - 0.5 * (x + 1) ** 2 + noise y /= y.std() data_train = pd.DataFrame(x, columns=[\"Feature\"]) data_test = pd.DataFrame( np.linspace(x_max, x_min, num=300), columns=[\"Feature\"]) target_train = pd.Series(y, name=\"Target\") return data_train, data_test, target_train import matplotlib.pyplot as plt import seaborn as sns X_train, X_test, y_train = generate_data(n_samples=30) sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) _ = plt.title(\"Dataset de regresión sintético\") La relación entre nuestras features y el objetivo a predecir es no lineal. Sin embargo, un árbol de decisión es capaz de aproximar tal dependencia no lineal:\ntree = DecisionTreeRegressor(max_depth=3, random_state=0) tree.fit(X_train, y_train) y_pred = tree.predict(X_test) En este caso, el término “test” se refiere a datos que no se han usado previamente para entrenar y calcular una métrica de evaluación en un conjunto de prueba de este tipo no tendría sentido.\nsns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) plt.plot(X_test[\"Feature\"], y_pred, label=\"Arbol entrenado\") plt.legend() _ = plt.title(\"Predicciones de un único árbol de decisión\") Veamos cómo podemos usar bootstraping para entrenar varios árboles.\nBootstrap resampling Una muestra bootstrap corresponde a un remuestreo con reemplazo del dataset original, una muestra que tiene el mismo tamaño que el dataset original. Por lo tanto, la muestra bootstrap contendrá varias veces algunos puntos de datos, mientras que algunos de los puntos de datos originales no estarán presentes.\nCrearemos una función que, dados X e y, devolverá una variación remuestreada X_boostrap e y_boostrap.\ndef bootstrap_sample(data, target): # indices corresponde a un muestreo con reemplazo del misma tamaño de muestra # que los datos originales bootstrap_indices = rng.choice( np.arange(target.shape[0]), size=target.shape[0], replace=True, ) data_bootstrap = data.iloc[bootstrap_indices] target_bootstrap = target.iloc[bootstrap_indices] return data_bootstrap, target_bootstrap Vamos a generar 3 muestras bootstrap y verificaremos cualitativamente la diferencia con el dataset original.\nn_bootstraps = 3 for bootstrap_idx in range(n_bootstraps): X_bootstrap, y_bootstrap = bootstrap_sample( X_train, y_train ) plt.figure() plt.scatter(X_bootstrap[\"Feature\"], y_bootstrap, color=\"tab:blue\", facecolor=\"none\", alpha=0.5, label=\"X remuestreado\", s=180, linewidth=5) plt.scatter(X_train[\"Feature\"], y_train, color=\"black\", label=\"X original\", s=60) plt.title(f\"X remuestrado #{bootstrap_idx}\") plt.legend() Observemos que las tres variaciones comparten puntos comunes con el dataset original. Algunos puntos son remuestreados aleatoriamente varias veces y aparecen como círculos azul oscuro.\nLas tres muestras bootstrap generadas son diferentes del dataset original y entre cada una de ellas. Para confirmar esta intuición, podemos comprobar el número de muestras únicas en las muestras bootstrap.\nX_train_huge, X_test_huge, y_train_huge = generate_data(n_samples=100_000) X_bootstrap_sample, y_bootstrap_sample = bootstrap_sample( X_train_huge, y_train_huge) ratio_unique_sample = (np.unique(X_bootstrap_sample).size / X_bootstrap_sample.size) print( f\"Porcentaje de muestras presentes en el dataset original: \" f\"{ratio_unique_sample * 100:.1f}%\" ) Porcentaje de muestras presentes en el dataset original: 63.2%  En promedio, un 63.2% de los puntos de datos del dataset original estarán presentes en una muestra bootstrap dada. El otro 36.8% son muestras repetidas. Somos capaces de generar muchos datasets, todos ligeramente diferentes.\nAhora podemos entrenar un árbol de decisión para cada uno de esos datasets y todos ellos serán también ligeramente diferentes.\nbag_of_trees = [] for bootstrap_idx in range(n_bootstraps): tree = DecisionTreeRegressor(max_depth=3, random_state=0) X_bootstrap_sample, y_bootstrap_sample = bootstrap_sample( X_train, y_train) tree.fit(X_bootstrap_sample, y_bootstrap_sample) bag_of_trees.append(tree) Ahora que podemos crear una bolsa de árboles diferentes, podemos usar cada uno de los tres árboles para predecir las muestras dentro del rango de los datos. Ellos proporcionarán predicciones ligeramente diferentes.\nsns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) for tree_idx, tree in enumerate(bag_of_trees): y_pred = tree.predict(X_test) plt.plot(X_test[\"Feature\"], y_pred, linestyle=\"--\", alpha=0.5, label=f\"Predicciones árbol #{tree_idx}\") plt.legend() _ = plt.title(\"Predicciones de árboles entrenados en diferentes bootstraps\") Agregación Una vez hemos entrenado nuestros árboles somos capaces de obtener predicciones de cada uno de ellos. En regresión, la forma más directa de combinar esas predicciones es promediarlas: para un punto de datos de prueba dado, alimentamos los valores de features de entrada a cada uno de los n models entrenados en el conjunto y, como resultado, calculamos n valores predichos para la variable objetivo. La predicción final del conjunto para los puntos de datos de prueba es la media de esos nvalores.\nPodemos dibujar las predicciones promediadas del ejemplo anterior.\nsns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) bag_predictions = [] for tree_idx, tree in enumerate(bag_of_trees): y_pred = tree.predict(X_test) plt.plot(X_test[\"Feature\"], y_pred, linestyle=\"--\", alpha=0.8, label=f\"Predicciones árbol #{tree_idx}\") bag_predictions.append(y_pred) bag_predictions = np.mean(bag_predictions, axis=0) plt.plot(X_test[\"Feature\"], bag_predictions, label=\"Predicciones medias\", linestyle=\"-\") plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Predicciones de los árboles de la bolsa\") La línea roja continua muestras las predicciones medias, las cuales serán las predicciones finales proporcionadas por nuestra bolsa de árboles de decisión regresores. Nótese que las predicciones de los conjuntos son más estables debido a la operación de promedio. Como resultado, el conjunto de árboles, en global, es menos probable que adolezca de overfitting que los árboles individuales.\nBagging en scikit-learn Scikit-learn implementa el procedimiento de bagging como un “meta-estimador”, que es un estimador que recubre otro estimador: toma un modelo base que es clonado varias veces y entrenado independientemente en cada muestra bootstrap.\nEl siguiente fragmento de código muestra cómo construir un conjunto bagging de árboles de decisión. Establecemos n_estimators=100 en lugar de los 3 que vimos anteriormente en la implementación manual para obtener un efecto de suavizado más fuerte.\nfrom sklearn.ensemble import BaggingRegressor bagged_trees = BaggingRegressor( base_estimator=DecisionTreeRegressor(max_depth=3), n_estimators=100 ) bagged_trees.fit(X_train, y_train) BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=3), n_estimators=100)  Vamos a visualizar las predicciones del conjunto en el mismo intervalo de datos:\nsns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) y_pred = bagged_trees.predict(X_test) plt.plot(X_test[\"Feature\"], y_pred) _ = plt.title(\"Predicciones de un clasificador bagging\") Debido a que hemos usado 100 árboles en el conjunto, la predicción media es ligeramente más suave pero muy similar a la del gráfico anterior. Es posible acceder después del entrenamiento a los modelos internos del conjunto, almacenados como una lista de Python en el atributo bagged_trees.estimators_.\nComparemos las predicciones de los modelos base con sus medias:\nfor tree_idx, tree in enumerate(bagged_trees.estimators_): label = \"Predicciones de árboles individuales\" if tree_idx == 0 else None # convertirmos `X_test` a un array Numpy para evitar el warning de scikit-learn y_pred = tree.predict(X_test.to_numpy()) plt.plot(X_test[\"Feature\"], y_pred, linestyle=\"--\", alpha=0.1, color=\"tab:blue\", label=label) sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) y_pred = bagged_trees.predict(X_test) plt.plot(X_test[\"Feature\"], y_pred, color=\"tab:orange\", label=\"Predicciones del conjunto\") _ = plt.legend() Usamos un valor bajo del parámetro de opacidad alpha para apreciar mejor el solape en las funciones de predicción de los árboles individuales.\nEsta visualización nos da algunas ideas sobre la incertidumbre de las predicciones en diferentes áreas del espacio de features.\nPipelines de bagging complejos Aunque hemos usado un árbol de decisión como modelo de base, nada nos impide usar cualquier otro tipo de modelo.\nComo sabemos que la función de generación de los datos originales es una transformación polinomial ruidosa de la variable de entrada, intentemos entrenar un pipeline de regresión polinomial bagged en este dataset.\nfrom sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler from sklearn.pipeline import make_pipeline polynomial_regressor = make_pipeline( MinMaxScaler(), PolynomialFeatures(degree=4), Ridge(alpha=1e-10), ) El pipeline en primer lugar escala los datos a un rango 0-1 con MinMaxScaler. Después extrae features polinomiales de grado 4. Las features resultantes permanecerán en el rango 0-1 por construcción: si x se encuentra en el rango 0-1, entonces x ** n también se encontrará en el rango 0-1 para cualquier valor de n.\nDespués el pipeline alimenta las features no lineales resultantes a un modelo de regresión lineal regularizado para la predicción final de la variable objetivo.\nNótese que se ha usado intencionadamente un valor pequeño para el parámetro de regularización alpha, ya que esperamos que el conjunto bagging funcione bien con modelos de base con ligero overfitting.\nEl conjunto en sí mismo se construye simplemente pasando el pipeline resultante al parámetro base_estimator de la clase BaggingRegressor:\nbagging = BaggingRegressor( base_estimator=polynomial_regressor, n_estimators=100, random_state=0, ) bagging.fit(X_train, y_train) BaggingRegressor(base_estimator=Pipeline(steps=[('minmaxscaler', MinMaxScaler()), ('polynomialfeatures', PolynomialFeatures(degree=4)), ('ridge', Ridge(alpha=1e-10))]), n_estimators=100, random_state=0)  for i, regressor in enumerate(bagging.estimators_): # convertirmos `X_test` a un array Numpy para evitar el warning de scikit-learn regressor_predictions = regressor.predict(X_test.to_numpy()) base_model_line = plt.plot( X_test[\"Feature\"], regressor_predictions, linestyle=\"--\", alpha=0.2, label=\"Predicciones de los modelos base\" if i == 0 else None, color=\"tab:blue\" ) sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) y_pred = bagging.predict(X_test) plt.plot(X_test[\"Feature\"], y_pred, color=\"tab:orange\", label=\"Predicciones del conjunto\") plt.ylim(y_train.min(), y_train.max()) plt.legend() _ = plt.title(\"Regresión polinómica Bagging\") Las predicciones de este modelo de regresión polinómica bagged parecen cualitativamente mejores que los áboles bagging. Esto era algo esperado dado que el modelo de base refleja mejor nuestro conocimiento del proceso de generación de los datos verdaderos.\nDe nuevo las diferentes opacidades inducidas por el solapamiento de las líneas azules nos permite apreciar la incertidumbre de las predicciones en el conjunto bagged.\nNótese que el procedimiento de bootstrapping es una herramienta genérica de estadística y no está limitada a construir conjuntos de modelos de machine learning. Se puede obtener más detalle sobre este punto en el artículo de Wikipedia sobre bootstrapping.\nEjercicio bagging Vamos a realizar un pequeño ejercicio para practicar lo visto hasta ahora. Queremos investigar si podemos ajustar los hiperparámetros de un regresor bagging y evaluar la ganancia obtenida. Para ello usaremos el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split X, y = fetch_california_housing(as_frame=True, return_X_y=True) y *= 100 # rescale the target in k$ X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=0, test_size=0.5) Vamos a crear un BaggingRegressor y proporcionaremos un DecisionTreeRegressor a su parámetro base_estimator. Entrenaremos el regresor y evaluaremos su rendimiento de generalización en el conjunto de prueba usando el error absoluto medio.\nfrom sklearn.ensemble import BaggingRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_absolute_error bagging = BaggingRegressor( base_estimator=DecisionTreeRegressor()) bagging.fit(X_train, y_train) y_pred = bagging.predict(X_test) print(f\"MAE del regresor bagging básico: \" f\"{mean_absolute_error(y_test, y_pred):.2f} k$\") MAE del regresor bagging básico: 35.97 k$  Ahora crearemos una instancia de RandomizedSearchCV usando el modelo previo y ajustando los parámetros importantes del regresor bagging. Debemos encontrar los mejores parámetros y comprobar que somos capaces de encontrar un conjunto de parámetros que mejoren el regresor por defecto, usando también como métrica el error absoluto medio.\n# Comprobemos cuáles son los parámetros de nuestro regresor bagging bagging.get_params() {'base_estimator__ccp_alpha': 0.0, 'base_estimator__criterion': 'squared_error', 'base_estimator__max_depth': None, 'base_estimator__max_features': None, 'base_estimator__max_leaf_nodes': None, 'base_estimator__min_impurity_decrease': 0.0, 'base_estimator__min_samples_leaf': 1, 'base_estimator__min_samples_split': 2, 'base_estimator__min_weight_fraction_leaf': 0.0, 'base_estimator__random_state': None, 'base_estimator__splitter': 'best', 'base_estimator': DecisionTreeRegressor(), 'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}  from scipy.stats import randint from sklearn.model_selection import RandomizedSearchCV param_grid = { \"n_estimators\": randint(10, 30), \"max_samples\": [0.5, 0.8, 1.0], \"max_features\": [0.5, 0.8, 1.0], \"base_estimator__max_depth\": randint(3, 10), } model = RandomizedSearchCV( bagging, param_grid, n_iter=20, scoring=\"neg_mean_absolute_error\") model.fit(X_train, y_train) RandomizedSearchCV(estimator=BaggingRegressor(base_estimator=DecisionTreeRegressor()), n_iter=20, param_distributions={'base_estimator__max_depth': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4B835A590\u003e, 'max_features': [0.5, 0.8, 1.0], 'max_samples': [0.5, 0.8, 1.0], 'n_estimators': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4B82D9840\u003e}, scoring='neg_mean_absolute_error')  model.best_params_ {'base_estimator__max_depth': 9, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 25}  import pandas as pd cv_results = pd.DataFrame(model.cv_results_) cv_results.sort_values(by=\"mean_test_score\", ascending=False)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time param_base_estimator__max_depth param_max_features param_max_samples param_n_estimators params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score     7 0.455792 0.006091 0.008107 3.745593e-04 9 0.8 0.8 25 {'base_estimator__max_depth': 9, 'max_features... -39.279991 -38.880628 -38.619408 -39.440439 -36.531615 -38.550416 1.050122 1   15 0.320576 0.000928 0.006406 2.000333e-04 8 1.0 0.5 21 {'base_estimator__max_depth': 8, 'max_features... -41.914622 -41.483535 -39.830309 -42.232385 -39.161815 -40.924533 1.208945 2   16 0.184759 0.002748 0.005205 2.456516e-04 7 0.8 0.5 17 {'base_estimator__max_depth': 7, 'max_features... -44.589440 -42.106688 -40.293463 -44.670376 -40.690151 -42.470024 1.863912 3   14 0.282943 0.004392 0.005605 2.004386e-04 7 0.8 0.8 19 {'base_estimator__max_depth': 7, 'max_features... -41.832809 -43.779636 -42.014627 -43.711636 -41.013014 -42.470344 1.094805 4   6 0.257421 0.002090 0.005805 2.449898e-04 7 0.8 0.8 17 {'base_estimator__max_depth': 7, 'max_features... -42.989436 -42.727496 -40.938417 -44.601246 -41.207918 -42.492903 1.327728 5   13 0.262926 0.002443 0.005605 2.003911e-04 7 1.0 0.5 19 {'base_estimator__max_depth': 7, 'max_features... -43.521755 -44.202781 -42.572490 -43.749080 -41.546292 -43.118480 0.949303 6   4 0.222692 0.001049 0.005305 2.453791e-04 6 1.0 0.5 18 {'base_estimator__max_depth': 6, 'max_features... -45.666625 -45.603503 -44.938197 -45.636675 -43.376790 -45.044358 0.876670 7   9 0.195368 0.001602 0.004604 1.995565e-04 6 0.8 0.8 15 {'base_estimator__max_depth': 6, 'max_features... -45.489748 -46.381155 -44.224186 -45.389135 -43.919159 -45.080677 0.898491 8   2 0.259023 0.002806 0.008507 2.861023e-07 9 0.5 0.5 28 {'base_estimator__max_depth': 9, 'max_features... -45.003335 -46.935110 -45.408900 -44.305677 -44.801457 -45.290896 0.895574 9   1 0.330484 0.000813 0.005204 2.451844e-04 6 1.0 1.0 17 {'base_estimator__max_depth': 6, 'max_features... -46.330157 -46.133009 -44.831581 -45.898168 -43.435013 -45.325586 1.077706 10   5 0.330083 0.000679 0.005405 3.748906e-04 6 1.0 1.0 17 {'base_estimator__max_depth': 6, 'max_features... -46.233147 -46.316019 -44.772506 -46.858990 -43.147684 -45.465669 1.349592 11   8 0.110395 0.002184 0.004504 3.234067e-07 9 0.5 0.5 12 {'base_estimator__max_depth': 9, 'max_features... -43.518541 -48.826323 -43.241693 -44.959974 -49.044992 -45.918305 2.532756 12   18 0.190764 0.000861 0.005505 1.507891e-07 5 0.8 0.5 23 {'base_estimator__max_depth': 5, 'max_features... -48.199928 -49.122217 -47.921591 -48.442045 -47.014668 -48.140090 0.689006 13   10 0.225894 0.004134 0.005705 2.448730e-04 5 0.5 1.0 25 {'base_estimator__max_depth': 5, 'max_features... -50.437443 -52.356826 -51.114233 -51.058152 -49.041999 -50.801730 1.078062 14   11 0.079468 0.000970 0.003603 2.003434e-04 5 0.5 0.5 13 {'base_estimator__max_depth': 5, 'max_features... -56.678626 -52.958267 -48.569071 -52.802426 -52.683538 -52.738386 2.567650 15   17 0.091579 0.002170 0.003303 2.452428e-04 5 0.5 1.0 10 {'base_estimator__max_depth': 5, 'max_features... -51.923348 -54.387221 -54.317287 -59.113367 -53.374348 -54.623114 2.414941 16   19 0.192765 0.000200 0.004804 2.449314e-04 3 1.0 0.8 20 {'base_estimator__max_depth': 3, 'max_features... -57.046149 -57.125298 -55.750560 -57.408820 -54.716769 -56.409519 1.021359 17   3 0.209280 0.001498 0.004404 2.003909e-04 3 1.0 1.0 19 {'base_estimator__max_depth': 3, 'max_features... -57.110032 -57.542201 -56.171485 -57.804065 -54.725094 -56.670575 1.119968 18   0 0.192165 0.003165 0.005004 2.611745e-07 3 0.8 0.8 25 {'base_estimator__max_depth': 3, 'max_features... -57.606682 -57.253821 -57.200298 -57.434140 -57.827740 -57.464536 0.231201 19   12 0.093881 0.002114 0.003603 2.001766e-04 3 0.5 1.0 15 {'base_estimator__max_depth': 3, 'max_features... -60.743445 -63.337419 -62.410111 -62.969337 -59.640312 -61.820125 1.405829 20     y_pred = model.predict(X_test) print(f\"MAE del regresor bagging tuneado: \" f\"{mean_absolute_error(y_test, y_pred):.2f} k$\") MAE del regresor bagging tuneado: 37.66 k$  Podemos ver que el predictor proporcionado por el regresor bagging no necesita de demasiado ajuste de hiperparámetros. No se consigue mejora con el tuneado.\nBosque aleatorio Vamos a presentar los modelos random forest y mostraremos las diferencias con los conjuntos bagging. Random forest es un modelo muy popular en machine learning. Es una modificación del algoritmo bagging. En bagging, se puede usar cualquier clasificador o regresor. En random forest, el clasificador o regresor base es siempre un árbol de decisión.\nRandom forest tiene otra particularidad: cuando se entrena un árbol, la búsqueda de la mejor división se realiza solo en un subconjunto de las features originales tomadas al azar. Los subconjuntos aleatorios son diferentes para cada división de nodo. El objetivo es inyectar aleatoriedad adicional en el procedimiento de aprendizaje para intentar decorrelacionar los errores predichos de los árboles individuales.\nPor tanto, random forest usan aleatoriedad en ambos ejes de la matriz de datos:\n en las muestras bootstrapping de cada árbol del bosque; seleccionando aleatoriamente un subconjunto de features en cada nodo del árbol.  Un vistazo a random forests Ilustraremos el uso de un clasificador random forest en el dataset del censo de adultos.\nadult_census = pd.read_csv(\"../data/adult_census.csv\") target_name = \"class\" X = adult_census.drop(columns=[target_name, \"education-num\"]) y = adult_census[target_name] El dataset del censo de adultos contiene algunos datos categóricos y codificaremos las variables categóricas usando un OrdinalEncoder, dado que los modelos basados en árbol pueden trabajar muy eficientemente con esta representación tan simple de variables categóricas.\nDado que existen categorías raras en este dataset, necesitamos especificar la codificación de las categorías desconocidas en tiempo de predicción para que sea capaz de usar validación cruzada. De lo contrario, algunas categorías raras solo podrían estar presentes en el lado de validación de la división de validación cruzada y OrdinalEncoder podría lanzar un error cuando llame a su método transform con los puntos de datos del conjunto de validación.\nfrom sklearn.preprocessing import OrdinalEncoder from sklearn.compose import make_column_transformer, make_column_selector categorial_encoder = OrdinalEncoder( handle_unknown=\"use_encoded_value\", unknown_value=-1 ) preprocessor = make_column_transformer( (categorial_encoder, make_column_selector(dtype_include=object)), remainder=\"passthrough\" ) Haremos primero un ejemplo simple donde entrenaremos un único clasificador de árbol de decisión y comprobaremos su rendimiento de generalización a través de validación cruzada.\nfrom sklearn.pipeline import make_pipeline from sklearn.tree import DecisionTreeClassifier tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=0)) from sklearn.model_selection import cross_val_score scores_tree = cross_val_score(tree, X, y) print(f\"Clasificador árbol decisión: \" f\"{scores_tree.mean():.3f} +/- {scores_tree.std():.3f}\") Clasificador árbol decisión: 0.812 +/- 0.002  De forma similar a como hemos hecho anteriormente, vamos a construir un BaggingClassifier con un clasificador de árbol de decisión como modelo base. Además, necesitamos especificar cuántos modelos queremos combinar. También necesitamos preprocesar los datos y, por tanto, usaremos un pipeline de scikit-learn.\nfrom sklearn.ensemble import BaggingClassifier bagged_trees = make_pipeline( preprocessor, BaggingClassifier( base_estimator=DecisionTreeClassifier(random_state=0), n_estimators=50, n_jobs=-1, random_state=0 ) ) scores_bagged_trees = cross_val_score(bagged_trees, X, y) print(f\"Clasificador arbol decisión bagged: \" f\"{scores_bagged_trees.mean():.3f} +/- \" f\"{scores_bagged_trees.std():.3f}\") Clasificador arbol decisión bagged: 0.853 +/- 0.003  El rendimiento de generalización de los árboles bagged es mucho mejor que el rendimiento de un único árbol.\nAhora vamos a usar random forest. Observaremos que no necesitamos especificar ningún base_estimator porque el estimador se fuerza que sea un árbol de decisión. Por tanto, sólo necesitamos especificar el número deseado de árboles del bosque.\nfrom sklearn.ensemble import RandomForestClassifier random_forest = make_pipeline( preprocessor, RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=0) ) scores_random_forest = cross_val_score(random_forest, X, y) print(f\"Clasificador random forest: \" f\"{scores_random_forest.mean():.3f} +/- \" f\"{scores_random_forest.std():.3f}\") Clasificador random forest: 0.856 +/- 0.002  Parece que el rendimiento de random forest es ligeramente superior a los árboles bagged, posiblemente debido a la selección aleatoria de las variables que decorrelacionan los errores de predicción de los árboles individuales y, en consecuencia, hacen el paso de promediado más eficiente al reducir el overfitting.\nDetalles sobre los hiperparámetros por defecto Para random forest es posible controlar la cantidad de aleatoriedad para cada división estableciendo el valor del hiperparámetro max_features:\n max_features=0.5 significa que se considerarán el 50% de las features en cada división; max_features=1.0 significa que se considerarán todas las features en cada división, lo que realmente deshabilita el submuestreo de features.  Por defecto, RandomForestRegressor deshabilita el submuestreo de features mientras que RandomForestClassifier usa max_features=np.sqrt(n_features). Estos valores por defecto reflejan buenas prácticas dadas en la literatura científica.\nSin embargo, max_features es uno de los hiperparámetros a tener en consideración cuando se ajusta un random forest:\n demasiada aleatoriedad en los árboles puede conducir a modelos base con underfitting y puede ir en detrimento del conjunto en su totalidad, muy poca aleatoriedad en los árboles conduce a mayor correlación de los errores de predicción y, como resultado, a reducir los beneficios del paso de promediado en términos de control del overfitting.  En scikit-learn, las clases bagging también exponen un parámetro max_features. Sin embargo, BaggingClassifier y BaggingRegressor son agnósticos respecto a sus modelos base y, por tanto, el submuestro de features aleatorio solo puede producirse una vez antes de entrenar cada modelo base, en lugar de varias veces por modelo base como es el caso cuando se añaden divisiones a un árbol dado.\nSe pueden resumir estos detalles en la siguiente tabla:\n   Clase modelo de conjunto Clase modelo base Valor por defecto para max_features Estrategia de submuestreo de features     BaggingClassifier especificado por usuario (flexible) n_features (sin submuestreo) Nivel de modelo   RandomForestClassifier DecisionTreeClassifier sqrt(n_features) Nivel de nodo de árbol   BaggingRegressor especificado por usuario (flexible) n_features (sin submuestreo) Nivel de modelo   RandomForestRegressor DecisionTreeRegressor n_features (sin submuestreo) Nivel de nodo de árbol    Ejercicio random forest Vamos a realizar un pequeño ejercicio de random forest. El objetivo es explorar algunos atributos disponibles en random forest de scikit-learn. Vamos a usar el dataset de regresión de pingüinos.\nfrom sklearn.model_selection import train_test_split penguins = pd.read_csv(\"../data/penguins_regression.csv\") feature_name = \"Flipper Length (mm)\" target_name = \"Body Mass (g)\" X, y = penguins[[feature_name]], penguins[target_name] X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=0) Crearemos un random forest conteniendo 3 árboles. Entrenaremos el bosque y comprobaremos el rendimiento de generalización en el conjunto de prueba en términos de error absoluto medio.\nfrom sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error random_forest = RandomForestRegressor(n_estimators=3, n_jobs=-1, random_state=0) random_forest.fit(X_train, y_train) y_pred = random_forest.predict(X_test) print(f\"MAE: {mean_absolute_error(y_test, y_pred):.3f} g\") MAE: 345.349 g  Lo siguiente que haremos será:\n crear un dataset que contenga los pinguinos con un tamaño de aleta entre 170 mm y 230 mm, dibujar los datos de entrenamiento usando un scatter plot, dibujar la decisión de cada árbol individual prediciendo en el recién creado dataset, dibujar la decisión del random forest usando este recién creado dataset.  data_range = pd.DataFrame(np.linspace(170, 235, num=300), columns=X.columns) tree_preds = [] for tree in random_forest.estimators_: tree_preds.append(tree.predict(data_range.to_numpy())) random_forest_preds = random_forest.predict(data_range) sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) # Dibuja las predicciones de los árboles for tree_idx, predictions in enumerate(tree_preds): plt.plot(data_range[feature_name], predictions, label=f\"Arbol #{tree_idx}\", linestyle=\"--\", alpha=0.8) plt.plot(data_range[feature_name], random_forest_preds, label=f\"Random forest\") _ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") Conjuntos basados en boosting Boosting adaptativo (AdaBoost) Vamos a presentar el algoritmo Boosting Adaptativo (AdaBoost). El objetivo es obtener información observando la maquinaria interna de AdaBoots y boosting en general.\nCargaremos el dataset de pingüinos y predeciremos su especie a partir de las features de longitud y anchura del pico.\npenguins = pd.read_csv(\"../data/penguins_classification.csv\") culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"] target_column = \"Species\" X, y = penguins[culmen_columns], penguins[target_column] Entrenaremos un árbol de decisión poco profundo intencionadamente. Dada su poca profundidad, es poco probable que tenga overfitting y algunas de las muestras de entrenamiento incluso serán mal clasificadas.\nfrom sklearn.tree import DecisionTreeClassifier palette = [\"tab:red\", \"tab:blue\", \"black\"] tree = DecisionTreeClassifier(max_depth=2, random_state=0) tree.fit(X, y) DecisionTreeClassifier(max_depth=2, random_state=0)  Podemos predecir en el mismo dataset y comprobar qué muestras están mal clasificadas.\ny_pred = tree.predict(X) misclassified_samples_idx = np.flatnonzero(y != y_pred) X_misclassified = X.iloc[misclassified_samples_idx] from helpers.plotting import DecisionBoundaryDisplay DecisionBoundaryDisplay.from_estimator( tree, X, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) # dibuja el dataset original sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=palette) # Dibuja los ejemplos mal clasificados sns.scatterplot(data=X_misclassified, x=culmen_columns[0], y=culmen_columns[1], label=\"Muestras mal clasificadas\", marker=\"+\", s=150, color=\"k\") plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\") _ = plt.title(\"Predicciones álbol decisión \\ndestacando muestras mal \" \"clasificadas\") Observamos que hay varios ejemplos que han sido mal clasificados por el clasificador.\nMencionamos que boosting se basa en la creación de un nuevo clasificador que intenta corregir esas clasificaciones erróneas. En scikit-learn, los aprendices tiene un parámetro sample_weight que fuerza a prestar más atención a los ejemplos con pesos altos durante el entrenamiento.\nEste parámetro se establece cuando se llama a classifier.fit(X, y, sample_weight=weights). Usaremos este truco para crear un nuevo clasificador “descartando” todas las muestras clasificadas correctamente y considerando únicamente las mal clasificadas. Así, a las muestras mal clasificadas se asignará un peso de 1 y a las bien clasificadas un peso de 0.\nsample_weight = np.zeros_like(y, dtype=int) sample_weight[misclassified_samples_idx] = 1 tree = DecisionTreeClassifier(max_depth=2, random_state=0) tree.fit(X, y, sample_weight=sample_weight) DecisionTreeClassifier(max_depth=2, random_state=0)  DecisionBoundaryDisplay.from_estimator( tree, X, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=palette) sns.scatterplot(data=X_misclassified, x=culmen_columns[0], y=culmen_columns[1], label=\"Muestras mal clasificadas previamente\", marker=\"+\", s=150, color=\"k\") plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\") _ = plt.title(\"Arbol decisión cambiando los pesos de las muestras\") Vemos que la función de decisión ha cambiado drásticamente. Cualitativamente vemos que los ejemplos que previamente estaban mal clasificados ahora son clasificados correctamente.\ny_pred = tree.predict(X) newly_misclassified_samples_idx = np.flatnonzero(y != y_pred) remaining_misclassified_samples_idx = np.intersect1d( misclassified_samples_idx, newly_misclassified_samples_idx ) print(f\"Número de muestras mal clasificadas previamente y \" f\"todavía mal clasificadas: {len(remaining_misclassified_samples_idx)}\") Número de muestras mal clasificadas previamente y todavía mal clasificadas: 0  Sin embargo, estamos cometiendo errores en muestras bien clasificadas previamente. Por tanto, tenemos la intuición de que debemos ponderar las predicciones de cada clasificador de forma diferente, muy probablemente utilizando el número de errores que comete cada clasificador.\nEntonces podríamos usar el error de clasificación para combinar ambos árboles.\nensemble_weight = [ (y.shape[0] - len(misclassified_samples_idx)) / y.shape[0], (y.shape[0] - len(newly_misclassified_samples_idx)) / y.shape[0], ] ensemble_weight [0.935672514619883, 0.6929824561403509]  El primer clasificador tiene una precisión del 93,5% y el segundo una precisión de 69,2%. Por lo tanto, cuando predecimos una clase, debemos confiar un poco más en el primer clasificador que en el segundo. Podríamos usar estos valores de precisión para ponderar las predicciones de cada aprendiz.\nPara resumir, boosting entrena varios clasificadores, cada uno de los cuales se enfocará más o menos en muestras específicas del dataset. Boosting es, por tanto, diferente de bagging: aquí nunca se remuestrea nuestro dataset, solo asignamos pesos diferentes al dataset original.\nBoosting requiere alguna estrategia para combinar los aprendices juntos:\n necesita definir una forma de calcular los pesos que serán asignados a las muestras; necesita asignar un peso a cada aprendiz al hacer predicciones.  De hecho, definimos un esquema realmente simple para asignar pesos a las muestras y pesos a los aprendices. Sin embargo, existen teorías estadísticas (como en AdaBoost) sobre cómo se deben calcular óptimamente estos pesos.\nUsaremos el clasificador AdaBoots implementado en scikit-learn y revisaremos los clasificadores de árbol de decisión entrenados subyacentes.\nfrom sklearn.ensemble import AdaBoostClassifier base_estimator = DecisionTreeClassifier(max_depth=3, random_state=0) adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=3, algorithm=\"SAMME\", random_state=0) adaboost.fit(X, y) AdaBoostClassifier(algorithm='SAMME', base_estimator=DecisionTreeClassifier(max_depth=3, random_state=0), n_estimators=3, random_state=0)  for boosting_round, tree in enumerate(adaboost.estimators_): plt.figure() # covertimos \"X\" en un array Numpy para eviar el warning lanzado por scikit-learn DecisionBoundaryDisplay.from_estimator( tree, X.to_numpy(), response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(x=culmen_columns[0], y=culmen_columns[1], hue=target_column, data=penguins, palette=palette) plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\") _ = plt.title(f\"Arbol decisión entrenado en ronda {boosting_round}\") \u003cFigure size 432x288 with 0 Axes\u003e  \u003cFigure size 432x288 with 0 Axes\u003e  \u003cFigure size 432x288 with 0 Axes\u003e  print(f\"Peso de cada clasificador: {adaboost.estimator_weights_}\") Peso de cada clasificador: [3.58351894 3.46901998 3.03303773]  print(f\"Error de cada clasificador: {adaboost.estimator_errors_}\") Error de cada clasificador: [0.05263158 0.05864198 0.08787269]  Vemos que AdaBoost ha entrenado tres clasificadores diferentes, cada uno de los cuales se enfoca en muestras diferentes. Revisando los pesos de cada aprendiz, vemos que el conjunto ofrece el mayor peso al primer clasificador. Esto de hecho tiene sentido cuando revisamos los errores de cada clasificador. El primer clasificador también obtiene la clasificación más alta en rendimiento de generalización.\nAunque AdaBoost es un buen algoritmo para demostrar la maquinaria interna de los algoritmos de boosting, no es el más eficiente. Este título se otorga al algoritmo de árbol de decisión gradient-boosting (GBDT).\nArbol de decisión gradient-boosting (GBDT) Vamos a ver el algoritmo de árbol de decisión gradient boosting y lo compararemos con AdaBoost.\nGradient-boosting difiere de AdaBoost en lo siguiente: en lugar de asignar pesos a muestras específicas, GBDT entrenará un árbol de decisión en los errores residuales (de ahí el nombre “gradiente”) del árbol anterior. Por lo tanto, cada nuevo árbol del conjunto predice el error cometido por el anterior en lugar de predecir directamente el objetivo.\nVamos a proporcionar algunas intuiciones sobre la forma en que se combinan los aprendices para proporcionar la predicción final.\nimport pandas as pd import numpy as np # Crea un generador de número aleatorio que utilizaremos para establecer la aleatoriedad rng = np.random.RandomState(0) def generate_data(n_samples=50): \"\"\"Genera dataset sintético. Devuelve `X_train`, `X_test`, `y_train`.\"\"\" x_max, x_min = 1.4, -1.4 len_x = x_max - x_min x = rng.rand(n_samples) * len_x - len_x / 2 noise = rng.randn(n_samples) * 0.3 y = x ** 3 - 0.5 * x ** 2 + noise X_train = pd.DataFrame(x, columns=[\"Feature\"]) X_test = pd.DataFrame(np.linspace(x_max, x_min, num=300), columns=[\"Feature\"]) y_train = pd.Series(y, name=\"Target\") return X_train, X_test, y_train X_train, X_test, y_train = generate_data() import matplotlib.pyplot as plt import seaborn as sns sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) _ = plt.title(\"Dataset de regresión sintético\") Como vimos anteriormente, boosting se basa en ensamblar una secuencia de aprendices. Empezaremos creando un regresor de arbol de decisión. Estableceremos la profundidad del árbol para que el aprendiz resultante produzca underfitting.\nfrom sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(max_depth=3, random_state=0) tree.fit(X_train, y_train) y_train_pred = tree.predict(X_train) y_test_pred = tree.predict(X_test) Usamos el término “test” para referirnos a datos que no se han usado para el entrenamiento. No debemos confundirlo con datos provenientes de una división entrenamiento-prueba, ya que se generó en intervalos espaciados equitativamente para la evaluación visual de las predicciones.\n# dibuja los datos sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) # dibuja las predicciones line_predictions = plt.plot(X_test[\"Feature\"], y_test_pred, \"--\") # dibuja los residuos for value, true, predicted in zip(X_train[\"Feature\"], y_train, y_train_pred): lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\") plt.legend([line_predictions[0], lines_residuals[0]], [\"Arbol entrenado\", \"Residuos\"]) _ = plt.title(\"Función predicción junto \\ncon errores en el conjunto entrenamiento\") Hemos editado manualmente la leyenda para obtener solo una única etiqueta para todas las líneas de residuos.\nDado que el arbol tiene underfit, su precisión está lejos de la perfección en los datos de entrenamiento. Podemos observar esto en el gráfico viendo las diferencias entre las predicciones y los datos reales. Estos errores, llamados residuos, se presentan mediante líneas rojas continuas.\nDe hecho, nuestro árbol inicial no era suficientemente expresivo para manejar la complejidad de los datos, como muestran los residuos. En un algoritmo grandient-boosting, la idea es crear un segundo árbol que, dados los mismos datos X, intentará predecir los residuos en lugar del vector y. Tendríamos, por tanto, un árbol que es capaz de predecir los errores que comete el árbol inicial.\nVamos a entrenar un árbol de estas características.\nresiduals = y_train - y_train_pred tree_residuals = DecisionTreeRegressor(max_depth=5, random_state=0) tree_residuals.fit(X_train, residuals) target_train_predicted_residuals = tree_residuals.predict(X_train) target_test_predicted_residuals = tree_residuals.predict(X_test) sns.scatterplot(x=X_train[\"Feature\"], y=residuals, color=\"black\", alpha=0.5) line_predictions = plt.plot( X_test[\"Feature\"], target_test_predicted_residuals, \"--\") # dibuja los residuos de los residuos predichos for value, true, predicted in zip(X_train[\"Feature\"], residuals, target_train_predicted_residuals): lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\") plt.legend([line_predictions[0], lines_residuals[0]], [\"Arbol entrenado\", \"Residuos\"], bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Predicciones de los residuos anteriores\") Vemos que este nuevo árbol solo logra manejar algunos de los residuos. Nos enfocaremos en una muestra específica del conjunto de entrenamiento (es decir, sabemos que la muestra será bien predicha usando dos árboles sucesivos). Usaremos este ejemplo para explicar cómo se combinan las predicciones de ambos árboles. Primero seleccionaremos esta muestra en X_train.\nsample = X_train.iloc[[-2]] x_sample = sample['Feature'].iloc[0] target_true = y_train.iloc[-2] target_true_residual = residuals.iloc[-2] Vamos a dibujar la información previa y destacaremos nuestra muestra. Empezaremos dibujando los datos originales y la predicción del primer árbol de decisión.\n# Dibuja la información previa: # * el dataset # * las predicciones # * los residuos sns.scatterplot(x=X_train[\"Feature\"], y=y_train, color=\"black\", alpha=0.5) plt.plot(X_test[\"Feature\"], y_test_pred, \"--\") for value, true, predicted in zip(X_train[\"Feature\"], y_train, y_train_pred): lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\") # Destacamos la muestra de interés plt.scatter(sample, target_true, label=\"Muestra de interés\", color=\"tab:orange\", s=200) plt.xlim([-1, 0]) plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\") _ = plt.title(\"Predicciones del árbol\") Ahora dibujaremos la información de los residuos. Dibujaremos los residuos calculados por el primer árbol de decisión y mostraremos las predicciones de residuos.\n# Dibuja la información previa: # * los residuso cometidos por el primer árbol # * las predicciones de residuos # * los residuos de las predicciones de residuos sns.scatterplot(x=X_train[\"Feature\"], y=residuals, color=\"black\", alpha=0.5) plt.plot(X_test[\"Feature\"], target_test_predicted_residuals, \"--\") for value, true, predicted in zip(X_train[\"Feature\"], residuals, target_train_predicted_residuals): lines_residuals = plt.plot([value, value], [true, predicted], color=\"red\") # Destaca la muestra de interés plt.scatter(sample, target_true_residual, label=\"Muestra de interés\", color=\"tab:orange\", s=200) plt.xlim([-1, 0]) plt.legend() _ = plt.title(\"Predicción de los residuos\") Para nuestra muestra de interés, nuestro árbol inicial cometía un error (residuo pequeño). Cuando entrenamos el segundo árbol, en este caso el residuo se entrena y se predice perfectamente. Verificaremos cuantitativamente esta predicción usando el árbol entrenado. Primero, verificaremos la predicción de árbol inicial y compararemos con su valor real.\nprint(f\"El valor real a predecir para \" f\"f(x={x_sample:.3f}) = {target_true:.3f}\") y_pred_first_tree = tree.predict(sample)[0] print(f\"Predicción del primer árbol de decisión para x={x_sample:.3f}: \" f\"y={y_pred_first_tree:.3f}\") print(f\"Error del árbol: {target_true - y_pred_first_tree:.3f}\") El valor real a predecir para f(x=-0.517) = -0.393 Predicción del primer árbol de decisión para x=-0.517: y=-0.145 Error del árbol: -0.248  Como observamos visualmente, tenemos un pequeño error. Ahora, usaremos el segundo árbol para intentar predecir este residuo.\nprint(f\"Predicción del residuo para x={x_sample:.3f}: \" f\"{tree_residuals.predict(sample)[0]:.3f}\") Predicción del residuo para x=-0.517: -0.248  Vemos que nuestro segundo árbol es capaz de predecir el residuo exacto (error) de nuestro primer árbol. Por tanto, podemos predecir el valor de x sumando la predicción de todos los árboles del conjunto.\ny_pred_first_and_second_tree = ( y_pred_first_tree + tree_residuals.predict(sample)[0] ) print(f\"Predicción del primer y segundo árbol de decisión combinados para \" f\"x={x_sample:.3f}: y={y_pred_first_and_second_tree:.3f}\") print(f\"Error del árbol: {target_true - y_pred_first_and_second_tree:.3f}\") Predicción del primer y segundo árbol de decisión combinados para x=-0.517: y=-0.393 Error del árbol: 0.000  Elegimos una muestra para la cual solo fueron suficientes dos árboles para hacer una predicción perfecta. Sin embargo, vimos anteriormente en el gráfico anterior que dos árboles no son suficientes para corregir los residuos de todas las muestras. Por tanto, se necesita añadir varios árboles al conjunto para corregir adecuadamente el error (es decir, el segundo árbol corrige el error del primer árbol, mientras que el tercer árbol corrige los errores del segundo árbol y así sucesivamente).\nCompararemos el rendimiento de generalización de random forest y gradient boosting en el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing from sklearn.model_selection import cross_validate X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 # reescala el objetivo a k$ from sklearn.ensemble import GradientBoostingRegressor gradient_boosting = GradientBoostingRegressor(n_estimators=200) cv_results_gbdt = cross_validate( gradient_boosting, X, y, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) print(\"Arbol decisión gradient boosting\") print(f\"Error absoluto medio a través de validación cruzada: \" f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \" f\"{-cv_results_gbdt['test_score'].std():.3f} k$\") print(f\"Tiempo de entrenamiento medio: \" f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\") print(f\"Tiempo de puntuación medio: \" f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\") Arbol decisión gradient boosting Error absoluto medio a través de validación cruzada: 46.408 +/- -2.907 k$ Tiempo de entrenamiento medio: 6.463 seconds Tiempo de puntuación medio: 0.008 seconds  from sklearn.ensemble import RandomForestRegressor random_forest = RandomForestRegressor(n_estimators=200, n_jobs=-1) cv_results_rf = cross_validate( random_forest, X, y, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) print(\"Random forest\") print(f\"Error absoluto medio a través de validación cruzada: \" f\"{-cv_results_rf['test_score'].mean():.3f} +/- \" f\"{-cv_results_rf['test_score'].std():.3f} k$\") print(f\"Tiempo de entrenamiento medio: \" f\"{cv_results_rf['fit_time'].mean():.3f} seconds\") print(f\"Tiempo de puntuación medio: \" f\"{cv_results_rf['score_time'].mean():.3f} seconds\") Random forest Error absoluto medio a través de validación cruzada: 46.510 +/- -4.689 k$ Tiempo de entrenamiento medio: 7.761 seconds Tiempo de puntuación medio: 0.216 seconds  En términos de rendimiento computacional, el bosque se puede paralelizar, beneficiándose del uso de múltiples cores de CPU. En términos de rendimiento de puntuación, ambos algoritmos conducen a resultados muy cercanos.\nSin embargo, vemos que gradient boosting es un algoritmo muy rápido para predecir comparado con random forest. Esto se debe al hecho de que gradient boosting usa árboles poco profundos.\nEjercicio de boosting Vamos a realizar un ejercicio donde verificaremos si un random forest o un árbol de decisión gradient boosting producen overfitting si el número de estimadores no se elige apropiadamente. Para obtener los mejores rendimientos de generalización, usaremos la estrategia de parada temprana (early-stopping) para evitar añadir innecesariamente árboles.\nPara conducir nuestro experimento, usaremos el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 # reescala el objetivo a k$ X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=0, test_size=0.5) Vamos a crear un árbol de decisión gradient-boosting con max_depth=5 y learning_rate=0.5.\nfrom sklearn.ensemble import GradientBoostingRegressor gradient_boosting = GradientBoostingRegressor(max_depth=5, learning_rate=0.5) También crearemos un random forest con árboles completamente desarrollados estableciendo max_depth=None.\nfrom sklearn.ensemble import RandomForestRegressor random_forest = RandomForestRegressor(max_depth=None) Para ambos modelos, crearemos una curva de validación usando el conjunto de entrenamiento para comprobar el impacto del número de árboles en el rendimiento de cada modelo.\nEvaluaremos la lista de parámetros param_range = [1, 2, 5, 10, 20, 50, 100] y usaremos el error absoluto medio.\nfrom sklearn.model_selection import validation_curve param_range = [1, 2, 5, 10, 20, 50, 100] gradient_boosting_train_scores, gradient_boosting_val_scores = validation_curve( gradient_boosting, X_train, y_train, param_name=\"n_estimators\", param_range=param_range, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) gradient_boosting_train_errors, gradient_boosting_val_errors = -gradient_boosting_train_scores, -gradient_boosting_val_scores random_forest_train_scores, random_forest_val_scores = validation_curve( random_forest, X_train, y_train, param_name=\"n_estimators\", param_range=param_range, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) random_forest_train_errors, random_forest_val_errors = -random_forest_train_scores, -random_forest_val_scores import matplotlib.pyplot as plt fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10, 4)) axs[0].errorbar( param_range, gradient_boosting_train_errors.mean(axis=1), yerr=gradient_boosting_train_errors.std(axis=1), label=\"Entrenamiento\", ) axs[0].errorbar( param_range, gradient_boosting_val_errors.mean(axis=1), yerr=gradient_boosting_val_errors.std(axis=1), label=\"Validación cruzada\", ) axs[0].set_title(\"Arbol decisión gradient boosting\") axs[0].set_xlabel(\"# estimadores\") axs[0].set_ylabel(\"Error absoluto medio en k$\\n(cuanto más pequeño mejor)\") axs[1].errorbar( param_range, random_forest_train_errors.mean(axis=1), yerr=random_forest_train_errors.std(axis=1), label=\"Entrenamiento\", ) axs[1].errorbar( param_range, random_forest_val_errors.mean(axis=1), yerr=random_forest_val_errors.std(axis=1), label=\"Cross-validation\", ) axs[1].set_title(\"Random forest\") axs[1].set_xlabel(\"# estimadores\") plt.legend() _ = fig.suptitle(\"Curvas de validación\", y=1.1) Tanto gradient boosting como random forest mejorarán siempre cuando se incrementa el número de árboles del conjunto. Sin embargo, se alcanza una meseta donde añadir nuevos árboles solo provoca que se ralentice el entrenamiento y el tiempo de puntuación.\nPara evitar añadir nuevos árboles innecesariamente, a diferencia de random forest, gradient boosting ofrece una opción de parada temprana. Internamente, el algoritmo usará un conjunto externo de muestras para calcular el rendimiento de generalización del modelo en cada adicción de un arbol. Por tanto, si el rendimiento de generalización no mejora en varias iteraciones, dejará de agregar árboles.\nAhora, crearemos un modelo gradient boosting con n_estimators=1_000. Este número de árboles será demasiado grande. Cambiaremos el parámetro n_iter_no_change de forma que gradient boosting detendrá el entrenamiento tras añadir 5 árboles sin obtener mejora en el rendimiento de generalización global.\ngradient_boosting = GradientBoostingRegressor(n_estimators=1_000, n_iter_no_change=5) gradient_boosting.fit(X_train, y_train) gradient_boosting.n_estimators_ 127  Vemos que el número de árboles usados está bastante por debajo de 1000 con el dataset actual. Entrenar el modelo gradient boosting con el total de 1000 árboles hubiera sido inútil.\nVamos a estimar otra vez el rendimiento de generalización de este modelo usando la métrica sklearn.metrics.mean_absolute_error, pero esta vez usando el conjunto de prueba y compararemos el valor resultante con los valores observados en la curva de validación.\nfrom sklearn.metrics import mean_absolute_error error = mean_absolute_error(y_test, gradient_boosting.predict(X_test)) print(f\"De media, nuestro regresor Gradient Boosting tiene un error de {error:.2f} k$\") De media, nuestro regresor Gradient Boosting tiene un error de 36.58 k$  Observamos que la medida del MAE en el conjunto de prueba está cercano al error de validación medido en el lado derecho de la curva de validación. Esto es tranquilizador, ya que significa que tanto el procedimiento de validación cruzada como la división externa entrenamiento-prueba coinciden aproximadamente como aproximaciones al rendimiento de generalización verdadero del modelo. Podemos observar que la evaluación final del error de prueba parece estar incluso ligeramente por debajo de las puntuaciones de prueba de validación cruzada. Esto puede explicarse porque el modelo final ha sido entrenado en el conjunto completo de entrenamiento mientras que los modelos de validación cruzada han sido entrenamos en subconjuntos más pequeños: en general, cuanto mayor sea el número de puntos de entrenamiento, menor es el error de prueba.\nAcelerando gradient boosting Vamos a presentar una versión modificada de gradient boosting que usa un número reducido de divisiones cuando construye los diferentes árboles. Este algoritmo se denomina “histogram gradient boosting” en scikit-learn.\nHemos mencionado anteriormente que random forest es un eficiente algoritmo dado que los árboles del conjunto se pueden entrenar al mismo tiempo de forma independiente. Por tanto, el algoritmo escala eficientemente tanto con el número de cores como con el número de muestras.\nEn gradient boosting, el algoritmo es secuencial. Requiere N-1 árboles entrenados para ser capaz de entrenar el árbol de la fase N. Por tanto, el algoritmo es bastante costoso desde el punto de vista computacional. La parte más costosa es la búsqueda de la mejor división del árbol, que se realiza con un enfoque de fuerza bruta: se evalúan todas las posibles divisiones y se elige la mejor.\nPara acelerar el algoritmo gradient boosting, podríamos reducir el número de divisiones a evaluar. Como consecuencia, el rendimiento de generalización de un árbol de este tipo también se reduciría. Sin embargo, dado que tenemos varios árboles combinados en gradient boosting, podemos añadir más estimadores para superar este problema.\nRealizaremos una implementación simple de este algoritmo construyendo bloques de scikit-learn. En primer lugar, carguemos el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 # reescala el objetivo en k$ Vamos a realizar una rápida comparativa del gradient boosting original.\nfrom sklearn.model_selection import cross_validate from sklearn.ensemble import GradientBoostingRegressor gradient_boosting = GradientBoostingRegressor(n_estimators=200) cv_results_gbdt = cross_validate( gradient_boosting, X, y, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) print(\"Arbol Decisión Gradient Boosting\") print(f\"Error absoluto medio via cross-validation: \" f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \" f\"{cv_results_gbdt['test_score'].std():.3f} k$\") print(f\"Average fit time: \" f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\") print(f\"Average score time: \" f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\") Arbol Decisión Gradient Boosting Error absoluto medio via cross-validation: 46.407 +/- 2.908 k$ Average fit time: 6.512 seconds Average score time: 0.008 seconds  Recordemos que una forma de acelerar gradient boosting es reducir el número de divisiones a considerar dentro de la estructura del árbol. Una forma es agrupar los datos antes de pasarlos a gradient boosting. Un transformador llamado KBinsDiscretizer se encarga de tal transformación. Por lo tanto, podemos usar un pipeline para este preprocesamiento de gradient boosting.\nEn primer lugar, vamos a desmostrar la transformación realizada por KBinsDiscretizer.\nimport numpy as np from sklearn.preprocessing import KBinsDiscretizer discretizer = KBinsDiscretizer( n_bins=256, encode=\"ordinal\", strategy=\"quantile\") X_trans = discretizer.fit_transform(X) X_trans C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., \u003c= 1e-8) in feature 1 are removed. Consider decreasing the number of bins. warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., \u003c= 1e-8) in feature 3 are removed. Consider decreasing the number of bins. warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., \u003c= 1e-8) in feature 6 are removed. Consider decreasing the number of bins. warnings.warn( C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., \u003c= 1e-8) in feature 7 are removed. Consider decreasing the number of bins. warnings.warn( array([[249., 39., 231., ..., 83., 162., 30.], [248., 19., 203., ..., 28., 161., 30.], [242., 49., 249., ..., 125., 160., 29.], ..., [ 17., 15., 126., ..., 49., 200., 82.], [ 23., 16., 136., ..., 29., 200., 77.], [ 53., 14., 130., ..., 93., 199., 81.]])  El código anterior generará una serie de alertas. De hecho para algunas features, solicitamos demasiados contenedores (bins) con respecto a la dispersión de los datos para dichas features. Los pequeños contenedores se eliminarán.\nVemos que discretizer transforma los datos originales en valores enteros (aunque estén codificados usando una representación de coma flotante). Cada valor representa el índice de contenedor cuando se realiza la distribución por cuantiles. Podemos verificar el número de contenedores por feature.\n[len(np.unique(col)) for col in X_trans.T] [256, 50, 256, 253, 256, 256, 207, 235]  Después de la transformación, vemos que tenemos una mayoría de 256 valores únicos por feature. Ahora, usaremos el transformador para discretizar los datos antes del entrenamiento del regresor gradient boosting.\nfrom sklearn.pipeline import make_pipeline gradient_boosting = make_pipeline( discretizer, GradientBoostingRegressor(n_estimators=200)) cv_results_gbdt = cross_validate( gradient_boosting, X, y, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) print(\"Arbol Decisión Gradient Boosting con KBinsDiscretizer\") print(f\"Error absoluto medio via cross-validation: \" f\"{-cv_results_gbdt['test_score'].mean():.3f} +/- \" f\"{cv_results_gbdt['test_score'].std():.3f} k$\") print(f\"Average fit time: \" f\"{cv_results_gbdt['fit_time'].mean():.3f} seconds\") print(f\"Average score time: \" f\"{cv_results_gbdt['score_time'].mean():.3f} seconds\") Arbol Decisión Gradient Boosting con KBinsDiscretizer Error absoluto medio via cross-validation: 46.130 +/- 2.230 k$ Average fit time: 4.108 seconds Average score time: 0.010 seconds  Vemos que el tiempo de entrenamiento se ha reducido, pero el rendimiento de generalización del modelo es muy parecido. Scikit-learn proporciona clases específicas que están aún más optimizadas para grandes datasets, llamadas HistGradientBoostingClassifier y HistGradientBoostingRegressor. Cada feature del dataset X se agrupa primero calculando los histogramas, que se usarán posteriormente para evaluar potenciales divisiones. El número de divisiones a evaluar es entonces mucho más pequeño. Este algoritmo se convierte en mucho más eficiente que el de gradient boosting cuando el dataset tiene más de 10.000 muestras.\nVamos a dar un ejemplo de dataset grande y compararemos los tiempos de cálculo con el experimento anterior.\nfrom sklearn.ensemble import HistGradientBoostingRegressor histogram_gradient_boosting = HistGradientBoostingRegressor( max_iter=200, random_state=0) cv_results_hgbdt = cross_validate( histogram_gradient_boosting, X, y, scoring=\"neg_mean_absolute_error\", n_jobs=-1, ) print(\"Arbol Decisión Histogram Gradient Boosting\") print(f\"Error absoluto medio via cross-validation: \" f\"{-cv_results_hgbdt['test_score'].mean():.3f} +/- \" f\"{cv_results_hgbdt['test_score'].std():.3f} k$\") print(f\"Average fit time: \" f\"{cv_results_hgbdt['fit_time'].mean():.3f} seconds\") print(f\"Average score time: \" f\"{cv_results_hgbdt['score_time'].mean():.3f} seconds\") Arbol Decisión Histogram Gradient Boosting Error absoluto medio via cross-validation: 43.758 +/- 2.694 k$ Average fit time: 1.000 seconds Average score time: 0.062 seconds  El histogram gradient boosting es el mejor algoritmo en términos de puntuación. También escala cuando el número de muestras incrementa, mientras que el gradient boosting normal no.\nAjuste de hiperparámetros con métodos de conjunto Random Forest El principal parámetro de random forest que hay que ajustar es el parámetro n_estimators. En general, cuantos más árboles en el bosque, mejor será el rendimiento de generalización. Sin embargo, se ralentizarán los tiempos de entrenamiento y predicción. El objetivo es balancear el tiempo de cálculo y el rendimiento de generalización al establecer el número de estimadores cuando se ponga en producción.\nLuego podríamos ajustar también un parámetro que controla la profundidad de cada árbol del bosque. Dos parámetros son los importantes para esto: max_depth y max_leaf_nodes. Difieren en la forma en que controlan la estructura del árbol. max_depth obliga a tener un árbol más simétrico, mientras que max_leaf_nodes no impone tal restricción.\nTengamos en cuenta que con random forest los árboles son generalmente profundos, dado que se busca overfitting en cada árbol de cada muestra bootstrap porque esto será mitigado al combinarlos todos juntos. El ensamblaje de árboles con underfitting (es decir, árboles poco profundos) también podría conducir a bosques con underfitting.\nfrom sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 # reescala en k$ X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=0) import pandas as pd from sklearn.model_selection import RandomizedSearchCV from sklearn.ensemble import RandomForestRegressor param_distributions = { \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500], \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100], } search_cv = RandomizedSearchCV( RandomForestRegressor(n_jobs=2), param_distributions=param_distributions, scoring=\"neg_mean_absolute_error\", n_iter=10, random_state=0, n_jobs=-1, ) search_cv.fit(X_train, y_train) columns = [f\"param_{name}\" for name in param_distributions.keys()] columns += [\"mean_test_error\", \"std_test_error\"] cv_results = pd.DataFrame(search_cv.cv_results_) cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"] cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"] cv_results[columns].sort_values(by=\"mean_test_error\")  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  param_n_estimators param_max_leaf_nodes mean_test_error std_test_error     0 500 100 40.785932 0.704460   2 10 100 41.661666 0.991435   7 100 50 43.925528 0.767559   8 1 100 47.130459 0.846356   6 50 20 49.303910 0.822624   1 100 20 49.483635 0.835842   9 10 20 50.097904 0.558017   3 500 10 54.572481 0.776725   4 5 5 61.592333 0.943317   5 5 2 72.811144 0.945929     Podemos observar en nuestra búsqueda que estamos obligados a tener un número alto de hojas y, por tanto, árboles profundos. Este parámetro parece particularmente impactante en comparación con el número de árboles para este dataset en particular: con al menos 50 árboles, el rendimiento de generalización será conducido por el número de hojas.\nAhora estimaremos el rendimiento de generalización del mejor modelo reentrenandolo con el conjunto de entrenamiento completo y usando el conjunto de prueba para evaluarlo en datos nunca vistos. Esto se hace por defecto llamando al método .fit.\nerror = -search_cv.score(X_test, y_test) print(f\"De media, nuestro regresor random forest tiene un error de {error:.2f} k$\") De media, nuestro regresor random forest tiene un error de 42.00 k$  Arboles de decisión gradient-boosting Para gradient boosting, los parámetros están emparejados, ya no se pueden establecer los parámetros uno tras otro. Los parámetros importantes son n_estimators, learning_rate y max_depth o max_leaf_nodes (como vimos anteriormente con random forest).\nPrimero veamos los parámetros max_depth (o max_leaf_nodes). Vimos anteriormente en gradient boosting que el algoritmo ajusta el error del árbol precedente en el conjunto. Por lo tanto, el ajuste de árboles completamente desarrollados sería perjudicial. De hecho, el primer árbol del conjunto ajustaría perfectamente (overfitting) los datos y, por lo tanto, no se requeriría ningún árbol posterior, dado que no habría residuos. Por lo tanto, el árbol usado en gradient boosting debe tener una profundidad baja, típicamente entre 3 y 8 niveles, o pocas hojas ($2^3=8$ a $2^8=256$). Tener árboles muy débiles en cada paso nos ayudará a reducir el overfitting.\nCon esta consideración en mente, cuanto más profundos sean los árboles, más rápido se corregirán los residuos y menos árboles se requerirán. Por tanto, n_estimators deberá incrementarse si max_depth es bajo.\nFinalmente, hemos pasado por alto el impacto del parámetro learning_rate hasta ahora. Cuando ajustamos los residuos, nos gustaría que el árbol intentara corregir todos los posibles errores o solo una fracción de ellos. La tasa de aprendizaje permite controlar este comportamiento. Una tasa baja podría corregir solo los resiudos de muy pocas muestras. Si se establece una tasa alta (por ejemplo, 1), podríamos ajustar los residuos de todas las muestras. Asi que, con una tasa de aprendizaje muy baja, necesitaremos más estimadores para corregir el error global. Sin embargo, una tasa de aprendizaje demasiado alta haría que obtuviéramos un conjunto con overfitting, de forma similar a tener una profundidad de árbol demasiado alta.\nfrom scipy.stats import loguniform from sklearn.ensemble import GradientBoostingRegressor param_distributions = { \"n_estimators\": [1, 2, 5, 10, 20, 50, 100, 200, 500], \"max_leaf_nodes\": [2, 5, 10, 20, 50, 100], \"learning_rate\": loguniform(0.01, 1), } search_cv = RandomizedSearchCV( GradientBoostingRegressor(), param_distributions=param_distributions, scoring=\"neg_mean_absolute_error\", n_iter=20, random_state=0, n_jobs=-1 ) search_cv.fit(X_train, y_train) columns = [f\"param_{name}\" for name in param_distributions.keys()] columns += [\"mean_test_error\", \"std_test_error\"] cv_results = pd.DataFrame(search_cv.cv_results_) cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"] cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"] cv_results[columns].sort_values(by=\"mean_test_error\")  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  param_n_estimators param_max_leaf_nodes param_learning_rate mean_test_error std_test_error     1 200 20 0.160519 33.914794 0.421969   12 200 50 0.110585 34.783054 0.288929   17 500 5 0.771785 34.857510 0.570347   10 200 20 0.109889 35.009108 0.379727   6 500 100 0.709894 35.540170 0.393424   18 10 5 0.637819 42.535730 0.338066   3 500 2 0.07502 43.457866 0.704599   4 100 5 0.0351 46.558900 0.578629   19 5 20 0.202432 61.387176 0.610988   8 5 2 0.462636 65.114017 0.846987   9 10 5 0.088556 66.243538 0.720131   15 50 100 0.010904 71.847050 0.683326   5 2 2 0.421054 74.384704 0.791104   2 5 100 0.070357 77.007841 0.789595   16 2 50 0.167568 77.131005 0.850380   11 1 5 0.190477 82.819015 0.976351   13 5 20 0.033815 83.765509 0.974672   0 1 100 0.125207 85.363288 1.040982   14 1 10 0.081715 87.373374 1.071555   7 1 20 0.014937 90.531295 1.113892     Aquí hemos ajustado n_estimators, pero habría que tener en cuenta que sería mejor usar parada temprana, como vimos anteriormente.\nEn esta búsqueda, vemos que learning_rate es obligado que sea suficientemente alto, es decir, \u003e0.1. También observamos que para los modelos mejor clasificados, al tener un learning_rate más pequeño, se requerirán más árboles o un número de hojas más grande por cada árbol. Sin embargo, es particularmente difícil dibujar conclusiones con más detalle, dado que el mejor valor de un hiperparámetro depende de los valores de los otros hiperparámetros.\nAhora estimaremos el rendimiento de generalización del mejor modelo usando el conjunto de prueba.\nerror = -search_cv.score(X_test, y_test) print(f\"De media, nuestro regresor GBDT tiene un error de {error:.2f} k$\") De media, nuestro regresor GBDT tiene un error de 32.99 k$  La puntuación de prueba media en el conjunto de prueba externo es ligeramente mejor que la puntuación del mejor modelo. La razón es que el modelo final se entrena en el conjunto de entrenamiento completo y, por tanto, en más datos que los modelos de validación cruzada internos del procedimiento grid search.\nEjercicio ajuste hiperparámetros El objetivo es familizarizarse con histogram gradient boosting en scikit-learn. Además, usaremos el modelo dentro de un framework de validación cruzada para inspeccionar los parámetros internos que encontramos vía grid search.\nCargaremos el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(return_X_y=True, as_frame=True) y *= 100 En primer lugar, creamos un regresor histogram gradient boosting. Podemos establecer que el número de árboles sea grande y configurar el modelo para que use parada temprana.\nfrom sklearn.ensemble import HistGradientBoostingRegressor histogram_gradient_boosting = HistGradientBoostingRegressor( max_iter=1000, early_stopping=True, random_state=0 ) Usaremos grid search para encontrar algunos parámetros óptimos para este modelo. En este grid search, deberemos buscar los siguientes parámetros:\n max_depth: [3, 8]; max_leaf_nodes: [15, 31]; learning_rate: [0.1, 1].  from sklearn.model_selection import GridSearchCV param_grid = { \"max_depth\": [3, 8], \"max_leaf_nodes\": [15, 31], \"learning_rate\": [0.1, 1], } search = GridSearchCV(histogram_gradient_boosting, param_grid=param_grid) Finalmente, ejecutaremos nuestro modelo a través de validación cruzada. En esta ocasión, definiremos 5 particiones de validación cruzada. Además, nos aseguraremos de mezclar los datos. En consecuencia, usaremos la función sklearn.model_selection.cross_validate para ejecutar la validación cruzada. También debemos establecer return_estimator=True, así podremos investigar los modelos internos entrenados por validación cruzada.\nfrom sklearn.model_selection import cross_validate, KFold cv = KFold(n_splits=5, shuffle=True, random_state=0) results = cross_validate( search, X, y, cv=cv, return_estimator=True, n_jobs=-1) Ahora que tenemos los resultados de la validación cruzada, mostremos la media y la desviación típica de la puntuación.\nprint(f\"R2 score con validación cruzada:\\n\" f\"{results['test_score'].mean():.3f} +/- \" f\"{results['test_score'].std():.3f}\") R2 score con validación cruzada: 0.839 +/- 0.006  Ahora inspeccionaremos la entrada estimator de los resultados y comprobaremos los valores de los mejores parámetros. Además, verificaremos el número de árboles usados por el modelo.\nfor estimator in results[\"estimator\"]: print(estimator.best_params_) print(f\"# trees: {estimator.best_estimator_.n_iter_}\") {'learning_rate': 0.1, 'max_depth': 3, 'max_leaf_nodes': 15} # trees: 528 {'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15} # trees: 447 {'learning_rate': 0.1, 'max_depth': 3, 'max_leaf_nodes': 15} # trees: 576 {'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15} # trees: 290 {'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15} # trees: 414  Inspeccionaremos los resultados de la validación cruzada interna para cada estimador de la validación cruzada externa. Calcularemos la puntuación de prueba media agregada para cada combinación de parámetro y dibujaremos un box plot de esas puntuaciones.\nimport pandas as pd index_columns = [f\"param_{name}\" for name in param_grid.keys()] columns = index_columns + [\"mean_test_score\"] inner_cv_results = [] for cv_idx, estimator in enumerate(results[\"estimator\"]): search_cv_results = pd.DataFrame(estimator.cv_results_) search_cv_results = search_cv_results[columns].set_index(index_columns) search_cv_results = search_cv_results.rename( columns={\"mean_test_score\": f\"CV {cv_idx}\"}) inner_cv_results.append(search_cv_results) inner_cv_results = pd.concat(inner_cv_results, axis=1).T import matplotlib.pyplot as plt color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"} inner_cv_results.plot.box(vert=False, color=color) plt.xlabel(\"R2 score\") plt.ylabel(\"Parámetros\") _ = plt.title(\" Resultados de CV interna con parámetros\\n\" \"(max_depth, max_leaf_nodes, learning_rate)\") Vemos que los primeros 4 conjuntos de parámetros clasificados están muy cercanos. Podríamos seleccionar cualquier de esas 4 combinaciones. Coincide con los resultados que observamos inspeccionando los mejor parámetros de la validación cruzada externa.\nEjercicio Vamos a poner en práctica lo aprendido en este post con un ejercicio. Para ellos usaremos el dataset de pingüinos, pero no usaremos el objetivo tradicional de predecir la especie.\nimport pandas as pd df = pd.read_csv(\"../data/penguins.csv\") feature_names = [ \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", ] target_name = \"Body Mass (g)\" df = df[feature_names + [target_name]].dropna(axis=\"rows\", how=\"any\") df = df.sample(frac=1, random_state=0).reset_index(drop=True) X, y = df[feature_names], df[target_name] El objetivo es predicir la masa corporal (Body Mass (g)) de un pingüino, dadas sus medidas de pico y aleta. Por lo tanto, se trata de un problema de regresión.\nVéase que hemos mezclado aleatoriamente las filas del dataset después de cargarlo (df.sample(frac=1, random_state=0)). La razón es romper una dependencia estadística espúrea relacionada con el orden que pudiera causar problemas con el procedimiento simple de validación cruzada que usamos en este post. Evaluaremos los siguientes modelos basados en árboles:\n un árbol de decisión regresor, es decir, sklearn.tree.DecisionTreeRegressor un random forest regresor, e decir, sklearn.ensemble.RandomForestRegressor  Usaremos las configuraciones por defecto de hiperparámetros para ambos modelos. Tenemos que evaluar el rendimiento de generalización de dichos modelos usando una validación cruzada de 10-particiones:\n usaremos sklearn.model_selection.cross_validate para ejecutar la rutina de validación cruzada estableceremos el parámetro cv=10 para usar una estrategia de validación cruzada de 10-particiones. Almacenaremos la puntuación de entrenamiento de validación cruzada estableciendo el parámetro return_train_score=True en la función cross_validate.  Comparando las puntuaciones de prueba de validación cruzada partición a partición, cuenta el número de veces que un random forest es mejor que un único árbol de decisión.\nfrom sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_validate cv=10 tree = DecisionTreeRegressor(random_state=0) random_forest = RandomForestRegressor(random_state=0) scores_tree = cross_validate(tree, X, y, cv=cv, return_train_score=True, n_jobs=-1) scores_random_forest = cross_validate(random_forest, X, y, cv=cv, return_train_score=True, n_jobs=-1) scores_tree[\"test_score\"].mean(), scores_tree[\"test_score\"].std() (0.6274987889886428, 0.099704183300966)  scores_random_forest[\"test_score\"].mean(), scores_random_forest[\"test_score\"].std() (0.8035860871438268, 0.04467350184659844)  scores_random_forest[\"test_score\"] \u003e scores_tree[\"test_score\"] array([ True, True, True, True, True, True, True, True, True, True])  print(f\"Random forest es mejor que un árbol de decisión en \" f\"{sum(scores_random_forest['test_score'] \u003e scores_tree['test_score'])} \" f\"iteraciones de {cv}\") Random forest es mejor que un árbol de decisión en 10 iteraciones de 10  Ahora vamos a entrenar y evaluar con la misma estrategia de validación cruzada un random forest con 5 árboles de decisión y otro que contenga 100 árboles de decisión. También almacenaremos la puntuación de entrenamiento.\nComparando las puntuaciones de test partición a partición, cuenta el número de veces que un random forest con 100 árboles de decisión es mejor que un árbol de decisión con 5 árboles decisión.\nrandom_forest_5 = RandomForestRegressor(n_estimators=5, random_state=0) random_forest_100 = RandomForestRegressor(n_estimators=100, random_state=0) scores_rf_5 = cross_validate(random_forest_5, X, y, cv=cv, return_train_score=True, n_jobs=-1) scores_rf_100 = cross_validate(random_forest_100, X, y, cv=cv, return_train_score=True, n_jobs=-1) scores_rf_5[\"test_score\"].mean(), scores_rf_5[\"test_score\"].std() (0.7668640137103877, 0.07359278138744398)  scores_rf_100[\"test_score\"].mean(), scores_rf_100[\"test_score\"].std() (0.8035860871438268, 0.04467350184659844)  print(f\"Random forest (100) es mejor que un random forest (5) en \" f\"{sum(scores_rf_100['test_score'] \u003e scores_rf_5['test_score'])} \" f\"iteraciones de {cv}\") Random forest (100) es mejor que un random forest (5) en 9 iteraciones de 10  Añadir árboles al bosque ayuda a mejorar el rendimiento de generalización del modelo. Podemos obtener algún conocimiento adicional comparando las puntuaciones de prueba y entrenamiento de cada modelo:\nprint(f\"Puntuaciones para random forest con 5 árboles: \\n\" f\" Entrenamiento: \" f\"{scores_rf_5['train_score'].mean():.3f} +/- \" f\"{scores_rf_5['train_score'].std():.3f}\\n\" f\" Prueba : \" f\"{scores_rf_5['test_score'].mean():.3f} +/- \" f\"{scores_rf_5['test_score'].std():.3f}\\n\") print(f\"Puntuaciones para random forest con 100 árboles: \\n\" f\" Entrenamiento: \" f\"{scores_rf_100['train_score'].mean():.3f} +/- \" f\"{scores_rf_100['train_score'].std():.3f}\\n\" f\" Prueba : \" f\"{scores_rf_100['test_score'].mean():.3f} +/- \" f\"{scores_rf_100['test_score'].std():.3f}\\n\") Puntuaciones para random forest con 5 árboles: Entrenamiento: 0.950 +/- 0.003 Prueba : 0.767 +/- 0.074 Puntuaciones para random forest con 100 árboles: Entrenamiento: 0.972 +/- 0.001 Prueba : 0.804 +/- 0.045  En el modelo con 5 árboles, la puntuación media de entrenamiento ya era bastante alta pero la puntuación media de prueba es bastante baja. El rendimiento de este random forest pequeño está limitado, por tanto, por el overfitting.\nEn el bosque con 100 árboles, la puntuación de entrenamiento sigue siendo alta (incluso ligeramente más alta), y la puntuación de prueba se ha reducido. El overfitting se ha reducido añadiendo más árboles al bosque.\nDibuja la curva de estimación de los parámetros n_estimators definidos por n_estimators = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000]\nfrom sklearn.model_selection import validation_curve rf = RandomForestRegressor(random_state=0) param_range = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000] rf_train_scores, rf_test_scores = validation_curve( rf, X, y, param_name=\"n_estimators\", param_range=param_range, n_jobs=-1 ) import matplotlib.pyplot as plt plt.errorbar( param_range, rf_train_scores.mean(axis=1), yerr=rf_train_scores.std(axis=1), label=\"Puntuación entrenamiento\", ) plt.errorbar( param_range, rf_test_scores.mean(axis=1), yerr=rf_test_scores.std(axis=1), label=\"Puntuación prueba\", ) plt.legend() plt.xscale(\"log\") plt.xlabel(\"Nº de árboles\") plt.ylabel(\"R2 score\") plt.ylim([0, 1]) _ = plt.title(\"Curva de validación de Random Forest\") Repite el experimento previo pero esta vez, en lugar de elegir los parámetros por defecto para random forest, establece el parámetro max_depth=5 y contruye la curva de validación.\nComparando la curva de validación (entrenamiento y prueba) del random forest con profundidad total y el random forest con la profundidad limitada, ¿Qué conclusiones se pueden obtener?\nrf_5 = RandomForestRegressor(max_depth=5, random_state=0) param_range = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000] rf_5_train_scores, rf_5_test_scores = validation_curve( rf_5, X, y, param_name=\"n_estimators\", param_range=param_range, n_jobs=-1 ) fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(20, 6)) axs[0].errorbar( param_range, rf_train_scores.mean(axis=1), yerr=rf_train_scores.std(axis=1), label=\"Puntuación entrenamiento\", ) axs[0].errorbar( param_range, rf_test_scores.mean(axis=1), yerr=rf_test_scores.std(axis=1), label=\"Puntuación prueba\", ) axs[0].set_title(\"Random forest profundidad total\") axs[0].set_xscale(\"log\") axs[0].set_xlabel(\"Nº de árboles\") axs[0].set_ylabel(\"R2 score\") axs[0].set_ylim([0, 1]) axs[1].errorbar( param_range, rf_5_train_scores.mean(axis=1), yerr=rf_5_train_scores.std(axis=1), label=\"Puntuación entrenamiento\", ) axs[1].errorbar( param_range, rf_5_test_scores.mean(axis=1), yerr=rf_5_test_scores.std(axis=1), label=\"Puntuación prueba\", ) axs[1].set_title(\"Random forest profundidad 5\") axs[1].set_xscale(\"log\") axs[1].set_xlabel(\"Nº de árboles\") axs[1].set_ylabel(\"R2 score\") axs[1].set_ylim([0, 1]) plt.legend() _ = fig.suptitle(\"Curva de validación de Random Forest (max_dept=5)\", y=1.1) La puntuación de entrenamiento del random forest con profundidad total es casi siempre mejor que la puntuación de entrenamiento del random forest con una profundidad limitada. La diferencia entre las puntuaciones de entrenamiento y prueba disminuye cuando se reduce la profundidad de los árboles del random forest.\nTambién observamos que el random forest con profundidad limitada tiene un mejor rendimiento de generalización para un pequeño número de árboles pero que es equivalente para un número alto de árboles. Podemos concluir que los modelos random forest con una profundidad limitada de árboles tienen menos overfitting que los random forest con árboles totalmente desarrollados, especialmente cuando el número de árboles del conjunto es pequeño.\nTambién observamos que limitar la profundidad tiene un efecto significativo en la limitación de la puntuación de entrenamiento (habilidad para memorizar exactamente los datos de entrenamiento) y que este efecto sigue siendo importante incluso cuando se aumenta el tamaño del conjunto.\nVamos a centrarnos ahora en el principio de las curvas de validación y a considerar la puntuación de entrenamiento de un random forest con un único árbol mientras usamos la configuración del parámetro por defecto max_depth=None:\nrf_1_tree = RandomForestRegressor(n_estimators=1, random_state=0) cv_results_tree = cross_validate( rf_1_tree, X, y, cv=10, return_train_score=True ) cv_results_tree[\"train_score\"] array([0.83120264, 0.83309064, 0.83195043, 0.84834224, 0.85790323, 0.86235297, 0.84791111, 0.85183089, 0.82241954, 0.85045978])  El hecho de que este random forest de un único árbol nunca pueda alcanzar la puntuación perfecta R2 de 1.0 en el entrenamiento puede ser sorprendente. De hecho, si evaluamos la precisión de entrenamiento del único DecissionTreeRegressor se obtiene una perfecta memorización de los datos de entrenamiento:\ntree = DecisionTreeRegressor(random_state=0) cv_results_tree = cross_validate( tree, X, y, cv=10, return_train_score=True ) cv_results_tree[\"train_score\"] array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])  ¿Cómo se explica que un random forest de un único árbol no pueda alcanzar la puntuación perfecta de entrenamiento?\nLa respuesta es que el único árbol del random forest se entrena usando bootstrap en el conjunto de entrenamiento y no el conjunto de entrenamiento en sí mismo (porque bootstrap = True por defecto).\nPor defecto, random forest entrena los árboles con el procedimiento de bootstrapping. Dado que cada árbol es entrenado en una muestra bootstrap, algunos puntos de datos del conjunto de entrenamiento original no son vistos por árboles individuales en el bosque. Como resultado, el único árbol del modelo random forest no hace predicciones perfectas en esos puntos (denominados muestras out-of-bag), lo cual previene que la puntuación de entrenamiento alcance 1.0.\nPodemos confirmar esta hipótesis desactivando la opción bootstrap y comprobando de nuevo las puntuaciones de entrenamiento (dejando resto de hiperparámetros inalterados):\nrf_1_tree = RandomForestRegressor(n_estimators=1, bootstrap=False, random_state=0) cv_results_tree = cross_validate( rf_1_tree, X, y, cv=10, return_train_score=True ) cv_results_tree[\"train_score\"] array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])  En este caso, recurrimos al mismo algoritmo que el árbol de decisión único, entrenandolo en los datos originales y, por tanto, generando overfitting en el dataset de entrenamiento original.\nTambién podemos advertir que cuando se incrementa el número de árboles a 5 ó 10, el entrenamiento sub-óptimo causado por bootstrapping se desvanece rápidamente. Los random forest son entrenados con al menos 10 árboles y, típicamente, muchos más que esos, así este efecto casi no se observa en la práctica.\nAdemás, incluso con un único árbol con bootstrapping, una puntuación de R2 mayor que 0.85 todavía es muy alta y, típicamente, mucho mayor que la puntuación de prueba del mismo modelo. Como resultado, incluso si la puntuación de entrenamiento de este modelo no es perfecta, no se puede concluir que el rendimiento de generalización de un random forest con un único árbol está limitado por el underfitting. En cambio, los modelos de árboles únicos normalmente están limitados por overfitting cuando la profundidad no está limitada.\nConstruye una curva de validación para un sklearn.ensemble.HistGradientBoostingRegressor, variando max_iter como sigue:\nmax_iter = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000]\nRecordemos que max_iter corresponde con el número de árboles en los modelos boosting.\nDibuja las puntuaciones medias de entrenamiento y prueba para cada valor de max_iter.\nfrom sklearn.ensemble import HistGradientBoostingRegressor hgbr = HistGradientBoostingRegressor(random_state=0) param_range = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000] hgbr_train_scores, hgbr_test_scores = validation_curve( hgbr, X, y, param_name=\"max_iter\", param_range=param_range, n_jobs=-1 ) plt.errorbar( param_range, hgbr_train_scores.mean(axis=1), yerr=hgbr_train_scores.std(axis=1), label=\"Puntuación entrenamiento\", ) plt.errorbar( param_range, hgbr_test_scores.mean(axis=1), yerr=hgbr_test_scores.std(axis=1), label=\"Puntuación prueba\", ) plt.legend() plt.xscale(\"log\") plt.xlabel(\"Nº de árboles\") plt.ylabel(\"R2 score\") plt.ylim([0, 1]) _ = plt.title(\"Curva de validación de HGBR\") En la gráfica podemos observar claramente las tres fases del comportamiento “undefitting / mejor generalización / overfitting” de los modelos gradient boosting. Con un número bajo de árboles, el modelo tiene una puntuaciones baja tanto para el entrenamiento como en la prueba. Podemos ver claramente que la puntuación de prueba está limitada por la puntuación de entrenamiento, que es una característica de los modelos con underfitting (contrariamente a lo observado anteriormente en la curva de aprendizaje de los modelos random forest).\nAmbas puntuaciones mejoran hasta alcanzar un punto dulce (alrededor de 50 árboles) donde la puntuación de prueba es máxima. Después de esto, el algoritmo de gradient boosting empieza con overfitting: la puntuación de entrenamiento mejora hasta alcanzar una puntuación perfecta de 1.0, mientras que la puntuación de prueba se reduce. De hecho, el modelo empieza a memorizar reglas específicas que solo se cumplen en el conjunto de entrenamiento. Estas reglas aprendidas van en detrimento del rendimiento de generalización del modelo.\nAquí se muestra la importancia de no añadir demasiados árboles a nuestro conjunto de gradient boosting. De hecho, podemos usar parada temprana y monitorizar el rendimiento en un conjunto de validación interno para detener la adición de nuevos árboles cuando la puntuación de validación no mejore. Este es un ejemplo que muestra cómo hacer esto de forma automática:\nhgbdt = HistGradientBoostingRegressor(early_stopping=True, random_state=0) cv_results_hgbdt = cross_validate( hgbdt, X, y, cv=cv, return_train_score=True, return_estimator=True ) cv_results_hgbdt[\"train_score\"].mean(), cv_results_hgbdt[\"train_score\"].std() (0.8802093174685013, 0.009772033922083758)  Vemos que la puntuación de entrenamiento no es perfecta, lo que significa que nuestro modelo se ha detenido mucho antes de añadir demasiados árboles. Podemos comprobar el rendimiento de generalización para asegurar que es un modelo equivalente al random forest previo:\ncv_results_hgbdt[\"test_score\"].mean(), cv_results_hgbdt[\"test_score\"].std() (0.8075456252855009, 0.030400979794505564)  Observamos que, de media, el rendimiento del modelo es tan bueno como un gran random forest. Por último, podemos comprobar cuántos árboles ha usado por cada iteración de validación cruzada:\nfor idx, est in enumerate(cv_results_hgbdt[\"estimator\"]): print( f\"Para la iteración CV {idx + 1}, se han construido {est.n_iter_} árboles\" ) Para la iteración CV 1, se han construido 60 árboles Para la iteración CV 2, se han construido 50 árboles Para la iteración CV 3, se han construido 46 árboles Para la iteración CV 4, se han construido 29 árboles Para la iteración CV 5, se han construido 33 árboles Para la iteración CV 6, se han construido 33 árboles Para la iteración CV 7, se han construido 36 árboles Para la iteración CV 8, se han construido 31 árboles Para la iteración CV 9, se han construido 24 árboles Para la iteración CV 10, se han construido 23 árboles  Por lo tanto, vemos que nunca usamos más de 60 árboles, antes de entrar en la zona de overfitting que observamos en la curva de validación.\nIncluso si este modelo no es tan fuerte como un gran random forest, es más pequeño lo que significa que puede ser más rápido para predecir y usará menos memoria (RAM) en las máquinas donde se despliegue. Esta es una ventaja práctica de los árboles gradient boosting con parada temprana sobre random forest con una gran número de profundidad de árboles.\nResumen En este post hemos discutido los predictores de conjunto, que son un tipo de predictores que combinan predictores más simples juntos. Hemos visto dos estrategias:\n una basada en muestras bootstrap que permite a los predictores ser entrenados en paralelo; la otra llamada boosting que entrena los predictores secuencialmente.  De estas dos familias nos hemos enfocado principalmente en proporcionar intuiciones sobre la maquinaria interna de los modelos de random forest y gradient-boosting, los cuales son métodos de última generación.\nAlgunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Parada temprana en gradient boosting Combinación de predictores usando stacking  ","description":"","tags":["ensemble models","bagging","bootstrap","boosting","random forest","gradient-boosting","parada temprana","early-stopping"],"title":"Modelos de conjunto","uri":"/posts/modelos-conjunto/"},{"categories":["tutoriales"],"content":"En este post presentaremos en detalle los modelos de árbol de decisión, explicándolos tanto para problemas de clasificación como de regresión. También mostraremos qué hiperparámetros de los árboles de decisión tienen importancia en su rendimiento, permitiendo encontrar el mejor equilibrio entre underfitting y overfitting.\nArboles de decisión en clasificación Vamos a explicar cómo usar árboles de decisión para entrenar datos usando un problema simple de clasificación usando el dataset de pingüinos.\nimport pandas as pd penguins = pd.read_csv(\"../data/penguins_classification.csv\") culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"] target_column = \"Species\" También dividimos los datos en dos subconjuntos para investigar cómo los árboles predecirán valores basados en un dataset externo.\nfrom sklearn.model_selection import train_test_split X, y = penguins[culmen_columns], penguins[target_column] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) Un clasificador lineal definirá una separación lineal para dividir clases usando una combinación lineal de las variables de entrada. En nuestro espacio bidimensional, significa que un clasificador lineal definirá algunas líneas oblícuas que mejor separen nuestras clases. A continuación, definimos una función que, dado un conjunto de puntos de datos y un clasificador, dibujará los límites de decisión aprendidos por el clasificador.\nAsí, para un clasificador lineal obtendremos los siguientes límites de decisión. Esas líneas límite indican dónde cambia el modelo su predicción de una clase a otra.\nfrom sklearn.linear_model import LogisticRegression linear_model = LogisticRegression() linear_model.fit(X_train, y_train) LogisticRegression()  import matplotlib.pyplot as plt import seaborn as sns from helpers.plotting import DecisionBoundaryDisplay palette = [\"tab:red\", \"tab:blue\", \"black\"] DecisionBoundaryDisplay.from_estimator( linear_model, X_train, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=palette) plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\") _ = plt.title(\"Límites de decisión usando regresión logística\") Vemos que las líneas son una combinación de variables de entrada, dado que no son perpendiculares a ningún eje específico. De hecho, es debido a la parametrización del modelo, controlado por los pesos y la constante (intercept) del modelo.\nAdemás, parece que el modelo lineal podría ser un buen candidato para este problema, ya que ofrece una buena precisión.\nlinear_model.fit(X_train, y_train) test_score = linear_model.score(X_test, y_test) print(f\"Precisión de la regresión logística {test_score:.2f}\") Precisión de la regresión logística 0.98  A diferencia de los modelos lineales, los árboles de decisión son modelos no paramétricos: no están controlados por una función de decisión matemática y no tienen pesos o intercept que optimizar.\nDe hecho, los árboles de decisión dividen el espacio considerando una única feature a la vez. Vamos a ilustrar este comportamiento haciendo que un árbol de decisión haga una única división para particionar el espacio de features.\nfrom sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier(max_depth=1) tree.fit(X_train, y_train) DecisionTreeClassifier(max_depth=1)  DecisionBoundaryDisplay.from_estimator( tree, X_train, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=palette) plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\") _ = plt.title(\"Límites de decisión usando un árbol de decisión\") Las particiones encontradas por el algoritmo separan los datos a lo largo del eje “Culmen Depth”, descartando la variable “Culmen Length”. Por tanto, destacar que un árbol de decisión no usa una combinación de features cuando hace una división. Podemos ver más en profundidad la estructura de un árbol.\nfrom sklearn.tree import plot_tree _, ax = plt.subplots(figsize=(8, 6)) _ = plot_tree(tree, feature_names=culmen_columns, class_names=tree.classes_, impurity=False, ax=ax) Vemos que la división se ha hecho a partir de la variable culmen depth. El dataset original se ha dividido en 2 conjuntos basados en la anchura del pico (inferior o superior a 16.45 mm).\nEsta partición del dataset minimiza la diversidad de clases en cada subpartición. Esta medida también se conoce como criterion, y es un parámetro configurable.\nSi miramos más de cerca la partición, vemos que la muestra superior a 16.45 pertenece principalmente a la clase Adelie. Mirando los valores, observamos 103 Adelie individuales en este espacio. También contamos 52 muestras de Chinstrap y 6 muestras de Gentoo. Podemos hacer una interpretación similar para la partición definida por un umbral inferior a 16.45 mm. En este caso, la clase más representativa es la especie Gentoo.\nVeamos cómo trabaja nuestro árbol como predictor. Empecemos viendo la clase predicha cuando culmen depth es inferior al umbral.\nsample_1 = pd.DataFrame( {\"Culmen Length (mm)\": [0], \"Culmen Depth (mm)\": [15]} ) tree.predict(sample_1) array(['Gentoo'], dtype=object)  La clase predicha es Gentoo. Ahora podemos verificar si pasamos a un culmen depth superior al umbral.\nsample_2 = pd.DataFrame( {\"Culmen Length (mm)\": [0], \"Culmen Depth (mm)\": [17]} ) tree.predict(sample_2) array(['Adelie'], dtype=object)  En este caso el árbol predice la especie Adelie. Por tanto, podemos concluir que un clasificador de árbol de decisión predecirá la clase más representativa dentro de una partición. Durante el entrenamiento, tenemos un recuento de las muestras de cada partición y, por tanto, podemos calcular la probabilidad de pertenecer a una clase específica dentro de esa partición.\ny_pred_proba = tree.predict_proba(sample_2) y_proba_class_0 = pd.Series(y_pred_proba[0], index=tree.classes_) y_proba_class_0.plot.bar() plt.ylabel(\"Probabilidad\") _ = plt.title(\"Probabilidad de pertenecer a una clase de pingüino\") Calcularemos manualmente las diferentes probabilidades directamente de la estructura del árbol.\nadelie_proba = 103 / 161 chinstrap_proba = 52 / 161 gentoo_proba = 6 / 161 print( f\"Probabilidades para las diferentes clases:\\n\" f\"Adelie: {adelie_proba:.3f}\\n\" f\"Chinstrap: {chinstrap_proba:.3f}\\n\" f\"Gentoo: {gentoo_proba:.3f}\" ) Probabilidades para las diferentes clases: Adelie: 0.640 Chinstrap: 0.323 Gentoo: 0.037  También es importante remarcar que culmen length ha sido descartada por el momento. Significa que cualquiera que sea el valor dado, no se usará durante la predicción.\nsample_3 = pd.DataFrame( {\"Culmen Length (mm)\": [10_000], \"Culmen Depth (mm)\": [17]} ) tree.predict_proba(sample_3) array([[0.63975155, 0.32298137, 0.03726708]])  Volviendo a nuestro problema de clasificación, la división encontrada con un máximo de profundidad de 1 no es lo suficientemente poderosa para separar las tres especies y la precisión del modelo es baja comparada con el modelo lineal.\ntree.fit(X_train, y_train) test_score = tree.score(X_test, y_test) print(f\"Precisión del DecisionTreeClassifier {test_score:.2f}\") Precisión del DecisionTreeClassifier 0.78  No es una sorpresa. Vimos anteriormente que una única feature no sería capaz de separar todas las especies. Sin embargo, a partir del análisis anterior vimos que usando ambas features deberíamos ser capaces de obtener buenos resultados.\nArboles de decisión para regresión Vamos a presentar cómo trabajan los árboles de decisión en problemas de regresión. Mostraremos las diferencias con los árboles de decisión que vimos anteriormente.\nEn primer lugar, cargaremos el dataset de pingüinos específicamente para resolver problemas de regresión.\npenguins = pd.read_csv(\"../data/penguins_regression.csv\") feature_name = \"Flipper Length (mm)\" target_name = \"Body Mass (g)\" X_train, y_train = penguins[[feature_name]], penguins[[target_name]] Para ilustrar cómo predicen un problema de regresión los árboles de decisión crearemos un dataset sintético conteniendo todos los posibles tamaños de aleta, desde el mínimo al máximo de los datos originales.\nimport numpy as np X_test = pd.DataFrame(np.arange(X_train[feature_name].min(), X_train[feature_name].max()), columns=[feature_name]) Usamos aquí el término “test” para referirnos a los datos que no se han usado para el entrenamiento. No se debe confundir con los datos provenientes de una división entrenamiento-prueba, ya que se generó en intervalos igualmente espaciados para la evaluación visual de las predicciones.\nTengamos en cuenta que esto es metodológicamente válido porque nuestro objetivo es obtener una comprensión intuitiva de la forma de la función de decisión de los árboles de decisión aprendidos.\nSin embargo, calcular una métrica de evaluación en tal conjunto de prueba sintético no tendría sentido, ya que el dataset sintético no sigue la misma distribución que los datos del mundo real en los cuales se desplegará el modelo.\nsns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) _ = plt.title(\"Ilustración del dataset de regresión usado\") En primer lugar, ilustraremos la diferencia entre un modelo lineal y un ábol de decisión.\nfrom sklearn.linear_model import LinearRegression linear_model = LinearRegression() linear_model.fit(X_train, y_train) y_predicted = linear_model.predict(X_test) sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) plt.plot(X_test[feature_name], y_predicted, label=\"Regresión lineal\", linestyle=\"--\") plt.legend() _ = plt.title(\"Función de predicción usando LinearRegression\") En la gráfica anterior, vemos que una LinearRegression no regularizada es capaz de entrenar los datos. Una característica de este modelo es que todas las nuevas predicciones estarán en la línea.\nsns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) plt.plot(X_test[feature_name], y_predicted, label=\"Regresión lineal\", linestyle=\"--\") plt.scatter(X_test[::3], y_predicted[::3], label=\"Predicciones\", color=\"tab:orange\") plt.legend() _ = plt.title(\"Función de predicción usando LinearRegression\") Al contrario que los modelos lineales, los árboles de decisión son modelos no paramétricos: no hacen asunciones sobre la forma en que se distribuyen los datos. Esto afectará al esquema de predicción. Destacaremos las diferencias repitiendo el experimento anterior.\nfrom sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(max_depth=1) tree.fit(X_train, y_train) y_predicted = tree.predict(X_test) sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) plt.plot(X_test[feature_name], y_predicted, label=\"Arbol decisión\", linestyle=\"--\") plt.legend() _ = plt.title(\"Función de predicción usando DecisionTreeRegressor\") Como vemos, el modelo de árbol de decisión no tiene una distribución a priori de los datos y no termina con una línea recta para correlar el tamaño de la aleta y la masa corporal.\nEn su lugar, observamos que las predicciones del árbol son constantes por tramos. De hecho, nuestro espacio de features se dividió en dos particiones. Vamos a comprobar la estructura del árbol para ver el umbral encontrado durante el entrenamiento.\nfrom sklearn.tree import plot_tree _, ax = plt.subplots(figsize=(8, 6)) _ = plot_tree(tree, feature_names=feature_name, ax=ax) El umbral para nuestra feature (tamaño de la aleta) es 206.5 mm. Los valores predichos en cada lado de la división son dos constantes: 3698.709 g y 5032.364 g. Esos valores corresponden a los valores medios de las muestras de entrenamiento de cada partición.\nEn clasificación, vimos que incrementando la profundidad del árbol obteníamos límites de decisión más complejos. Vamos a comprobar el efecto de incrementar la profundidad en un problema de regresión:\ntree = DecisionTreeRegressor(max_depth=3) tree.fit(X_train, y_train) y_predicted = tree.predict(X_test) sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) plt.plot(X_test[feature_name], y_predicted, label=\"Arbol decisión\", linestyle=\"--\") plt.legend() _ = plt.title(\"Función de predicción usando DecisionTreeRegressor\") Incrementar la profundidad del árbol incrementará el número de particiones y, por tanto, el número de valores constantes que el árbol es capaz de predecir.\n_, ax = plt.subplots(figsize=(8, 6)) _ = plot_tree(tree, feature_names=feature_name, ax=ax) Hiperparámetros del árbol de decisión Vamos a mostrar la importancia de algunos hiperparámetros clave en los árboles de decisión y demostraremos sus efectos en problemas de clasificación y regresión.\nEn primer lugar, cargaremos los datasets de clasificación y regresión.\ndata_clf_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"] target_clf_column = \"Species\" data_clf = pd.read_csv(\"../data/penguins_classification.csv\") data_reg_columns = [\"Flipper Length (mm)\"] target_reg_column = \"Body Mass (g)\" data_reg = pd.read_csv(\"../data/penguins_regression.csv\") Creación de helpers Vamos a crear algunas funciones helpers para dibujar las muestras de datos así como los límites de decisión para la clasificación y las líneas de regresión para la regresión.\nfrom helpers.plotting import DecisionBoundaryDisplay def fit_and_plot_classification(model, data, feature_names, target_names): model.fit(data[feature_names], data[target_names]) if data[target_names].nunique() == 2: palette = [\"tab:red\", \"tab:blue\"] else: palette = [\"tab:red\", \"tab:blue\", \"black\"] DecisionBoundaryDisplay.from_estimator( model, data[feature_names], response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=data, x=feature_names[0], y=feature_names[1], hue=target_names, palette=palette) plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') def fit_and_plot_regression(model, data, feature_names, target_names): model.fit(data[feature_names], data[target_names]) data_test = pd.DataFrame( np.arange(data.iloc[:, 0].min(), data.iloc[:, 0].max()), columns=data[feature_names].columns, ) target_predicted = model.predict(data_test) sns.scatterplot( x=data.iloc[:, 0], y=data[target_names], color=\"black\", alpha=0.5) plt.plot(data_test.iloc[:, 0], target_predicted, linewidth=4) Efecto del parámetro max_depth El hiperparámetro max_depth controla la complejidad global de un árbol de decisión. Este hiperparámetro permite obtener un equilibrio entre underfit y overfit. Vamos a construir un árbol poco profundo y después otro más profundo, tanto para clasificación como para regresión, para comprender el impacto del parámetro.\nPrimero podemos establecer el parámetro max_depth a un valor muy bajo.\nmax_depth = 2 tree_clf = DecisionTreeClassifier(max_depth=max_depth) tree_reg = DecisionTreeRegressor(max_depth=max_depth) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Arbol poco profundo de clasificación con max_depth = {max_depth}\") fit_and_plot_regression( tree_reg, data_reg, data_reg_columns, target_reg_column) _ = plt.title(f\"Arbol poco profundo de regresión con max_depth = {max_depth}\") Ahora incrementemos el parámetro max_depth para comprobar la diferencia observando la función de decisión.\nmax_depth = 30 tree_clf = DecisionTreeClassifier(max_depth=max_depth) tree_reg = DecisionTreeRegressor(max_depth=max_depth) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Arbol poco profundo de clasificación con max_depth = {max_depth}\") fit_and_plot_regression( tree_reg, data_reg, data_reg_columns, target_reg_column) _ = plt.title(f\"Arbol poco profundo de regresión con max_depth = {max_depth}\") Tanto para clasificación como regresión observamos que incrementar la profundidad nos hará el modelo más expresivo. Sin embargo, un árbol que es demasiado profundo sufrirá de overfitting, creando particiones que solo son correctas para “valores atípicos” (muestras ruidosas). max_depth es uno de los hiperparámetros que se debe optimizar a través de validación cruzada y grid search.\nfrom sklearn.model_selection import GridSearchCV param_grid = {\"max_depth\": np.arange(2, 10, 1)} tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid) tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Optima profundidad encontrada via CV: \" f\"{tree_clf.best_params_['max_depth']}\") fit_and_plot_regression( tree_reg, data_reg, data_reg_columns, target_reg_column) _ = plt.title(f\"Optima profundidad encontrada via CV: \" f\"{tree_reg.best_params_['max_depth']}\") Con este ejemplo, vemos que no existe un único valor que sea óptimo para cada dataset. Por tanto, se requiere que este parámetro sea optimizado para cada aplicación.\nOtros hiperparámetros en áboles de decisión El hiperparámetro max_depth controla la complejidad global del árbol. Este parámetro es adecuado bajo la asunción de que un árbol construido es simétrico. Sin embargo, no hay garantía de que un árbol será simétrico. De hecho, el rendimiento de generalización óptimo se puede alcanzar haciendo crecer algunas ramas más que otras.\nConstruiremos un dataset donde ilustraremos esta asimetría. Generaremos un dataset compuesto de 2 subconjuntos: un subconjunto donde el árbol debe encontrar una separación clara y otro subconjunto donde las muestras de ambas clases están mezcladas. Esto implica que un árbol de decisión necesitará más divisiones para clasificar adecuadamente las muestras del segundo subconjunto que las del primer subconjunto.\nfrom sklearn.datasets import make_blobs data_clf_columns = [\"Feature #0\", \"Feature #1\"] target_clf_column = \"Class\" # Blobs that will be interlaced X_1, y_1 = make_blobs( n_samples=300, centers=[[0, 0], [-1, -1]], random_state=0) # Blobs that will be easily separated X_2, y_2 = make_blobs( n_samples=300, centers=[[3, 6], [7, 0]], random_state=0) X = np.concatenate([X_1, X_2], axis=0) y = np.concatenate([y_1, y_2]) data_clf = np.concatenate([X, y[:, np.newaxis]], axis=1) data_clf = pd.DataFrame( data_clf, columns=data_clf_columns + [target_clf_column]) data_clf[target_clf_column] = data_clf[target_clf_column].astype(np.int32) sns.scatterplot(data=data_clf, x=data_clf_columns[0], y=data_clf_columns[1], hue=target_clf_column, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Dataset sintético\") Primero entrenaremos un árbol poco profundo con max_depth=2. Cabría esperar que esta profundidad sea suficiente para separar los bloques que son fáciles de separar.\nmax_depth = 2 tree_clf = DecisionTreeClassifier(max_depth=max_depth) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Arbol de decisión con max-depth = {max_depth}\") Como se esperaba, vemos que el bloque azul de la derecha y rojo de la parte superior son fácilmente separables. Sin embargo, se requerirán más divisiones para dividir mejor el bloque donde los puntos de datos rojos y azules están mezclados.\nDe hecho, vemos que el bloque rojo de la parte superior y el azul de la parte derecha están perfectamente separados. Sin embargo, el árbol aún comete errores en el área donde los bloques están mezclados juntos. Verifiquemos la representación del árbol.\n_, ax = plt.subplots(figsize=(10, 10)) _ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns) Vemos que la rama derecha alcanza una clasificación perfecta. Ahora, incrementemos la profundidad para verificar cómo crecerá el árbol.\nmax_depth = 6 tree_clf = DecisionTreeClassifier(max_depth=max_depth) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Arbol de decisión con max-depth = {max_depth}\") _, ax = plt.subplots(figsize=(11, 7)) _ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns) Como se esperaba, la rama izquierda del árbol continuó creciendo mientras que no se hacían más divisiones en la rama derecha. Fijar el parámetro max_depth cortaría el árbol horizontalmente a un nivel específico, ya sea o no más beneficioso que una rama siga creciendo.\nLos hiperparámetros min_samples_leaf, min_samples_split, max_leaf_nodes o min_impurity_decrease permiten crecimientos asimétricos de árboles y aplicar una restricción a nivel de hojas o nodos. Comprobaremos el efecto de min_samples_leaf.\nmin_samples_leaf = 60 tree_clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf) fit_and_plot_classification( tree_clf, data_clf, data_clf_columns, target_clf_column) _ = plt.title(f\"Arbol de decisión con hojas de al menos {min_samples_leaf} muestras\") _, ax = plt.subplots(figsize=(10, 7)) _ = plot_tree(tree_clf, ax=ax, feature_names=data_clf_columns) Este hiperparámetro permite tener hojas con un mínimo número de muestras y, de lo contrario, no se buscarán más divisiones. Por lo tanto, estos hiperparámetros podrían ser una alternativa para corregir el hiperparámetro max_depth.\nEjercicio Vamos a poner en práctica lo aprendido en este post con un ejemplo. Para ello vamos a usar el dataset ames_housing_no_missing.csv.\names_housing = pd.read_csv(\"../data/ames_housing_no_missing.csv\") target_name = \"SalePrice\" X = ames_housing.drop(columns=target_name) y = ames_housing[target_name] Para simplificar sólo utilizaremos las variables numéricas definidas a continuación:\nnumerical_features = [ \"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"BedroomAbvGr\", \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\", \"GarageCars\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\", ] X = X[numerical_features] Queremos comparar el rendimiendo de generalización de un árbol de decisión y una regresión lineal. Para ello, crearemos dos modelos predictivos por separado y los evaluaremos con una validación cruzada de 10 particiones. Por tanto, usaremos sklearn.linear_model.LinearRegression y sklearn.tree.DecisionTreeRegressor para crear los modelos. Usaremos los parámetros por defecto para ambos modelos.\nTengamos en cuenta que un modelo lineal requiere escalar las variables numéricas. Por tanto, usaremos sklearn.preprocessing.StandardScaler.\nComparando las puntuaciones de prueba de validación cruzada para ambos modelos, partición a partición, ¿Cuántas veces el modelo lineal es mejor en dicha puntuación que el modelo de árbol de decisión?\nfrom sklearn.model_selection import cross_validate from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler linear_model = make_pipeline(StandardScaler(), LinearRegression()) cv_results_lm = cross_validate(linear_model, X, y, cv=10, return_estimator=True) cv_results_lm[\"test_score\"] array([0.76129977, 0.80685587, 0.81188989, 0.66592199, 0.79785737, 0.76868787, 0.74564258, 0.71822127, 0.31479306, 0.78635221])  tree = DecisionTreeRegressor() cv_results_tree = cross_validate(tree, X, y, cv=10, return_estimator=True) cv_results_tree[\"test_score\"] array([0.54944429, 0.71949314, 0.71557951, 0.62398197, 0.72493666, 0.64684232, 0.47305743, 0.62410671, 0.62849893, 0.72697255])  print(f\"¿Cuantas veces es mejor la regresión lineal sobre el árbol de decisión?: \" f\"{sum(cv_results_lm['test_score'] \u003e cv_results_tree['test_score'])} veces\") ¿Cuantas veces es mejor la regresión lineal sobre el árbol de decisión?: 9 veces  En lugar de usar los parámetros por defecto para el árbol de decisión regresor, optimizaremos max_depth en el árbol. Probaremos max_depth desde el nivel 1 al nivel 15. Usaremos validación cruzada anidada para evaluar un grid search. Definamos cv=10 tanto para la validación cruzada interna como externa.\n¿Cuál es la profundidad óptima para el árbol?\nfrom sklearn.model_selection import GridSearchCV param_grid = {\"max_depth\": np.arange(1, 16, 1)} tree = GridSearchCV(DecisionTreeRegressor(random_state=42), cv=10, param_grid=param_grid) cv_results_tree = cross_validate(tree, X, y, cv=10, return_estimator=True) cv_results_tree[\"test_score\"] array([0.64240567, 0.76720246, 0.69448254, 0.48481112, 0.76735608, 0.65537153, 0.66928018, 0.76053576, 0.47306355, 0.70204973])  for estimator in cv_results_tree[\"estimator\"]: print(estimator.best_params_) {'max_depth': 7} {'max_depth': 5} {'max_depth': 6} {'max_depth': 6} {'max_depth': 8} {'max_depth': 10} {'max_depth': 5} {'max_depth': 6} {'max_depth': 8} {'max_depth': 8}  En lugar de usar únicamente las variables numéricas usaremos el dataset completo. Vamos a crear un preprocesador para manejar por separado las variables numéricas y las columnas categóricas. Por simplicidad asumiremos que:\n las columnas categóricas serán aquellas cuyo tipo de datos es object; usaremos un OrdinalEncoder para codificar las columnas categóricas; las columnas numéricas serán aquellas cuyo tipo de datos no sea object.  Además, estableceremos el parámetro max_depth del árbol a 7 y avaluaremos el modelo usando cross_validate.\n¿Este modelo es mejor o peor que el entrenado solo con las variables numéricas?\nfrom sklearn.preprocessing import OrdinalEncoder from sklearn.compose import make_column_selector as selector from sklearn.compose import make_column_transformer from sklearn.compose import ColumnTransformer ames_housing = pd.read_csv(\"../data/ames_housing_no_missing.csv\") target_name = \"SalePrice\" X = ames_housing.drop(columns=target_name) y = ames_housing[target_name] categorical_processor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1) preprocessor = make_column_transformer( (categorical_processor, selector(dtype_include=object)), (\"passthrough\", selector(dtype_exclude=object)) ) model = make_pipeline(preprocessor, DecisionTreeRegressor(max_depth=7)) cv_results = cross_validate(model, X, y, cv=10, return_estimator=True) cv_results[\"test_score\"] array([0.73480057, 0.7919978 , 0.83190014, 0.79477965, 0.81379685, 0.84669201, 0.5312336 , 0.75213134, 0.59825712, 0.75729819])  La respuesta es que, en este caso, el modelo entrenado con todas las variables (numéricas y categóricas) es mejor que el modelo entrenado únicamente con las variables numéricas.\nResumen  Los árboles de decisión son adecuados tanto para problemas de clasificación como de regresión; son modelos no paramátricos; no son capaces de extrapolar; son sensibles al ajuste de hiperparámetros.  Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Ejemplo de árbol de decisión regresor Ejemplo de árbol de decisión clasificador Comprensión de la estructura de árbol en scikit-learn Poda de árboles de decisión  ","description":"","tags":["decision tree models"],"title":"Modelos de árbol de decisión","uri":"/posts/modelos-arbol-decision/"},{"categories":["tutoriales"],"content":"En este post profundizaremos en los detalles de los modelos que utilizan parametrización lineal. También veremos cómo usar esta familia de modelos tanto para problemas de clasificación como de regresión. Además, explicaremos cómo enfrentarnos al overfitting usando regularización. Por último, mostraremos cómo los modelos lineales se pueden usar con datos que no presentan linealidad.\nRegresión lineal sin scikit-learn Antes de presentar las clases disponibles de scikit-learn, vamos a ofrecer algunas ideas con un ejemplo simple. Usaremos el dataset que contiene medidas tomadas de pingüinos.\nimport pandas as pd penguins = pd.read_csv(\"penguins_regression.csv\") penguins.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  flipper_length_mm body_mass_g     0 181 3750   1 186 3800   2 195 3250   3 193 3450   4 190 3650     Formularemos el siguiente problema: usando la longitud de la aleta (flipper length) nos gustaría inferir su peso corporal.\nimport seaborn as sns import matplotlib.pyplot as plt feature_name = \"flipper_length_mm\" target_name = \"body_mass_g\" X, y = penguins[[feature_name]], penguins[[target_name]] ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) ax.set_title(\"Longitud aleta en función del peso corporal\"); La función scatterplot de seaborn toma como entrada el dataframe completo y los parámetros x e y permiten especificar las columnas a dibujar. Esta función devuelve un eje de matplotlib (denominado ax en el ejemplo anterior) que se puede usar para añadir elementos en el mismo eje (como un título).\nEn este problema, el peso corporal es nuestro objetivo. Es una variable continua que varía aproximadamente entre 2700 g y 6300 g. Por tanto, es un problema de regresión. También veremos que existe una relación casi lineal entre el peso corporal del pingüino y la longitud de su aleta. Cuanto más larga es, más pesado el pingüino.\nPor lo tanto, podríamos llegar a una fórmula simple, donde dada la longitud de la aleta podríamos calcular el peso corporal usando una relación lineal de la forma y = a * x + b, donde a y b son los 2 parámetros de nuestro modelo.\ndef linear_model_flipper_mass(flipper_lenght, weight_flipper_length, intercept_body_mass): \"\"\"Modelo lineal de la forma y = a * x + b\"\"\" body_mass = weight_flipper_length * flipper_lenght + intercept_body_mass return body_mass Usando el modelo que hemos definido anteriormente, podemos verificar los valores predichos de peso corporal para un rango de longitudes de aletas. Estableceremos weight_flipper_length para que sea 45 e intercept_body_mass sea -5000.\nimport numpy as np weight_flipper_length = 45 intercept_body_mass = -5000 flipper_length_range = np.linspace(X.min(), X.max(), num=300) predicted_body_mass = linear_model_flipper_mass( flipper_length_range, weight_flipper_length, intercept_body_mass ) Podemos dibujar todas las instancias y la predicción del modelo lineal.\nlabel = \"{0:.2f} (g / mm) * flipper length + {1:.2f} (g)\" ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) ax.plot(flipper_length_range, predicted_body_mass) _ = ax.set_title(label.format(weight_flipper_length, intercept_body_mass)) La variable weight_flipper_length es un peso aplicado a la feature flipper_length para realizar la inferencia. Cuando este coeficiente es positivo, significa que los pingüinos con las aletas más largas tendrán mayor peso corporal. Si el coeficiente es negativo, significa que los pingüinos con las aletas más cortas tendrán mayor peso corporal. Gráficamente, este coeficiente se representa por la pendiente de la curva de la gráfica. A continuación, mostraremos cómo sería la curva cuando el coeficiente weight_flipper_length es negativo.\nweight_flipper_length = -40 intercept_body_mass = 13000 predicted_body_mass = linear_model_flipper_mass( flipper_length_range, weight_flipper_length, intercept_body_mass ) ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) ax.plot(flipper_length_range, predicted_body_mass) _ = ax.set_title(label.format(weight_flipper_length, intercept_body_mass)) En nuestro caso, este coeficiente tiene una unidad con significado: g/mm. Por ejemplo, un coeficiente de 40 g/mm significa que por cada milímetro adicional de longitud de aleta, el peso corporal predicho aumentará 40 g.\nbody_mass_180 = linear_model_flipper_mass( flipper_lenght=180, weight_flipper_length=40, intercept_body_mass=0) body_mass_181 = linear_model_flipper_mass( flipper_lenght=181, weight_flipper_length=40, intercept_body_mass=0) print(f\"El peso corporal para una longitud de aleta de 180 mm \" f\"es {body_mass_180} g \\n\" f\"El peso corporal para una longitud de aleta de 181 mm \" f\"es {body_mass_181} g\") El peso corporal para una longitud de aleta de 180 mm es 7200 g El peso corporal para una longitud de aleta de 181 mm es 7240 g  También podemos observar que tenemos un parámetro intercept_body_mass en nuestro modelo. Este parámetro corresponde al valor del eje y si flipper_length=0 (que en nuestro caso es solo una consideración matemática, ya que en nuestros datos el valor de flipper_length solo va de 170 mm hasta 230 mm). Este valor de y, cuando x=0 se denomina constante (intercept). Si intercept_body_mass es 0, la curva pasará por el origen:\nweight_flipper_length = 25 intercept_body_mass = 0 # Redefinición de longitud de aleta para empezar en 0 para dibujar el valor de intercept flipper_length_range = np.linspace(0, X.max(), num=300) predicted_body_mass = linear_model_flipper_mass( flipper_length_range, weight_flipper_length, intercept_body_mass) ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) ax.plot(flipper_length_range, predicted_body_mass) _ = ax.set_title(label.format(weight_flipper_length, intercept_body_mass)) De lo contrario, pasará por el valor de intercept_body_mass:\nweight_flipper_length = 45 intercept_body_mass = -5000 predicted_body_mass = linear_model_flipper_mass( flipper_length_range, weight_flipper_length, intercept_body_mass) ax = sns.scatterplot(data=penguins, x=feature_name, y=target_name, color=\"black\", alpha=0.5) ax.plot(flipper_length_range, predicted_body_mass) _ = ax.set_title(label.format(weight_flipper_length, intercept_body_mass)) Regresión lineal usando scikit-learn Cuando hacemos machine learning, estamos interesados en seleccionar el modelo que minimice al máximo el error en los datos disponibles. Hemos visto anteriormente que podríamos implementar un enfoque de fuerza bruta, variando los pesos y la constante y seleccionando el modelo con el menor error. Afortunadamente, este problema de encontrar los mejores valores de hiperparámetros (es decir, que dan como resultado el menor error) se puede solucionar sin necesidad de comprobar cada potencial combinación de parámetros. De hecho, este problema tiene una solución cerrada: los mejores valores de hiperparámetros se encuentran resolviendo una ecuación. Esto evita la necesidad de búsqueda por fuerza bruta. Esta estrategia está implementada en scikit-learn.\nfrom sklearn.linear_model import LinearRegression linear_regression = LinearRegression() linear_regression.fit(X, y) LinearRegression()  La instancia linear_regression almacenará los valores de parámetros en los atributos coef_ e intercept_. Podemos comprobar que el modelo óptimo encontrado es:\nweight_flipper_length = linear_regression.coef_[0] weight_flipper_length array([49.68556641])  intercept_body_mass = linear_regression.intercept_ intercept_body_mass array([-5780.83135808])  Usaremos el peso e intercept para dibujar el modelo encontrado usando scikit-learn.\nflipper_length_range = np.linspace(X.min(), X.max(), num=300) predicted_body_mass = ( weight_flipper_length * flipper_length_range + intercept_body_mass) sns.scatterplot(x=X[feature_name], y=y[target_name], color=\"black\", alpha=0.5) plt.plot(flipper_length_range, predicted_body_mass) _ = plt.title(\"Modelo usando LinearRegression de scikit-learn\") Vamos a calcular el error cuadrático medio.\nfrom sklearn.metrics import mean_squared_error inferred_body_mass = linear_regression.predict(X) model_error = mean_squared_error(y, inferred_body_mass) print(f\"El error cuadrático medio del modelo óptimo es {model_error:.2f}\") El error cuadrático medio del modelo óptimo es 154546.19  Un modelo de regresión lineal minimiza el error cuadrático medio en el conjunto de entrenamiento. Esto significa que los parámetros obtenidos tras el entrenamiento (es decir, coef_ e intercept_) son los parámetros óptimos que minimizan el error cuadrático medio. En otras palabras, cualquier otra combinación de parámetros producirá un modelo con un error cuadrático medio mayor en el conjunto de entrenamiento.\nSin embargo, el error cuadrático medio es difícil de interpretar. El error absoluto medio es más intuitivo, dado que proporciona un error en las mismas unidades que las del objetivo.\nfrom sklearn.metrics import mean_absolute_error model_error = mean_absolute_error(y, inferred_body_mass) print(f\"El error absoluto medio del modelo óptimo es {model_error:.2f} g\") El error absoluto medio del modelo óptimo es 313.00 g  Un error absoluto medio de 313 significa que, de media, nuestro modelo produce un error de +/- 313 gramos cuando predice el peso corporal de un pingüino dada la longitud de su aleta.\nRegresión lineal sin relación de linealidad entre los datos y el objetivo Aunque la parametrización de modelos lineales no se adapte de forma nativa al problema en cuestión, aún es posible hacer modelos lineales más expresivos mediante la ingeniería de features adicionales. Un pipeline de machine learning que combina un paso de ingeniería de features no lineales seguido por un paso de regresión lineal puede considerarse un modelo de regresión no lineal en su conjunto.\nPara ilustar estos conceptos, vamos a generar un dataset.\nrng = np.random.RandomState(0) n_sample = 100 data_max, data_min = 1.4, -1.4 len_data = (data_max - data_min) # sort the data to make plotting easier later X = np.sort(rng.rand(n_sample) * len_data - len_data / 2) noise = rng.randn(n_sample) * .3 y = X ** 3 - 0.5 * X ** 2 + noise Para dibujar fácilmente el dataset, crearemos un dataframe de pandas que contenga los datos y el objetivo.\nfull_data = pd.DataFrame({\"input_feature\": X, \"target\": y}) _ = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.5) Por convención, en scikit-learn los datos (X) deben ser una matriz 2D de tamaño (n_samples, n_features). Si los datos son un vector 1D, necesitamos redimensionarlos en una matriz con una única columna si el vector representa una feature o una única fila si el vector representa una instancia.\nX = X.reshape(-1, 1) X.shape (100, 1)  linear_regression = LinearRegression() linear_regression.fit(X, y) y_predicted = linear_regression.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Aquí el coeficiente e intercept aprendidos por LinearRegression definen la mejor “línea recta” que ajusta los datos. Vamos a inspeccionar dichos parámetros:\nprint(f\"peso: {linear_regression.coef_[0]:.2f}, \" f\"intercept: {linear_regression.intercept_:.2f}\") peso: 1.18, intercept: -0.29  Es importante tener en cuenta que el modelo aprendido no es capaz de manejar relaciones no lineales entre X e y dado que los modelos lineales asumen una relación lineal entre X e y. Por tanto, existen 3 posibilidades para resolver este problema:\n elegir un modelo que pueda manejar nativamente la no linealidad, diseñar un conjunto más rico de features al incluir conocimiento experto que pueda usarse directamente por un modelo lineal simple, o usar un “kernel” para tener una función de decisión local en lugar de tener un función de decisión global.  Vamos a ilustrar rápidamente el primer punto usando un regresor de árbol de decisión que puede manejar nativamente la no linealidad.\nfrom sklearn.tree import DecisionTreeRegressor tree = DecisionTreeRegressor(max_depth=3).fit(X, y) y_predicted = tree.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") En lugar de tener un modelo que pueda manejarse nativamente con la no linealidad, también podríamos crear nuevas features, derivadas de las features originales, usando algún conocimiento experto. En este ejemplo, sabemos que tenemos una relación cúbica y cuadrática entre X e y (porque hemos generado los datos). Por tanto, podemos crear dos nuevas features (X ** 2 y X ** 3). Este tipo de transformación de denomina expansión de features polinomial.\nX.shape (100, 1)  X_expanded = np.concatenate([X, X ** 2, X ** 3], axis=1) X_expanded.shape (100, 3)  linear_regression.fit(X_expanded, y) y_predicted = linear_regression.predict(X_expanded) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Podemos ver que incluso con un modelo lineal, podemos superar la limitación de linealidad del modelo añadiendo componentes no lineales en el diseño de features adicionales. Aquí, hemos creado nuevas features conociendo la forma en que el objetivo fue generado.\nEn lugar de crear manualmente tales features polinómicas podríamos usar directamente sklearn.preprocessing.PolynomialFeatures. Para demostrar el uso de la clase PolynomialFeatures, usamos un pipeline que en primer lugar transforma las features y luego entrena el modelo de regresión.\nfrom sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures polynomial_regression = make_pipeline( PolynomialFeatures(degree=3), LinearRegression(), ) polynomial_regression.fit(X, y) y_predicted = polynomial_regression.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Como se esperaba, observamos que las predicciones de este pipeline PolynomialFeatures son iguales que las predicciones del modelo lineal entrenado con las features creadas manualmente.\nLa última posibilidad para hacer un modelo lineal más expresivo es usar un “kernel”. En lugar de aprender un peso por feature como enfatizamos anteriormente, se asignará un peso a cada muestra. Sin embargo, no se usarán todas las muestras. Esta es la base de algoritmo de support vector machine (SVM). Para más detalles del este algoritmo se puede acudir a la documentación de scikit-learn.\nEn este caso, solo vamos a desarrollar algunas intuiciones en el poder expresivo relativo de SVM con kernels lineales y no lineales entrenándolo en el mismo dataset.\nEn primer lugar, vamos a considerar un SVM con un kernel lineal:\nfrom sklearn.svm import SVR svr = SVR(kernel=\"linear\") svr.fit(X, y) y_predicted = svr.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Las predicciones de nuestro SVR con un kernel lineal están todas alineadas en una línea recta. SVR(kernel=\"linear\") es de hecho otro ejemplo de modelo lineal.\nEl estimador también se puede configurar para usar un kernel no lineal. Luego, puede aprender una función de predicción que calcule la interacción no lineal entre muestras para las que queremos hacer una predicción y seleccionar muestras del conjunto de entrenamiento.\nEl resultado es otro tipo de modelo de regresión no lineal con una expresividad similar a nuestro pipeline previo de regresión polinómica:\nsvr = SVR(kernel=\"poly\", degree=3) svr.fit(X, y) y_predicted = svr.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Los métodos de kernel como SVR son muy eficientes para datasets pequeños o medianos. Para datasets grandes con n_samples \u003e 10000, a menudo resulta mas eficiente computacionalmente explicitar features de expansión usando PolynomialFeatures u otros transformadores no lineales de scikit-learn, como KBinsDiscretizer or Nystroem.\nVamos a dar una visión intuitiva de las predicciones que podríamos obtener usando nuestro dataset de juguete:\nfrom sklearn.preprocessing import KBinsDiscretizer binned_regression = make_pipeline( KBinsDiscretizer(n_bins=8), LinearRegression(), ) binned_regression.fit(X, y) y_predicted = binned_regression.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") from sklearn.kernel_approximation import Nystroem nystroem_regression = make_pipeline( Nystroem(n_components=5), LinearRegression(), ) nystroem_regression.fit(X, y) y_predicted = nystroem_regression.predict(X) mse = mean_squared_error(y, y_predicted) ax = sns.scatterplot(data=full_data, x=\"input_feature\", y=\"target\") ax.plot(X, y_predicted) _ = ax.set_title(f\"Error cuadrático medio = {mse:.2f}\") Regularización de modelos de regresión lineales Vamos a ver las limitaciones de los modelos de regresión lineales y las ventajas de usar, en su lugar, modelos regularizados. También veremos el preprocesamiento requerido cuando lidiamos con modelos regularizados, además de la regularización de parámetros necesarios para ajustarlos.\nEmpezaremos destacando el problema de overfitting que puede surgir con un simpel modelo de regresión lineal.\nEn primer lugar, carguemos el dataset de la vivienda de California.\nfrom sklearn.datasets import fetch_california_housing X, y = fetch_california_housing(as_frame=True, return_X_y=True) y *= 100 #reescala el objetivo en k$ X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude     0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23   1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22   2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24   3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25   4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25     Ya vimos que los modelos lineales pueden usarse incluso en configuraciones donde X e y no tengan relaciones de linealidad. También vimos que se puede usar el transformador PolynomialFeatures para crear features adicionales, codificando interaciones no lineales entre features.\nAhora usaremos este transformador para aumentar el espacio de features. Posteriormente, entrenaremos un modelo de regresión lineal. Usaremos el conjunto de prueba externo para evaluar las capacidades de generalización de nuestro modelo.\nfrom sklearn.model_selection import cross_validate from sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression linear_regression = make_pipeline(PolynomialFeatures(degree=2), LinearRegression()) cv_results = cross_validate(linear_regression, X, y, cv=10, scoring=\"neg_mean_squared_error\", return_train_score=True, return_estimator=True) Podemos comparar el error cuadrático medio en el conjunto de entrenamiento y prueba para verificar el rendimiento de generalización de nuestro modelo.\ntrain_error = -cv_results[\"train_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: \\n\" f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: 4190.212 +/- 151.123  test_error = -cv_results[\"test_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: \\n\" f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: 13334.943 +/- 20292.681  La puntuación en el conjunto de entrenamiento es mucho mejor. Esta diferencia del rendimiento de generalización entre la puntuación de entrenmiento y la de prueba es una indicación de que nuestro modelo sobreajustó nuestro conjunto de entrenamiento. De hecho, este es uno de los problemas cuando aumentamos el número de características con un transformador PolynomialFeatures. Nuestro modelo se centrará en algunas features específicas. Podemos comprobar los pesos del modelo para tener confirmación de esto. Vamos a crear un dataframe: las columnas contendrán el nombre de la feature mientras que la fila contendrá los valores de los coeficientes almacenados para cada modelo durante la validación cruzada.\nDado que usamos un PolynomialFeature para aumentar los datos, crearemos nombres de features representativos de cada combinación de features. Scikit-learn ofrece el método get_feature_names_out para este propósito. En primer lugar, obtengamos el primer modelo entrenado de la validación cruzada.\nmodel_first_fold = cv_results[\"estimator\"][0] Ahora podemos acceder al PolynomialFeatures entrenado para generar los nombre de las features.\nfeature_names = model_first_fold[0].get_feature_names_out( input_features=X.columns) feature_names array(['1', 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedInc^2', 'MedInc HouseAge', 'MedInc AveRooms', 'MedInc AveBedrms', 'MedInc Population', 'MedInc AveOccup', 'MedInc Latitude', 'MedInc Longitude', 'HouseAge^2', 'HouseAge AveRooms', 'HouseAge AveBedrms', 'HouseAge Population', 'HouseAge AveOccup', 'HouseAge Latitude', 'HouseAge Longitude', 'AveRooms^2', 'AveRooms AveBedrms', 'AveRooms Population', 'AveRooms AveOccup', 'AveRooms Latitude', 'AveRooms Longitude', 'AveBedrms^2', 'AveBedrms Population', 'AveBedrms AveOccup', 'AveBedrms Latitude', 'AveBedrms Longitude', 'Population^2', 'Population AveOccup', 'Population Latitude', 'Population Longitude', 'AveOccup^2', 'AveOccup Latitude', 'AveOccup Longitude', 'Latitude^2', 'Latitude Longitude', 'Longitude^2'], dtype=object)  Finalmente, podemos crear el dataframe conteniendo toda la información.\ncoefs = [est[-1].coef_ for est in cv_results[\"estimator\"]] weights_linear_regression = pd.DataFrame(coefs, columns=feature_names) Ahora vamos a usar un boxplot para ver las variaciones de los coeficientes.\ncolor = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"} weights_linear_regression.plot.box(color=color, vert=False, figsize=(6, 16)) _ = plt.title(\"Coeficientes de la regresión lineal\") Podemos forzar al modelo de regresión lineal a considerar todas las features de un forma más homogénea. De hecho, podríamos forzar a un mayor peso positivo o un menor peso tendente a cero. Esto se conoce como regularización. Usaremos un modelo ridge que fuerza tal comportamiento.\nfrom sklearn.linear_model import Ridge ridge = make_pipeline(PolynomialFeatures(degree=2), Ridge(alpha=100)) cv_results = cross_validate(ridge, X, y, cv=10, scoring=\"neg_mean_squared_error\", return_train_score=True, return_estimator=True) C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.672e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.67257e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.75536e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.67367e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=3.5546e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.75974e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.82401e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.96672e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.68318e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T C:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=2.68514e-17): result may not be accurate. return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T  El código anterior genera varios warnings debido a que las features incluidas tiene valores extremadamentes grandes o extremadamente pequeños, lo que causa problemas numéricos cuando entrenamos el modelo predictivo.\nVamos a explorar las puntuaciones de entrenamiento y prueba de este modelo.\ntrain_error = -cv_results[\"train_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: \\n\" f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: 4373.180 +/- 153.942  test_error = -cv_results[\"test_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: \\n\" f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: 7303.589 +/- 4950.732  Vemos que las puntuaciones de entrenamiento y prueba son muchas más cercanas, lo que indica que nuestro modelo tiene menos overfitting. Podemos comparar los valores de los pesos de ridge con una regresión lineal no regularizada.\ncoefs = [est[-1].coef_ for est in cv_results[\"estimator\"]] weights_ridge = pd.DataFrame(coefs, columns=feature_names) weights_ridge.plot.box(color=color, vert=False, figsize=(6, 16)) _ = plt.title(\"Coeficientes de ridge\") Comparando la magnitud de los pesos de este gráfico con los del anterior, vemos que un modelo ridge fuerza que todos los pesos tengan una magnitud similar, mientras que la magnitud general de los pesos tiende a cero con respecto a los del modelo de regresión lineal.\nSin embargo, en este ejemplo, omitimos dos aspectos importantes: (i) la necesidad de escalar los datos y (ii) la necesidad de buscar el mejor parámetro de regularización.\nEscalado de features y regularización Por un lado, los pesos definen el vínculo entre los valores de las features y el objetivo predicho. Por otro lado, la regularización añade restricciones en los pesos del modelo a través del parámetro alpha. Por lo tanto, el efecto que el escalado de features tiene en los pesos finales también interactúa con la regularización.\nConsideremos el caso donde las features viven en la misma escala/unidades: si el modelo determina que dos features son igualmente importantes, se verán afectadas de forma similar por la fuerza de la regularización.\nAhora, consideremos el escenario donde las features tienen escalas completamente diferentes (por ejemplo la edad en años y los ingresos anuales en dólares). Si dos features son tan importantes, nuestro modelo aumentará los pesos de las features con menor escala y reducirá los pesos de las features con mayor escala.\nRecordemos que la regularización fuerza la cercanía de los pesos. Por tanto, intuitivamente, si queremos usar regularización, tratar con datos reescalados podría facilitar el encontrar un parámetro de regularización óptimo y, por tanto, un modelo más adecuado.\nComo nota adicional, algunos solucionadores basados en cálculo de gradiente esperan tales datos reescalados. Los datos sin escalar serán perjudiciales cuando calculemos los pesos óptimos. Por lo tanto, cuando trabajamos con un modelo lineal y datos numéricos es una buena práctica escalar los datos.\nAsí que vamos a añadir un StandardScaler en el pipeline de machine learning. Este scaler será ubicado justo antes del regresor.\nfrom sklearn.preprocessing import StandardScaler ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), Ridge(alpha=0.5)) cv_results = cross_validate(ridge, X, y, cv=10, scoring=\"neg_mean_squared_error\", return_train_score=True, return_estimator=True) train_error = -cv_results[\"train_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: \\n\" f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: 4347.036 +/- 156.666  test_error = -cv_results[\"test_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: \\n\" f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: 5508.472 +/- 1816.642  Observamos que el escalado de los datos tiene un impacto positivo en la puntuación de prueba y dicha puntuación es más cercana a la puntuación de entrenamiento. Significa que nuestro modelo tiene menos overfitting y que se encuentra más cerca del punto óptimo de generalización.\nEchemos un vistazo adicional a los diferentes pesos.\ncoefs = [est[-1].coef_ for est in cv_results[\"estimator\"]] weights_ridge = pd.DataFrame(coefs, columns=feature_names) weights_ridge.plot.box(color=color, vert=False, figsize=(6, 16)) _ = plt.title(\"Coeficientes de ridge con escalado de datos\") Comparando con los gráficos anteriores, vemos que ahora todas las magnitudes de los pesos están más cercanas y todas las features contribuyen de forma más igualitaria.\nEn el ejemplo anterior, establecimos alpha=0.5. Vamos a comprobar el impacto de alpha si incrementamos su valor.\nridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), Ridge(alpha=1_000_000)) cv_results = cross_validate(ridge, X, y, cv=10, scoring=\"neg_mean_squared_error\", return_train_score=True, return_estimator=True) train_error = -cv_results[\"train_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: \\n\" f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: 12020.650 +/- 399.508  test_error = -cv_results[\"test_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: \\n\" f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: 12543.890 +/- 3846.344  coefs = [est[-1].coef_ for est in cv_results[\"estimator\"]] weights_ridge = pd.DataFrame(coefs, columns=feature_names) weights_ridge.plot.box(color=color, vert=False, figsize=(6, 16)) _ = plt.title(\"Coeficientes de ridge con escalado de datos y alpha alto\") Mirando específicamente los valores de los pesos, observamos que incrementar el valor de alpha hace disminuir los valores de los pesos. Un valor negativo de alpha en realidad mejoraría los pesos grandes y promovería el overfitting.\nAquí nos hemos enfocado en las features numéricas. Para las features categóricas es generalmente común omitir el escalado cuando las features están codificadas con OneHotEncoder, dado que los valores de las features ya se encuentran en una escala similar. Sin embargo, esta opción puede cuestionarse, dado que el escalado también interactúa con la regularización. Por ejemplo, escalar las features categóricas que están desbalanceadas (por ejemplo, más ocurrencias de una categoría específica) igualaría el impacto de la regularización en cada categoría. Sin embargo, escalar tales features en presencia de categorías raras podría ser problemático (es decir, división por una desviación típica muy pequeña) y podría introducir, además, problemas numéricos.\nEn los análisis anteriores, no estudiamos si el parámetro alpha tenía un efecto en el rendimiento. Seleccionamos el parámetro de antemano y lo fijamos para el análisis. Vamos a comprobar el impacto de este parámetro en la regularización y cómo podemos ajustarlo.\nAjuste del parámetro de regularización Como decíamos, el parámetro de regularización necesita ser ajustado en cada dataset. El parámetro por defecto no conducirá al modelo óptimo. Por lo tanto, necesitamos ajustar el parámetro alpha.\nEl ajuste de hiperparámetros del modelo debe hacerse con cuidado. Queremos encontrar un parámetro óptimo que maximice algunes métricas. Por tanto, requiere tanto del conjunto de entrenamiento como del conjunto de prueba.\nSin embargo, este conjunto de prueba debería ser diferente del conjunto de prueba externo que usemos para evaluar nuestro modelo: si usamos el mismo, estamos usando un alpha que ha sido optimizado para este conjunto de prueba y rompería la regla de que sea externo a la muestra.\nPor tanto, debemos incluir la búsqueda del hiperparámetro alpha en la validación cruzada. Como vimo anteriormente, podemos usar un grid-search. Sin embargo, algunos predictores de scikit-learn cuentan con una búsqueda de hiperparámetros integrada más eficiente que usar grid-search. El nombre de esos predictores termina por CV. En el caso de Ridge, scikit-learn proporciona el regresor RidgeCV.\nAsí que podemos usar este predictor en el último paso del pipeline. Incluir en el pipeline una validaciónc cruzada permite realizar una validación cruzada anidada: la validación cruzada interna buscará el mejor alpha, mientras que la validación cruzada externa proporcionará una estimación de la puntuación de prueba.\nfrom sklearn.linear_model import RidgeCV alphas = np.logspace(-2, 0, num=20) ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), RidgeCV(alphas=alphas, store_cv_values=True)) from sklearn.model_selection import ShuffleSplit cv = ShuffleSplit(n_splits=5, random_state=1) cv_results = cross_validate(ridge, X, y, cv=cv, scoring=\"neg_mean_squared_error\", return_train_score=True, return_estimator=True) train_error = -cv_results[\"train_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: \\n\" f\"{train_error.mean():.3f} +/- {train_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de entrenamiento: 4306.562 +/- 25.918  test_error = -cv_results[\"test_score\"] print(f\"Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: \\n\" f\"{test_error.mean():.3f} +/- {test_error.std():.3f}\") Error cuadrático medio del modelo de regresión lineal en el conjunto de prueba: 4348.657 +/- 252.921  Optimizando alpha, vemos que se acercan las puntuaciones de entrenamiento y prueba. Lo que indica que nuestro modelo no tiene overfitting. Cuando entrenamos el regresor ridge, también le pedimos que almacene el error encontrado durante la validación cruzada (con el parámetro store_cv_values=True). Podemos dibujar el error cuadrático medio para las diferentes alphas que hemos intentado.\nmse_alphas = [est[-1].cv_values_.mean(axis=0) for est in cv_results[\"estimator\"]] cv_alphas = pd.DataFrame(mse_alphas, columns=alphas) cv_alphas  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  0.010000 0.012743 0.016238 0.020691 0.026367 0.033598 0.042813 0.054556 0.069519 0.088587 0.112884 0.143845 0.183298 0.233572 0.297635 0.379269 0.483293 0.615848 0.784760 1.000000     0 7587.897141 7059.531216 6579.796667 6161.839629 5813.048345 5535.350138 5326.646369 5182.950517 5100.749049 5079.212663 5122.029454 5238.704149 5445.118890 5763.012259 6217.925433 6835.274126 7634.692137 8623.620241 9791.918443 11109.476019   1 7079.341771 6696.622046 6329.022308 5987.727181 5681.306000 5415.171818 5191.880969 5012.209177 4876.780585 4787.974274 4751.851652 4779.853646 4889.937328 5106.656625 5459.549144 5979.280717 6691.530974 7609.581815 8727.609953 10017.092899   2 24857.406605 21448.284772 18293.478416 15481.009011 13067.031991 11071.457116 9480.437579 8254.431555 7338.963939 6675.026438 6207.024331 5887.541169 5679.263698 5554.913823 5496.051755 5491.367683 5534.775517 5623.398006 5755.456140 5928.154410   3 7504.216958 7125.074257 6754.172985 6400.999897 6072.937965 5774.941819 5509.955509 5280.027720 5087.960263 4939.271829 4844.228092 4819.618117 4889.808854 5086.420585 5445.877349 6004.320850 6790.113763 7815.311733 9068.542918 10511.939341   4 6999.938808 6598.898743 6215.363499 5861.328579 5546.025773 5275.377052 5052.349007 4878.140850 4753.964890 4683.117084 4673.049960 4737.162496 4895.935124 5176.863333 5612.500529 6236.037942 7074.449338 8140.303978 9424.471281 10892.352852     cv_alphas.mean(axis=0).plot(marker=\"+\") plt.ylabel(\"Error cuadrático medio\\n(menos es mejor)\") plt.xlabel(\"alpha\") _ = plt.title(\"Error obtenido por validación cruzada\") Como podemos ver, la regularización es como la sal en la cocina: debemos equilibrar su cantidad para obtener el mejor rendimiento de generalización. Podemos comprobar si el mejor alpha encontrado es estable a través de las particiones de validación cruzada.\nbest_alphas = [est[-1].alpha_ for est in cv_results[\"estimator\"]] best_alphas [0.08858667904100823, 0.11288378916846889, 0.37926901907322497, 0.14384498882876628, 0.11288378916846889]  La fuerza de regularización óptima no es necesariamente la misma en todas las iteraciones de validación cruzada. Pero dado que esperamos que cada remuestreo de validación cruzada provenga de la misma distribución de datos, es una práctica común usar el valor promedio del mejor alpha encontrado en las diferentes particiones de validación cruzada como nuestra estimación final para el alpha tuneado.\nprint(f\"El alpha óptimo medio que conduce al mejor rendimiento de generalización es:\\n\" f\"{np.mean(best_alphas):.2f} +/- {np.std(best_alphas):.2f}\") El alpha óptimo medio que conduce al mejor rendimiento de generalización es: 0.17 +/- 0.11  Modelos lineales para clasificación Vimos anteriormente el dataset de pingüinos. Sin embargo, en esta ocasión intentaremos predecir las especies de pingüinos usando la información del pico (en concreto, la cresta superior del pico). Vamos a simplificar el problema de clasificación seleccionando solo 2 de las especies para solucionar un problema de clasificación binaria.\npenguins = pd.read_csv(\"penguins_classification.csv\") penguins = penguins.set_index(\"Species\").loc[ [\"Adelie\", \"Chinstrap\"]].reset_index() culmen_columns = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"] target_column = \"Species\" Podemos empezar rápidamente visualizando la distribución de features por clase:\nfor feature_name in culmen_columns: plt.figure() penguins.groupby(\"Species\")[feature_name].plot.hist(alpha=0.5, legend=True) plt.xlabel(feature_name) Podemos observar que tenemos un problema bastante simple. Cuando la longitud del pico (Culmen Length) aumenta, la probabilidad de que el pingüino sea un Chinstrap es cercano a 1. Sin embargo, la anchura del pico (Culmen Depth) no es útil para predecir las especies de pingüinos.\nPara el entrenamiento del modelo, separaremos el objetivo de los datos y crearemos un conjunto de entrenamiento y otro de prueba.\nfrom sklearn.model_selection import train_test_split X, y = train_test_split(penguins, random_state=0) X_train = X[culmen_columns] X_test = y[culmen_columns] y_train = X[target_column] y_test = y[target_column] Cuando el objetivo es un resultado binario, podemos usar la función logística para modelar la probabilidad. Este modelo es conocido como regresión logística.\nScikit-learn proporciona la clase LogisticRegression que implementa este algoritmo.\nfrom sklearn import set_config set_config(display=\"diagram\") from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression logistic_regression = make_pipeline( StandardScaler(), LogisticRegression(penalty=\"none\") ) logistic_regression.fit(X_train, y_train) accuracy = logistic_regression.score(X_test, y_test) print(f\"Precisión en el conjunto de prueba: {accuracy:.3f}\") Precisión en el conjunto de prueba: 1.000  Dado que estamos manejando un problema de clasificación que contiene solo 2 features, es posible observar el limite de la función de decisión. El límite es la regla usada por nuestro modelo predictivo para afectar una etiqueta de clase dados los valores de features de la instancia.\nAquí usaremos la clase DecisionBoundaryDisplay. Proporcionamos esta clase para permitir dibujar los límites de la función de decisión en un espacio de 2 dimensiones.\nfrom plotting import DecisionBoundaryDisplay DecisionBoundaryDisplay.from_estimator( logistic_regression, X_test, response_method=\"predict\", cmap=\"RdBu_r\", alpha=0.5 ) sns.scatterplot( data=y, x=culmen_columns[0], y=culmen_columns[1], hue=target_column, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Límite decisión del entrenamiento\\nLogisticRegression\") Vemos que nuestra función de decisión está representada por una línea separando las dos clases. También debemos tener en cuenta que no impusimos ninguna regularización estableciendo el parámetro penalty a none. Dado que la línea es oblícua significa que hemos usado una combinación de ambas features:\ncoefs = logistic_regression[-1].coef_[0] # los coeficientes son una matriz 2d weights = pd.Series(coefs, index=culmen_columns) weights.plot.barh() _ = plt.title(\"Pesos de la regresión logística\") Más allá de la separación lineal en la clasificación Como vimos anteriormente, el modelo de clasificación lineal espera que los datos sean linealmente separables. Cuando esta asunción no se cumple, el modelo no es suficientemente expresivo para ajustarse a los datos. Por tanto, necesitamos aplicar los mismos trucos que en la regresión: aumento de features (potencialmente usando conocimiento experto) o usar un método basado en kernel.\nProporcionaremos ejemplos donde usaremos un kernel SVM para ejecutar clasificación en algún dataset de juguete donde es imposible encontrar una separación lineal perfecta.\nGeneraremos un primer dataset donde los datos están representados como semicírculos entrelazados. Este dataset se genera usando la función sklearn.datasets.make_moons.\nfrom sklearn.datasets import make_moons feature_names = [\"Feature #0\", \"Feature #1\"] target_name = \"class\" X, y = make_moons(n_samples=100, noise=0.13, random_state=42) moons = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=1), columns=feature_names + [target_name]) X_moons, y_moons = moons[feature_names], moons[target_name] Dado que el dataset contiene únicamente dos features, podemos hacer un scatterplot para echarle un vistazo.\nsns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1], hue=y_moons, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Ilustración del datasets moons\") De las intuiciones que obtuvimos de estudiar modelos lineales, debería ser obvio que un clasificador lineal no será capaz de encontrar un función de decisión perfecta para separar las dos clases. Intentemos ver cuál es el límite de decisión de tal clasificador lineal. Crearemos un modelo predictivo estandarizando el dataset seguido por un clasificador SVM lineal.\nfrom sklearn.svm import SVC linear_model = make_pipeline(StandardScaler(), SVC(kernel=\"linear\")) linear_model.fit(X_moons, y_moons) #sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 {color: black;background-color: white;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 pre{padding: 0;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-toggleable {background-color: white;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-estimator:hover {background-color: #d4ebff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-item {z-index: 1;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-parallel-item:only-child::after {width: 0;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-08e61b51-4487-43f9-91c4-cc6b09ac7929 div.sk-text-repr-fallback {display: none;}Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(kernel='linear'))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('standardscaler', StandardScaler()),('svc', SVC(kernel='linear'))])StandardScalerStandardScaler()SVCSVC(kernel='linear') Tengamos en cuenta que entrenamos y verificamos el límite de decisión del clasificador en el mismo dataset, sin dividir el dataset en un conjunto de entrenamiento y uno de prueba. Aunque esto es una mala práctica, lo usamos aquí por simplicidad para representar el comportamiento del modelo. Siempre debemos usar validación cruzada para evaluar el rendimiento de generalización de un modelo de machine learning.\nVamos a comprobar el límite de decisión del modelo lineal en este dataset.\nfrom plotting import DecisionBoundaryDisplay DecisionBoundaryDisplay.from_estimator( linear_model, X_moons, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1], hue=y_moons, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Límite decisión de un modelo lineal\") Como era de esperar, un límite de decisión lineal no es lo suficientemente flexible para dividir las dos clases.\nPara llevar este ejemplo al límite, crearemos otro dataset donde las muestras de una clase estén rodeadas por las de la otra clase.\nfrom sklearn.datasets import make_gaussian_quantiles feature_names = [\"Feature #0\", \"Feature #1\"] target_name = \"class\" X, y = make_gaussian_quantiles( n_samples=100, n_features=2, n_classes=2, random_state=42) gauss = pd.DataFrame(np.concatenate([X, y[:, np.newaxis]], axis=1), columns=feature_names + [target_name]) X_gauss, y_gauss = gauss[feature_names], gauss[target_name] ax = sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1], hue=y_gauss, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Ilustración del datasets de cuantiles gaussianos\") Aquí es incluso más obvio que no es apta una función de decisión lineal. Podemos verificar qué función de decisión encontrará un SVM lineal.\nlinear_model.fit(X_gauss, y_gauss) DecisionBoundaryDisplay.from_estimator( linear_model, X_gauss, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1], hue=y_moons, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Límite decisión de un modelo lineal\") Como era de esperar, no se puede usar una separación lineal para separar las clases apropiadamente: el modelo tendrá underfitting y generará errores incluso en el conjunto de entrenamiento.\nVimos anteriormente que podíamos usar varios trucos para hacer un modelo lineal más flexible aumentando las features o usando un kernel. Aquí usaremos la última solución usando un kernel de función de base radial (RBF) junto con un clasificador SVM.\nRepetiremos los dos experimentos previos y comprobaremos la función de decisión obtenida.\nkernel_model = make_pipeline(StandardScaler(), SVC(kernel=\"rbf\", gamma=5)) kernel_model.fit(X_moons, y_moons) DecisionBoundaryDisplay.from_estimator( kernel_model, X_moons, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=moons, x=feature_names[0], y=feature_names[1], hue=y_moons, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Límite decisión con un modelo usando kernel RBF\") Vemos que el límite de decisión ya no es una línea recta. De hecho, se define un área alrededor de las muestras rojas y podemos imaginar que este clasificador debería ser capaz de generalizar a datos nunca vistos.\nVerifiquemos la función de decisión del segundo dataset.\nkernel_model.fit(X_gauss, y_gauss) DecisionBoundaryDisplay.from_estimator( kernel_model, X_gauss, response_method=\"predict\", cmap=\"RdBu\", alpha=0.5 ) sns.scatterplot(data=gauss, x=feature_names[0], y=feature_names[1], hue=y_gauss, palette=[\"tab:red\", \"tab:blue\"]) _ = plt.title(\"Límite decisión con un modelo usando kernel RBF\") Observamos algo similar al caso anterior. La función de decisión es más flexible y ya no produce underfitting. Por lo tanto, el truco del kernel o la expansión de features son trucos para hacer más expresivo un clasificador, exactamente igual a como vimos en la regresión.\nTengamos en mente que añadir flexibilidad a un modelo también puede conllevar el riesgo de incrementer el overfitting haciendo que la función de decisión sea sensible a puntos de datos individuales (posiblemente ruido) del conjunto de entrenamiento. Aquí podemos observar que las funciones de decisión permanecen lo suficientemente suaves para preservar una buena generalización. Por curiosidad, podemos repetir el experimento anterior con gamma=100 y ver las funciones de decisión.\nResumen  Las predicciones de un modelo lineal dependen de la suma ponderada de los valores de las variables de entrada añadido a un parámetros de intersección (intercept). Entrenar un modelo lineal consiste en ajustar tanto el peso de los coeficientes como el valor de la constante para minimizar los errores de predicción en el conjunto de entrenamiento. Para entrenar con éxito modelos lineales a menudo se requiere escalar las features de entrada aproximadamente al mismo rango dinámico. La regualización se puede usar para reducir el overfitting: los coeficientes de los pesos se restringen para que se mantengan pequeños durante el entrenamiento. Los hiperparámetros de regularización necesitan ser ajustados por validación cruzada para cada nuevo problema de machine learning y dataset. Los modelos lineales se pueden usar en problemas donde la variable objetivo no se relaciona linealmente con las variables de entrada, pero esto requiere de ingeniería de features para transformar los datos a fin de evitar el underfitting.  Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Ejemplo de regresión lineal Comparación entre una regresión lineal y un regresor ridge  ","description":"","tags":["modelos lineales","regularización","PolynomialFeatures","Ridge","alpha","regresión logística","regresión lineal"],"title":"Modelos Lineales","uri":"/posts/modelos-lineales/"},{"categories":["tutoriales"],"content":"En posts anteriores vimos cómo crear, entrenar, predecir e incluso evaluar un modelo predictivo. Sin embargo, no cambiamos ninguno de los parámetros del modelo que tenemos a nuestra disposición cuando creamos una instancia. Por ejemplo, para k-nearest neighbors, inicialmente usamos los parámetros por defecto: n_neighbors=5 antes de probar otros parámetros del modelo.\nEstos parámetros se denominan hiperparámetros: son parámetros usados para controlar el proceso de aprendizaje, por ejemplo el parámetro k de k-nearest neighbors. Los hiperparámetros son especificados por el usuario, a menudo ajustados manualmente (o por una búsqueda automática exhaustiva) y no pueden ser estimados a partir de los datos. No deben confundirse con los otros parámetros que son inferidos durante el proceso de entrenamiento. Estos parámetros definen el modelo en sí mismo, por ejemplo coef_ para los modelos lineales.\nEn este post mostraremos en primer lugar que los hiperparámetros tienen un impacto en el rendimiento del modelo y que los valores por defecto no son necesariamente la mejor opción. Posteriormente, mostraremos cómo definir hiperparámetros en un modelo de scikit-learn. Por último, mostraremos estrategias que nos permitirán seleccionar una combinación de hiperparámetros que maximicen el rendimiento del modelo.\nEn concreto repasaremos los siguientes aspectos:\n cómo usar get_params y set_params para obtener los parámetros de un modelo y establecerlos, respectivamente; cómo optimizar los hiperparámetros de un modelo predictivo a través de grid-search; cómo la búsqueda de más de dos hiperparámetros es demasiado costosa; cómo grid-search no encuentra necesariamente una solución óptima; cómo la búsqueda aleatoria ofrece una buena alternativa a grid-search cuando el número de parámetros a ajustar es más de dos. También evita la regularidad impuesta por grid-search que puede resultar problemática en ocasiones; cómo evaluar el rendimiento predictivo de un modelo con hiperparámetros ajustados usando el procedimiento de validación cruzada anidada.  Establecer y obtener hiperparámetros en scikit-learn El proceso de aprendizaje de un modelo predictivo es conducido por un conjunto de parámetros internos y un conjunto de datos de entrenamiento. Estos parámetros internos se denominan hiperparámetros y son específicos de cada familia de modelos. Además, un conjunto específico de hiperparámetros es óptimo para un dataset específico y, por lo tanto, necesitan optimizarse.\nVamos a mostrar como podemos obtener y establecer el valor de un hiperparámetro en un estimador de scikit-learn. Recordemos que los hiperparámetros se refieren a los parámetros que controlarán el proceso de aprendizaje. No debemos confundirlos con los parámetros entrenados, resultado del entrenamiento. Estos parámetros entrenados se reconocen en scikit-learn porque tienen el sufijo _, por ejemplo, model_coef_.\nUtilizaremos el dataset del Censo US de 1944, del que únicamente usaremos las variables numéricas.\nimport pandas as pd adult_census = pd.read_csv(\"adult_census.csv\") target_name = \"class\" y = adult_census[target_name] data = adult_census.drop(columns=[target_name]) numerical_columns = [ \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"] X = data[numerical_columns] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age capital-gain capital-loss hours-per-week     0 25 0 0 40   1 38 0 0 50   2 28 0 0 40   3 44 7688 0 40   4 18 0 0 30     Vamos a crear un modelo predictivo simple compuesto por un scaler seguido por un clasificador de regresión logística.\nMuchos modelos, incluidos los lineales, trabajan mejor si todas las características tienen un escalado similar. Para este propósito, usaremos un StandardScaler, que transforma los datos escalando las features.\nfrom sklearn import set_config from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression set_config(display=\"diagram\") model = Pipeline(steps=[ (\"preprocessor\", StandardScaler()), (\"classifier\", LogisticRegression()) ]) Podemos evaluar el rendimiento de generalización del modelo a través de validación cruzada.\nfrom sklearn.model_selection import cross_validate cv_results = cross_validate(model, X, y) scores = cv_results[\"test_score\"] print(f\"Puntuación de precisión a través de validación cruzada:\\n\" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación de precisión a través de validación cruzada: 0.800 +/- 0.003  Hemos creado un modelo con el valor por defecto de C que es igual a 1. Si quisiéramos usar un parámetro C distinto, podríamos haberlo hecho cuando creamos el objeto LogisticRegression con algo como LogisticRegression(C=1e-3). También podemos cambiar el parámetro de un modelo después de que haya sido creado con el método set_params, disponible para todos los estimadores de scikit-learn. Por ejemplo, podemos establecer C=1e-3, entrenar y evaluar el modelo:\nmodel.set_params(classifier__C=1e-3) cv_results = cross_validate(model, X, y) print(f\"Puntuación de precisión a través de validación cruzada:\\n\" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación de precisión a través de validación cruzada: 0.800 +/- 0.003  Cuando el modelo está en un Pipeline, los nombres de los parámetros tiene la forma \u003cnombre_modelo\u003e__\u003cnombre_parámetro\u003e. En nuestro caso, classifier proviene de la definición del Pipeline y C es el nombre del parámetro de LogisticRegression.\nGeneralmente, podemos usar el método get_params en los modelos de scikit-learn para listar todos los parámetros con sus respectivos valores.\nmodel.get_params() {'memory': None, 'steps': [('preprocessor', StandardScaler()), ('classifier', LogisticRegression(C=0.001))], 'verbose': False, 'preprocessor': StandardScaler(), 'classifier': LogisticRegression(C=0.001), 'preprocessor__copy': True, 'preprocessor__with_mean': True, 'preprocessor__with_std': True, 'classifier__C': 0.001, 'classifier__class_weight': None, 'classifier__dual': False, 'classifier__fit_intercept': True, 'classifier__intercept_scaling': 1, 'classifier__l1_ratio': None, 'classifier__max_iter': 100, 'classifier__multi_class': 'auto', 'classifier__n_jobs': None, 'classifier__penalty': 'l2', 'classifier__random_state': None, 'classifier__solver': 'lbfgs', 'classifier__tol': 0.0001, 'classifier__verbose': 0, 'classifier__warm_start': False}  get_params devuelve un diccionario cuyas claves son los nombres de los parámetros y sus valores los valores de dichos parámetros. Si queremos obtener el valor de un único parámetro, por ejemplo, classifier__C usamos lo siguiente:\nmodel.get_params()[\"classifier__C\"] 0.001  Podemos variar sistemáticamente el valor de C para ver si existe un valor óptimo.\nfor C in [1e-3, 1e-2, 1e-1, 1, 10]: model.set_params(classifier__C=C) cv_results = cross_validate(model, X, y) scores = cv_results[\"test_score\"] print(f\"Puntuación de precisión de validación cruzada con C={C}:\\n\" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") Puntuación de precisión de validación cruzada con C=0.001: 0.787 +/- 0.002 Puntuación de precisión de validación cruzada con C=0.01: 0.799 +/- 0.003 Puntuación de precisión de validación cruzada con C=0.1: 0.800 +/- 0.003 Puntuación de precisión de validación cruzada con C=1: 0.800 +/- 0.003 Puntuación de precisión de validación cruzada con C=10: 0.800 +/- 0.003  Podemos ver que mientras C sea lo suficientemente alto, el modelo parece rendir bien.\nLo que hemos hecho aquí es muy manual: implica recorrer los valores de C y seleccionar manualmente el mejor. Veremos cómo realizar esta tarea de forma automática.\nCuando evaluamos una familia de modelos en datos de prueba y seleccionamos el que mejor se ejecuta, no podemos confiar en la correpondiente precisión de la estimación y necesitamos aplicar el modelo en nuevos datos. De hecho, los datos de prueba se han usado para seleccionar el modelo y, por lo tanto, ya no es independiente de este modelo.\nAjuste de hiperparámetros por grid-search Vamos a mostrar cómo optimizar hiperparámetros usando el enfoque de grid-search.\nSeguimos con nuestro dataset del censo.\ntarget_name = \"class\" y = adult_census[target_name] y.head() 0 \u003c=50K 1 \u003c=50K 2 \u003e50K 3 \u003e50K 4 \u003c=50K Name: class, dtype: object  Vamos a eliminar de nuestro datos el objetivo y la columna \"education-num\", dado que es información duplicada de la columna \"education\".\nX = adult_census.drop(columns=[target_name, \"education-num\"]) X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  age workclass fnlwgt education marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country     0 25 Private 226802 11th Never-married Machine-op-inspct Own-child Black Male 0 0 40 United-States   1 38 Private 89814 HS-grad Married-civ-spouse Farming-fishing Husband White Male 0 0 50 United-States   2 28 Local-gov 336951 Assoc-acdm Married-civ-spouse Protective-serv Husband White Male 0 0 40 United-States   3 44 Private 160323 Some-college Married-civ-spouse Machine-op-inspct Husband Black Male 7688 0 40 United-States   4 18 ? 103497 Some-college Never-married ? Own-child White Female 0 0 30 United-States     Dividimos el dataset en entrenamiento y prueba.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) Vamos a definir un pipeline y manejaremos tanto las variables numéricas como las categóricas.\nfrom sklearn.compose import make_column_selector as selector categorical_columns_selector = selector(dtype_include=object) categorical_columns = categorical_columns_selector(X) En este caso, estamos usando un modelo basado en árbol como un clasificador (es decir, HistGradientBoostingClassifier). Esto significa que:\n las variables numéricas no necesitan escalado; las variables categóricas se pueden manejar con un OrdinalEncoder incluso si el orden codificado no tiene sentido; En los modelos basados en árbol, OrdinalEncoder evita tener representaciones de alta dimensionalidad.  Vamos a construir nuestro OrdinalEncoder pasándole las categorías conocidas.\nfrom sklearn.preprocessing import OrdinalEncoder categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1) Usaremos un ColumnTransformer para seleccionar las columnas categóricas y aplicarles el OrdinalEncoder.\nfrom sklearn.compose import ColumnTransformer preprocessor = ColumnTransformer([ (\"cat_preprocessor\", categorical_preprocessor, categorical_columns)], remainder=\"passthrough\", sparse_threshold=0) Por último, usaremos un clasificador de árbol (por ejemplo, histogram gradient-boosting) para predecir si una persona gana más de 50 k$ al año.\nfrom sklearn.ensemble import HistGradientBoostingClassifier model = Pipeline([ (\"preprocessor\", preprocessor), (\"classifier\", HistGradientBoostingClassifier( random_state=42, max_leaf_nodes=4 )) ]) model #sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 {color: black;background-color: white;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 pre{padding: 0;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-toggleable {background-color: white;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-estimator:hover {background-color: #d4ebff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-item {z-index: 1;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-parallel-item:only-child::after {width: 0;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-bfa4b090-bb0e-4721-afa1-b8867206ea58 div.sk-text-repr-fallback {display: none;}Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1), ['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Pasemos ahora al ajuste con grid-search. Anteriormente usamos un bucle for para cada hiperparámetro con el fin de encontrar la mejor combinación a partir de un conjunto de valores. La clase GridSearchCV de scikit-learn implementa una lógica muy similar con mucho menos código repetitivo. Vamos a ver cómo usar el estimador GridSearchCV para realizar esta búsqueda. Dado que grid-search puede ser costoso, únicamente exploraremos la combinación tasa de aprendizaje y máximo número de nodos.\n%%time from sklearn.model_selection import GridSearchCV param_grid = { \"classifier__learning_rate\": (0.01, 0.1, 1, 10), \"classifier__max_leaf_nodes\": (3, 10, 30)} model_grid_search = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=2) model_grid_search.fit(X_train, y_train) CPU times: total: 14.3 s Wall time: 5.51 s  #sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 {color: black;background-color: white;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 pre{padding: 0;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-toggleable {background-color: white;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-item {z-index: 1;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-parallel-item:only-child::after {width: 0;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-9b70fb2d-44a4-4963-afbb-8a2c6bac99a3 div.sk-text-repr-fallback {display: none;}GridSearchCV(cv=2,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))]),n_jobs=-1,param_grid={'classifier__learning_rate': (0.01, 0.1, 1, 10),'classifier__max_leaf_nodes': (3, 10, 30)})Please rerun this cell to show the HTML repr or trust the notebook.GridSearchCVGridSearchCV(cv=2,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))]),n_jobs=-1,param_grid={'classifier__learning_rate': (0.01, 0.1, 1, 10),'classifier__max_leaf_nodes': (3, 10, 30)})preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Finalmente, comprobamos la precisión de nuestro modelo usando el conjunto de prueba.\naccuracy = model_grid_search.score(X_test, y_test) print(f\"La puntuación de precisión de prueba del pipeline grid-search es:\" f\"{accuracy:.2f}\") La puntuación de precisión de prueba del pipeline grid-search es:0.88  El estimador GridSearchCV toma una parámetro param_grid que define todos los hiperparámetros y sus valores asociados. Grid-search se encargará de crear todas las posibles combinaciones y probarlas.\nEl número de combinaciones será igual al producto del número de valores a explorar para cada parámetros (es decir, en nuestro ejemplo 4 x 3 combinaciones). Por tanto, añadir nuevos parámetros con sus valores asociados a ser explorados se vuelve rápidamente computacionalmente costoso.\nUna vez que grid-search es entrenado, se puede usar como cualquier otro predictor llamando a sus métodos predict y predict_proba. Internamente, usará el modelo con los mejores parámetros encontrados durante el fit.\nVamos a obtener las predicciones para los primeros 5 ejemplos usando el estimador con los mejores parámetros.\nmodel_grid_search.predict(X_test.iloc[0:5]) array([' \u003c=50K', ' \u003c=50K', ' \u003e50K', ' \u003c=50K', ' \u003e50K'], dtype=object)  Podemos conocer cuáles son esos parámetros mirando el atributo best_params_.\nmodel_grid_search.best_params_ {'classifier__learning_rate': 0.1, 'classifier__max_leaf_nodes': 30}  La precisión y los mejores parámetros del pipeline de grid-search son similares a los que encontramos anteriormente, donde localizamos los mejores parámetros “a mano” usando un doble bucle for. Además, podemos inspeccionar todos los resultados, los cuales se almacenan en el atributo cv_results_ de grid-search. Filtraremos algunas columnas específicas de estos resultados.\ncv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values( \"mean_test_score\", ascending=False) cv_results.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time param_classifier__learning_rate param_classifier__max_leaf_nodes params split0_test_score split1_test_score mean_test_score std_test_score rank_test_score     5 1.032387 4.768372e-07 0.214684 0.006005 0.1 30 {'classifier__learning_rate': 0.1, 'classifier... 0.867766 0.867649 0.867708 0.000058 1   4 0.762404 4.478788e-02 0.288248 0.020517 0.1 10 {'classifier__learning_rate': 0.1, 'classifier... 0.866729 0.866557 0.866643 0.000086 2   6 0.374071 4.829156e-02 0.169396 0.021768 1 3 {'classifier__learning_rate': 1, 'classifier__... 0.860559 0.861261 0.860910 0.000351 3   7 0.241707 8.007050e-03 0.133865 0.009759 1 10 {'classifier__learning_rate': 1, 'classifier__... 0.857993 0.861862 0.859927 0.001934 4   3 0.545218 5.529642e-02 0.250966 0.017766 0.1 3 {'classifier__learning_rate': 0.1, 'classifier... 0.852752 0.854272 0.853512 0.000760 5     Con solo dos parámetros podriamos visualizar el grid-search con un mapa de calor. Necesitamos transformar nuestro cv_results en un dataframe donde:\n las filas corresponderán a los valores de la tasa de aprendizaje; las columnas corresponderán al mnúmero máximo de hojas; el contenido del dataframe serán las puntuaciones de prueba medias.  pivoted_cv_results = cv_results.pivot_table( values=\"mean_test_score\", index=['param_classifier__learning_rate'], columns=[\"param_classifier__max_leaf_nodes\"]) pivoted_cv_results  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n param_classifier__max_leaf_nodes 3 10 30   param_classifier__learning_rate        0.01 0.797166 0.817832 0.845541   0.10 0.853512 0.866643 0.867708   1.00 0.860910 0.859927 0.851547   10.00 0.283476 0.618080 0.351642     Podemos usa una representación de mapa de calor para mostrar visualmente el dataframe anterior.\nimport seaborn as sns ax = sns.heatmap(pivoted_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.7, vmax=0.9) ax.invert_yaxis() Observando el mapa de calor podemos resaltar algunas cosas:\n Para valores muy altos de learning_rate, el rendimiento de generalización del modelo se degrada y ajustar el valor de max_leaf_nodes no arregla el problema; fuera de esta región problemática, observamos que la opción óptima de max_leaf_nodes depende del valor de learning_rate; en particular, observamos una “diagonal” de buenos modelos con una precisión cercana al máximo de 0.87: cuando el valor de max_leaf_nodes se incrementa, debemos disminuir el valor de learning_rate acordemente para mantener una buena precisión.  Por ahora, tengamos en cuenta que, en general, no existe una única configuración óptima de parámetros: 4 modelos de las 12 configuraciones de parámetros alcanzan la máxima precisión (hasta pequeñas fluctuaciones aleatorias causadas por el muestreo del conjunto de entrenamiento).\nAjuste de hiperparámetros por randomized-search Hemos visto que el enfoque de grid-search tiene sus limitaciones. No escala cuando el número de parámetros a ajustar aumenta. Además, grid-search impone una regularidad durante la búsqueda que podría ser problemática. Vamos a presentar otro método para ajustar hiperparámetros denominado búsqueda aleatoria.\nPartimos del mismo dataset, el cual hemos dividido en entrenamiento y prueba, y hemos realizado el mismo pipeline de preprocesado.\nmodel #sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a {color: black;background-color: white;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a pre{padding: 0;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-toggleable {background-color: white;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-estimator:hover {background-color: #d4ebff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-item {z-index: 1;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-parallel-item:only-child::after {width: 0;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-1fa48bfd-9d1b-43f1-b949-c6f9c13aff1a div.sk-text-repr-fallback {display: none;}Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Con el estimador GridSearchCV, los parámetros necesitan ser explicitamente especificados. Ya mencionamos que explorar un gran número de valores para diferentes valores sería rápidamente intratable. En su lugar, podemos generar aleatoriamente parámetros candidatos. De hecho, este enfoque evita la regularidad de grid-search. Por tanto, agregar más evaluaciones puede aumentar la resolución en cada dirección. Este es el caso de la frecuente situación en la que la elección de algunos hiperparámetros no es muy importante, como ocurre con el hiperparámetro 2 del siguiente diagrama.\nDe hecho, el número de puntos de evaluación debe ser dividido entre los dos diferentes hiperparámetros. Con un grid-search, el peligro es que esta región de buenos hiperparámetros quede entre la línea del grid: esta región está alineada con el grid dado que el hiperparámetro 2 tiene una influencia débil. Por contra, la búsqueda estocástica muestreará el hiperparámetro 1 independientemente del hiperparámetro 2 y buscará la región óptima.\nLa clase RandomizedSearchCV permite esta búsqueda estocástica. Se usa de forma similar a GridSearchCV pero se necesitan especificar las distribuciones de muestreo en lugar de los valores de los parámetros. Por ejemplo, dibujaremos candidatos usando una distribución logarítmica uniforme porque los parámetros que nos interesan toman valores posivos con una escala logarítmica natural (.1 es tan cercano a 1 como éste lo es a 10).\nNormalmente, para optimizar 3 o más hiperparámetros, la búsqueda aleatoria es más beneficiosa que grid-search.\nOptimizaremos otros 3 parámetros además de los que ya optimizamos con grid-search:\n l2_regularization: corresponde con la fortaleza de la regularización; min_samples_leaf: corresponde con el número mínimo de muestras requerida en una hoja; max_bins: corresponde con el número máximo de contenedores para construir histogramas.  Podemos usar scipy.stats.loguniform para generar números flotantes. Para generar valores aleatorios para parámetros con valores enteros (por ejemplo, min_samples_leaf) podemos adaptarlo como sigue:\nfrom scipy.stats import loguniform class loguniform_int: \"\"\"versión para valores enteror de la distribución log-uniform\"\"\" def __init__(self, a, b): self._distribution = loguniform(a, b) def rvs(self, *args, **kwargs): \"\"\"Ejemplo de variable aleatoria\"\"\" return self._distribution.rvs(*args, **kwargs).astype(int) Ahora podemos definir la búsqueda aleatoria usando diferentes distribuciones. Ejecutar 10 iteraciones de 5-particiones de validación cruzada para parametrizaciones aleatorias de este modelo en este dataset puede llevar desde 10 segundos a varios minutos, dependiendo de la velocidad de la máquina y del número de procesadores disponibles.\n%%time from sklearn.model_selection import RandomizedSearchCV param_distributions = { 'classifier__l2_regularization': loguniform(1e-6, 1e3), 'classifier__learning_rate': loguniform(0.001, 10), 'classifier__max_leaf_nodes': loguniform_int(2, 256), 'classifier__min_samples_leaf': loguniform_int(1, 100), 'classifier__max_bins': loguniform_int(2, 255), } model_random_search = RandomizedSearchCV( model, param_distributions=param_distributions, n_iter=10, cv=5, verbose=1, n_jobs=-1 ) model_random_search.fit(X_train, y_train) Fitting 5 folds for each of 10 candidates, totalling 50 fits CPU times: total: 4.31 s Wall time: 7.23 s  #sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced {color: black;background-color: white;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced pre{padding: 0;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-toggleable {background-color: white;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-estimator:hover {background-color: #d4ebff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-item {z-index: 1;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-parallel-item:only-child::after {width: 0;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-5fc71a5d-1746-48f6-a3f4-0b91ba882ced div.sk-text-repr-fallback {display: none;}RandomizedSearchCV(cv=5,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',Hi...'classifier__learning_rate': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x0000023201C853F0\u003e,'classifier__max_bins': \u003c__main__.loguniform_int object at 0x0000023201C86110\u003e,'classifier__max_leaf_nodes': \u003c__main__.loguniform_int object at 0x0000023201C86020\u003e,'classifier__min_samples_leaf': \u003c__main__.loguniform_int object at 0x0000023201C844F0\u003e},verbose=1)Please rerun this cell to show the HTML repr or trust the notebook.RandomizedSearchCVRandomizedSearchCV(cv=5,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',Hi...'classifier__learning_rate': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x0000023201C853F0\u003e,'classifier__max_bins': \u003c__main__.loguniform_int object at 0x0000023201C86110\u003e,'classifier__max_leaf_nodes': \u003c__main__.loguniform_int object at 0x0000023201C86020\u003e,'classifier__min_samples_leaf': \u003c__main__.loguniform_int object at 0x0000023201C844F0\u003e},verbose=1)preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Después podemos calcular la puntuación de precisión en el conjunto de prueba.\naccuracy = model_random_search.score(X_test, y_test) print(f\"La puntuación de precisión de prueba del mejor modelo es:\" f\"{accuracy:.2f}\") La puntuación de precisión de prueba del mejor modelo es:0.87  model_random_search.best_params_ {'classifier__l2_regularization': 0.0006474800575651534, 'classifier__learning_rate': 0.9584980078111938, 'classifier__max_bins': 131, 'classifier__max_leaf_nodes': 23, 'classifier__min_samples_leaf': 98}  Como ya vimos, podemos inspeccionar los resultados usando el atributo cv_results.\ncv_results  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time param_classifier__learning_rate param_classifier__max_leaf_nodes params split0_test_score split1_test_score mean_test_score std_test_score rank_test_score     5 1.032387 4.768372e-07 0.214684 0.006005 0.1 30 {'classifier__learning_rate': 0.1, 'classifier... 0.867766 0.867649 0.867708 0.000058 1   4 0.762404 4.478788e-02 0.288248 0.020517 0.1 10 {'classifier__learning_rate': 0.1, 'classifier... 0.866729 0.866557 0.866643 0.000086 2   6 0.374071 4.829156e-02 0.169396 0.021768 1 3 {'classifier__learning_rate': 1, 'classifier__... 0.860559 0.861261 0.860910 0.000351 3   7 0.241707 8.007050e-03 0.133865 0.009759 1 10 {'classifier__learning_rate': 1, 'classifier__... 0.857993 0.861862 0.859927 0.001934 4   3 0.545218 5.529642e-02 0.250966 0.017766 0.1 3 {'classifier__learning_rate': 0.1, 'classifier... 0.852752 0.854272 0.853512 0.000760 5   8 0.309014 1.376224e-02 0.131613 0.006506 1 30 {'classifier__learning_rate': 1, 'classifier__... 0.849749 0.853344 0.851547 0.001798 6   2 1.302117 5.504763e-02 0.221190 0.011009 0.01 30 {'classifier__learning_rate': 0.01, 'classifie... 0.843252 0.847830 0.845541 0.002289 7   1 0.884009 4.879224e-02 0.331785 0.014512 0.01 10 {'classifier__learning_rate': 0.01, 'classifie... 0.818956 0.816708 0.817832 0.001124 8   0 0.568239 5.479753e-02 0.272483 0.028774 0.01 3 {'classifier__learning_rate': 0.01, 'classifie... 0.797882 0.796451 0.797166 0.000715 9   10 0.200672 2.001750e-02 0.090077 0.011010 10 10 {'classifier__learning_rate': 10, 'classifier_... 0.742356 0.493803 0.618080 0.124277 10   11 0.177403 1.376188e-02 0.084322 0.009258 10 30 {'classifier__learning_rate': 10, 'classifier_... 0.364545 0.338739 0.351642 0.012903 11   9 0.214183 5.004406e-03 0.123356 0.020768 10 3 {'classifier__learning_rate': 10, 'classifier_... 0.279701 0.287251 0.283476 0.003775 12     Tengamos en mente que este ajuste está limitado por el número de combinaciones diferentes de parámetros que se puntúan mediante búsqueda aleatoria. De hecho, puede haber otros conjuntos de parámetros que conduzcan a un similar o mejor rendimiento de generalización pero que no hayan sido probados en la búsqueda. En la práctica, la búsqueda aleatoria de hiperparámetros se ejecuta con un gran número de iteraciones. Para evitar el coste computacional y aun así realizar un análisis decente, cargamos los resultados obtenidos de una búsqueda similar con 200 iteraciones.\n%%time model_random_search = RandomizedSearchCV( model, param_distributions=param_distributions, n_iter=200, cv=5, verbose=1, n_jobs=-1 ) model_random_search.fit(X_train, y_train) Fitting 5 folds for each of 200 candidates, totalling 1000 fits CPU times: total: 39.7 s Wall time: 1min 28s  #sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 {color: black;background-color: white;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 pre{padding: 0;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-toggleable {background-color: white;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-item {z-index: 1;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-parallel-item:only-child::after {width: 0;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-14b38d11-fbf7-43ba-a230-21f8dfbb7cc5 div.sk-text-repr-fallback {display: none;}RandomizedSearchCV(cv=5,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',Hi...'classifier__learning_rate': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x0000023201C853F0\u003e,'classifier__max_bins': \u003c__main__.loguniform_int object at 0x0000023201C86110\u003e,'classifier__max_leaf_nodes': \u003c__main__.loguniform_int object at 0x0000023201C86020\u003e,'classifier__min_samples_leaf': \u003c__main__.loguniform_int object at 0x0000023201C844F0\u003e},verbose=1)Please rerun this cell to show the HTML repr or trust the notebook.RandomizedSearchCVRandomizedSearchCV(cv=5,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',Hi...'classifier__learning_rate': \u003cscipy.stats._distn_infrastructure.rv_frozen object at 0x0000023201C853F0\u003e,'classifier__max_bins': \u003c__main__.loguniform_int object at 0x0000023201C86110\u003e,'classifier__max_leaf_nodes': \u003c__main__.loguniform_int object at 0x0000023201C86020\u003e,'classifier__min_samples_leaf': \u003c__main__.loguniform_int object at 0x0000023201C844F0\u003e},verbose=1)preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) accuracy = model_random_search.score(X_test, y_test) print(f\"La puntuación de precisión de prueba del mejor modelo es: \" f\"{accuracy:.2f}\") La puntuación de precisión de prueba del mejor modelo es: 0.88  def shorten_param(param_name): if \"__\" in param_name: return param_name.rsplit(\"__\", 1)[1] return param_name cv_results = pd.DataFrame(model_random_search.cv_results_).sort_values( \"mean_test_score\", ascending=False) cv_results = cv_results.rename(shorten_param, axis=1) cv_results.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time l2_regularization learning_rate max_bins max_leaf_nodes min_samples_leaf params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score     13 1.229856 0.098360 0.121906 0.002678 0.947496 0.126528 173 21 3 {'classifier__l2_regularization': 0.9474964020... 0.868705 0.872782 0.873191 0.866912 0.870325 0.870383 0.002388 1   186 1.809153 0.109481 0.156434 0.003111 6.955393 0.136036 168 32 1 {'classifier__l2_regularization': 6.9553925685... 0.867886 0.872236 0.871007 0.867731 0.871690 0.870110 0.001920 2   195 1.138879 0.130649 0.128410 0.010519 16.334498 0.25793 252 16 4 {'classifier__l2_regularization': 16.334498052... 0.868978 0.872372 0.870461 0.864728 0.871963 0.869701 0.002760 3   47 1.520606 0.067083 0.144124 0.011037 0.253969 0.040045 138 28 8 {'classifier__l2_regularization': 0.2539688800... 0.866112 0.871553 0.869915 0.865684 0.868960 0.868445 0.002243 4   98 0.409552 0.040553 0.064756 0.010694 0.000011 0.955962 222 5 4 {'classifier__l2_regularization': 1.1497260106... 0.860516 0.870598 0.868823 0.866230 0.864182 0.866070 0.003536 5     Como tenemos más de 2 parámetros en nuestra búsqueda aleatoria no podemos visualizar los resultados con un mapa de calor. Aún podríamos hacerlo por parejas, pero tener una proyección bidimensional de un problema multidimensional nos puede conducir a una interpresación errónea de las puntuaciones.\nimport numpy as np df = pd.DataFrame( { \"max_leaf_nodes\": cv_results[\"max_leaf_nodes\"], \"learning_rate\": cv_results[\"learning_rate\"], \"score_bin\": pd.cut( cv_results[\"mean_test_score\"], bins=np.linspace(0.5, 1.0, 6) ), } ) sns.set_palette(\"YlGnBu_r\") ax = sns.scatterplot( data=df, x=\"max_leaf_nodes\", y=\"learning_rate\", hue=\"score_bin\", s=50, color=\"k\", edgecolor=None, ) ax.set_xscale(\"log\") ax.set_yscale(\"log\") _ = ax.legend(title=\"mean_test_score\", loc=\"center left\", bbox_to_anchor=(1, 0.5)) En el gráfico podemos ver que las mejores ejecuciones se encuentran en un rango de tasa de aprendizaje de entre 0.01 y 1.0, pero no tenemos control sobre cómo interactúan los otros hiperparámetros en la tasa de aprendizaje. En su lugar, podemos visualizar todos los hiperparámetros al mismo tiempo usando un gráfico de coordenadas paralelas.\ncv_results[\"l2_regularization\"] = cv_results[\"l2_regularization\"].astype(\"float64\") cv_results[\"learning_rate\"] = cv_results[\"learning_rate\"].astype(\"float64\") cv_results[\"max_bins\"] = cv_results[\"max_bins\"].astype(\"float64\") cv_results[\"max_leaf_nodes\"] = cv_results[\"max_leaf_nodes\"].astype(\"float64\") cv_results[\"min_samples_leaf\"] = cv_results[\"min_samples_leaf\"].astype(\"float64\") import plotly.express as px fig = px.parallel_coordinates( cv_results.rename(shorten_param, axis=1).apply( { \"learning_rate\": np.log10, \"max_leaf_nodes\": np.log2, \"max_bins\": np.log2, \"min_samples_leaf\": np.log10, \"l2_regularization\": np.log10, \"mean_test_score\": lambda x: x, } ), color=\"mean_test_score\", color_continuous_scale=px.colors.sequential.Viridis, ) fig.show() Transformamos la mayoría de los valores de los ejes tomando log10 o log2 para distribuir los rangos activos y mejorar la legibilidad del gráfico.\nEl gráfico de coordenadas paralelas muestra los valores de los hiperparámetros en diferentes columnas, mientras que la métrica de rendimiento está codificada por colores. Por tanto, somos capaces de inspeccionar rápidamente si existe un rango de hiperparámetros que funcionan o no.\nEs posible seleccionar un rango de resultados haciendo clic y manteniendo presionado cualquier eje de coordenadas paralelas del gráfico. Luego podemos deslizar (mover) la selección del rango y cruzar dos selecciones para ver las interacciones. Podemos deshacer la selección haciendo clic una vez más en el mismo eje.\nEn particular para esta búsqueda de hiperparámetros, es interesante confirmar que las líneas amarillas (modelos de mejor rendimiento) alcanzan valores intermedios para la tasa de aprendizaje, es decir, valores entre las marcas -2 y 0 que corresponden a valores de tasa de aprendizaje de 0,01 y 1, una vez revertimos la transformación log10 para ese eje.\nPero ahora también podemos observar que no es posible seleccionar modelos de mayor rendimiento seleccionado líneas en el eje max_bins con valores de marcas entre 1 y 3.\nLos otros hiperparámetros no son muy sensibles. Podemos comprobar que si seleccionamos en el eje learning_rate valores entre las marcas -1.5 y -0.5 y en el eje max_bins valores entre las marcas 5 y 8, siempre seleccionamos modelos con el mejor rendimiento, independientemente de los valores de los otros hiperparámetros.\nEvaluación y ajuste de hiperparámetros Hasta el momento hemosvisto dos enfoques para ajustar hiperparámetros. Sin embargo, no hemos presentado una forma apropiada para evaluar los modelos “tuneados”. En su lugar, nos hemos enfocado en el mecanismo usado para encontrar el mejor conjunto de hiperparámetros. Vamos a mostrar cómo evaluar modelos donde los hiperparámetros necesitan ser ajustados.\nPartimos del mismo dataset, el cual hemos dividido en entrenamiento y prueba, y hemos realizado el mismo pipeline de preprocesado.\nmodel #sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 {color: black;background-color: white;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 pre{padding: 0;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-toggleable {background-color: white;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-estimator:hover {background-color: #d4ebff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-item {z-index: 1;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-parallel-item:only-child::after {width: 0;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-54bb3de2-7cb5-486a-8f57-b1d5847e7c38 div.sk-text-repr-fallback {display: none;}Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])Please rerun this cell to show the HTML repr or trust the notebook.PipelinePipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education','marital-status','occupation', 'relationship','race', 'sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))])preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Evaluación sin ajuste de hiperparámetros cv_results = cross_validate(model, X, y, cv=5) cv_results = pd.DataFrame(cv_results) cv_results  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fit_time score_time test_score     0 0.527953 0.057048 0.863036   1 0.525952 0.051044 0.860784   2 0.545971 0.056046 0.860360   3 0.491422 0.052546 0.863124   4 0.495926 0.048542 0.867219     Las puntuaciones de validación cruzada provienen de 5-particiones. Entonces, podemos calcular la media y la desviación típica de la puntuación de generalización.\nprint( f\"Puntuación de generalización sin ajuste de hiperparámetros:\\n\" f\"{cv_results['test_score'].mean():.3f} +/- {cv_results['test_score'].std():.3f}\" ) Puntuación de generalización sin ajuste de hiperparámetros: 0.863 +/- 0.003  Evaluación con ajuste de hiperparámetros Vamos a presentar cómo evaluar el modelo con ajuste de hiperparámetros, lo que requiere un paso extra para seleccionar el mejor conjunto de parámetros. Ya vimos que podemos usar una estrategia de búsqueda que utiliza validación cruzada para encontrar el mejor conjunto de hiperparámetros. Aquí vamos a usar una estrategia de grid-search y reproduciremos los pasos que ya vimos anteriormente.\nEn primer lugar, vamos a incrustar nuestro modelo en un grid-search y especificar los parámetros y los valores de los parámetros que queremos explorar.\nparam_grid = { 'classifier__learning_rate': (0.05, 0.5), 'classifier__max_leaf_nodes': (10, 30), } model_grid_search = GridSearchCV( model, param_grid=param_grid, n_jobs=-1, cv=2 ) model_grid_search.fit(X, y) #sk-d91f18ac-4f33-4d2a-8157-7265844aee0c {color: black;background-color: white;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c pre{padding: 0;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-toggleable {background-color: white;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-estimator:hover {background-color: #d4ebff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-item {z-index: 1;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-parallel-item:only-child::after {width: 0;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-d91f18ac-4f33-4d2a-8157-7265844aee0c div.sk-text-repr-fallback {display: none;}GridSearchCV(cv=2,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))]),n_jobs=-1,param_grid={'classifier__learning_rate': (0.05, 0.5),'classifier__max_leaf_nodes': (10, 30)})Please rerun this cell to show the HTML repr or trust the notebook.GridSearchCVGridSearchCV(cv=2,estimator=Pipeline(steps=[('preprocessor',ColumnTransformer(remainder='passthrough',sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass','education','marital-status','occupation','relationship','race','sex','native-country'])])),('classifier',HistGradientBoostingClassifier(max_leaf_nodes=4,random_state=42))]),n_jobs=-1,param_grid={'classifier__learning_rate': (0.05, 0.5),'classifier__max_leaf_nodes': (10, 30)})preprocessor: ColumnTransformerColumnTransformer(remainder='passthrough', sparse_threshold=0,transformers=[('cat_preprocessor',OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),['workclass', 'education', 'marital-status','occupation', 'relationship', 'race', 'sex','native-country'])])cat_preprocessor['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']OrdinalEncoderOrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)remainderpassthroughpassthroughHistGradientBoostingClassifierHistGradientBoostingClassifier(max_leaf_nodes=4, random_state=42) Como vimos, cuando llamamos al método fit, el modelo embebido en grid-search es entrenado con cada una de las posibles combinaciones de parámetros resultado del cuadrante de parámetros. Se selecciona la mejor combinación, manteniendo aquella combinación que conduce a la mejor puntuación media de validación cruzada.\ncv_results = pd.DataFrame(model_grid_search.cv_results_) cv_results  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  mean_fit_time std_fit_time mean_score_time std_score_time param_classifier__learning_rate param_classifier__max_leaf_nodes params split0_test_score split1_test_score mean_test_score std_test_score rank_test_score     0 0.603518 0.013511 0.260224 0.006005 0.05 10 {'classifier__learning_rate': 0.05, 'classifie... 0.863970 0.864707 0.864338 0.000369 4   1 0.898772 0.002502 0.319024 0.004754 0.05 30 {'classifier__learning_rate': 0.05, 'classifie... 0.871013 0.870317 0.870665 0.000348 1   2 0.300508 0.036280 0.163140 0.023520 0.5 10 {'classifier__learning_rate': 0.5, 'classifier... 0.866426 0.868679 0.867553 0.001126 2   3 0.261725 0.007006 0.152131 0.001501 0.5 30 {'classifier__learning_rate': 0.5, 'classifier... 0.867164 0.866836 0.867000 0.000164 3     model_grid_search.best_params_ {'classifier__learning_rate': 0.05, 'classifier__max_leaf_nodes': 30}  Una importante advertencia aquí es la concerniente a la evaluación del rendimiento de generalización. De hecho, la media y la desviación típica de las puntuaciones calculadas por la validación cruzada en grid-search no son potencialmente buenas estimaciones del rendimiento de generalización que obtendríamos reentrenando un modelo con la mejor combinación de valores de hiperparámetros en el dataset completo. Hay que tener en cuenta que scikit-learn, por defecto, ejecuta automáticamente este reentreno cuando llamamos a model_grid_search.fit. Este modelo reentrenado se entrena con más datos que los diferentes modelos entrenados internamente durante la validación cruzada de grid-search.\nPor lo tanto, usamos el conocimiento del dataset completo para decidir los hiperparámetros de nuestro modelo y entrenar el modelo reajustado. Debido a esto, se debe mantener un conjunto de prueba externo para la evaluación final del modelo reajustado. Destacamos aquí el proceso usando una única división entrenamiento-prueba.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) model_grid_search.fit(X_train, y_train) accuracy = model_grid_search.score(X_test, y_test) print(f\"Precisión en el conjunto de prueba: {accuracy:.3f}\") Precisión en el conjunto de prueba: 0.879  La medida de puntuación en el conjunto de prueba final está casi en el mismo rango que la puntuación de validación cruzada interna para la mejor combinación de hiperparámetros. Esto es tranquilizador, ya que significa que el procedimiento de ajuste no ha provocado un overfitting significativo en sí mismo (de lo contrario, la puntuación de prueba final habría sido más baja que la puntuación de validación cruzada interna). Eso era de esperar porque nuestro grid-search exploró muy pocas combinaciones de hiperparámetros en aras de la velocidad. La puntuación de prueba del modelo final es realmente un poco más alta de la que cabría esperar de la validación cruzada interna. Esto también era de esperar porque el modelo reajustado se entrena en un dataset más grande que los modelos evaluados en el bucle de validación cruzada interno del procedimiento de grid-search. Este suele ser el caso de los modelos entrenados con un gran número de instancias, tienden a generalizar mejor.\nEn el código anterior, la selección de los mejores hiperparámetros se realizó únicamente en el conjunto de entrenamiento de la división inicial entrenamiento-prueba. Después, evaluamos el rendimiento de generalización de nuestro modelo tuneado en el conjunto de prueba restante. Esto se puede mostrar esquemáticamente en el siguiente diagrama:\nEsta figura muestra el caso particular de la estrategia de validación cruzada de K-particiones usando n_splits=5 para dividir el conjunto de entrenamiento proveniente de la división entrenamient-prueba. Para cada división de validación cruzada, el procedimiento entrena un modelo en todas las instancias rojas, evalúa la puntuación de un conjunto dado de hiperparámetros en las instancias verdes. Los mejores hiperparámetros se seleccionan basándose en estas puntuaciones intermedias. El modelo final tuneado con esos hiperparámetros se entrena en la concatenación de instancias rojas y verdes y se evalúa en las instancias azules.\nLas instancias verdes a menudo se denominan conjuntos de validación para diferenciarlos del conjunto de prueba final en azul.\nSin embargo, esta evaluación solo nos proporciona una estimación puntual única del rendimiento de generalización. Como recordamos al principio, es beneficioso disponer de una idea aproximada de la incertidumbre de nuestro rendimiento de generalización estimado. Por lo tanto, deberíamos usar adicionalmente una validación cruzada para esta estimación.\nEste patrón se denomina validación-cruzada anidada. Usamos la validación cruzada interna para la selección de los hiperparámetros y la validación cruzada externa para la evaluación del rendimiento de generalización del modelo tuneado reajustado.\nEn la práctica, solo necesitamos incrustar grid-search en la función cross-validate para ejecutar dicha evaluación.\ncv_results = cross_validate( model_grid_search, X, y, cv=5, n_jobs=-1, return_estimator=True ) cv_results = pd.DataFrame(cv_results) cv_test_scores = cv_results[\"test_score\"] print( \"Puntuación de generalización con ajuste de hiperparámetros:\\n\" f\"{cv_test_scores.mean():.3f} +/- {cv_test_scores.std():.3f}\" ) Puntuación de generalización con ajuste de hiperparámetros: 0.871 +/- 0.003  Este resultado es compatible con la puntuación de prueba medida en la división externa entrenamiento-prueba. Sin embargo, en este caso obtenemos conocimiento sobre la variablidad de nuestra estimación del rendimiento de generalización gracias a la medida de la desviación típica de las puntuaciones medidas en la validación cruzada externa.\nA continuación se muestra una representación esquemática del procedimiento completo de validación cruzada anidada.\nEn la figura se ilustra la estrategia de validación cruzada anidada usando cv_inner = Kfold(n_splits=4) y cv_outer = Kfold(n_splits=5).\nPara cada división de validación cruzada interna (indexada en la parte izquierda), el procedimiento entrena un modelo en todas las muestras rojas y evalúa la calidad de los hiperparámetros en las muestras verdes.\nPara cada división de validación cruzada externa (indexada en la parte derecha), se seleccionan los mejores hiperparámetros basándose en las puntuaciones de validación (calculadas en las muestras verdes) y se reajusta un modelo en la concatenación de las instancias rojas y verdes para esa iteración de validación cruzada externa.\nEl rendimiento de generalización de los 5 modelos reajustados del bucle de validación cruzada externa se evalúa en las instancias azules para obtener las puntuaciones finales.\nPasando el parámetro return_estimator=True podemos comprobar el valor de los mejores hiperparámetros obtenidos para cada partición de la validación cruzada externa.\nfor cv_fold, estimator_in_fold in enumerate(cv_results[\"estimator\"]): print( f\"Mejores hiperparámetros para la partición nº{cv_fold+1}:\\n\" f\"{estimator_in_fold.best_params_}\" ) Mejores hiperparámetros para la partición nº1: {'classifier__learning_rate': 0.05, 'classifier__max_leaf_nodes': 30} Mejores hiperparámetros para la partición nº2: {'classifier__learning_rate': 0.05, 'classifier__max_leaf_nodes': 30} Mejores hiperparámetros para la partición nº3: {'classifier__learning_rate': 0.05, 'classifier__max_leaf_nodes': 30} Mejores hiperparámetros para la partición nº4: {'classifier__learning_rate': 0.5, 'classifier__max_leaf_nodes': 10} Mejores hiperparámetros para la partición nº5: {'classifier__learning_rate': 0.05, 'classifier__max_leaf_nodes': 30}  Es interesante ver si el procedimiento de ajuste de hiperparámetros siempre selecciona valores similares para los hiperparámetros. Si es el caso, entonces todo está bien. Significa que podemos desplegar un modelo ajustado con esos hiperparámetros y esperar que tenga un rendimiento predictivo real cercano al que medimos en la validación cruzada externa.\nPero también es posible que algunos hiperparámetros no tengan ninguna importancia y, como resultado de diferentes sesiones de ajuste, den resultados diferentes. En este caso, servirá cualquier valor. Normalmente esto se puede confirmar haciendo un gráfico de coordenadas paralelas de los resultados de una gran búsqueda de hiperparáemtros, como ya vimos.\nDesde el punto de vista de la implementación, se podría optar por implementar todos los modelos encontrados en el ciclo de validación cruzada externa y votar para obtener las predicciones finales. Sin embargo, esto puede causar problemas operativos debido a que usa más memoria y hace que la predicción sea más lenta, lo que resulta en un mayor uso de recursos computacionales por predicción.\nResumen   Los hiperparámetros tienen un impacto en el rendimiento de los modelos y deben ser elegirse sabiamente;\n  La búsqueda de los mejores hiperparámetros se puede automatizar con un enfoque de grid-search o búsqueda automática;\n  Grid-search es costoso y no escala cuando el número de hiperparámetros a optimizar incrementa. Además, la combinación se muestrea únicamente en una retícula regular.\n  Una búsqueda aleatoria permite buscar con una propuesta fija incluso con un número creciente de hiperparámetros. Además, la combinación se muestrea en una retícula no regular.\n  El overfitting es causado por el tamaño limitado del conjunto de entrenamiento, el ruido en los datos y la alta flexibilidad de los modelos de machine learning comunes.\n  El underfitting sucede cuando las funciones de predicción aprendidas sufren de errores sistemáticos. Esto se puede producir por la elección de la familia del modelo y los parámetros, lo cuales conducen a una carencia de flexibilidad para capturar la estructura repetible del verdadero proceso de generación de datos.\n  Para un conjunto de entrenamiento dado, el objetivo es minimizar el error de preba ajustando la familia del modelo y sus parámetros para encontrar el mejor equilibrio entre overfitting y underfitting.\n  Para una familia de modelo y parámetros dados, incrementar el tamaño del conjunto de entrenamiento disminuirá el overfitting, pero puede causar un incremento del underfitting.\n  El error de prueba de un modelo que no tiene overfitting ni underfitting puede ser alto todavía si las variaciones de la variable objetivo no pueden ser determinadas completamente por las variables de entrada. Este error irreductible es causado por lo que algunas veces llamamos error de etiqueta. En la práctica, esto sucede a menudo cuando por una razón u otra no tenemos acceso a features importantes.\n  Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Ejemplo de un grid-search Ejemplo de una búsqueda aleatoria Ejemplo de una validación cruzada anidada  ","description":"","tags":["hiperparámetros","grid-search","randomized-search","nested-cross-validation","parallel coordinates"],"title":"Ajuste de hiperparámetros","uri":"/posts/hyperparameters-tuning/"},{"categories":["tutoriales"],"content":"En este post proporcionaremos una introducción intuitiva a los conceptos fundamentales de overfitting y underfitting en machine learning. Los modelos de machine learning nunca pueden hacer predicciones perfectas: el error de prueba nunca es exactamente cero. Esta carencia proviene del equilibrio fundamental entre la flexibilidad de modelado y el tamaño limitado del dataset de entrenamiento.\nEn un primer momento definiremos ambos problemas y caracterizaremos cómo y por qué surgen.\nPosteriormente presentaremos una metodología para cuantificar estos problemas contrastando el error de entrenamiento con el error de prueba para varias opciones de la familia de modelos, los parámetros del modelo. Más importante aún, enfatizaremos el impacto del tamaño del dataset de entrenamiento en este equilibrio.\nEn concreto mostraremos los siguientes aspectos:\n la necesidad de dividir los datos en un conjunto de entrenamiento y uno de prueba; el significado de los errores de entrenamiento y prueba; el framework global de validación cruzada con la posibilidad de estudiar las variaciones en el rendimiento de generalización; cómo identificar si un modelo generaliza, existe overfitting o underfitting; cómo comprobar la influencia de un hiperparámetro en el equilibrio underfitting/overfitting; la influencia del número de muestras en un dataset, especialmente en la variabilidad de los errores reportados cuando ejecutamos validación cruzada; la curva de aprendizaje, que es una representación visual de la capacidad de un modelo para mejorar añadiendo nuevas muestras.  Framework de validación cruzada En posts anteriores vimos algunos conceptos relacionados con la evaluación de modelos predictivos. Ahora vamos a analizar algunos detalles del framework de validación cruzada. Antes de ir a ello, vamos a detenernos en las razones de tener siempre conjuntos de entrenamiento y prueba. En primer lugar, echemos un vistazo a la limitación de usar un dataset sin excluir ninguna muestra.\nPara ello vamos a usar el dataset de propiedades de California.\nfrom sklearn.datasets import fetch_california_housing housing = fetch_california_housing(as_frame=True) X, y = housing.data.copy(), housing.target.copy() En este dataset, el objetivo es predecir el valor medio de las casas en un área de California. Las features recopiladas se basan en el mercado de la propiedad y en información geográfica. En este caso, el objetivo a predecir es una variable continua. Por tanto, es una tarea de regresión. Usaremos un modelo predictivo específico de regresión.\nprint(housing.DESCR) .. _california_housing_dataset: California Housing dataset -------------------------- **Data Set Characteristics:** :Number of Instances: 20640 :Number of Attributes: 8 numeric, predictive attributes and the target :Attribute Information: - MedInc median income in block - HouseAge median house age in block - AveRooms average number of rooms - AveBedrms average number of bedrooms - Population block population - AveOccup average house occupancy - Latitude house block latitude - Longitude house block longitude :Missing Attribute Values: None This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/ The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). It can be downloaded/loaded using the :func:`sklearn.datasets.fetch_california_housing` function. .. topic:: References - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297  X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude     0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23   1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22   2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24   3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25   4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25     Para simplificar la visualización, vamos a transformar los precios del rango de cien mil dólares al rango de mil dólares.\ny *= 100 y.head() 0 452.6 1 358.5 2 352.1 3 341.3 4 342.2 Name: MedHouseVal, dtype: float64  Error de entrenamiento vs error de prueba Para resolver esta tarea de regresión usaremos un arbol de decisión de regresión.\nfrom sklearn.tree import DecisionTreeRegressor regressor = DecisionTreeRegressor(random_state=42) regressor.fit(X, y) DecisionTreeRegressor(random_state=42)  Después de entrenar el regresor, nos gustaría saber su potencial rendimiento de generalización una vez lo despleguemos en producción. Para ello, usaremos el error absoluto medio que nos proporciona un error en las mismas unidades del objetivo, es decir, en miles de dólares.\nfrom sklearn.metrics import mean_absolute_error y_predicted = regressor.predict(X) score = mean_absolute_error(y, y_predicted) print(f\"De media, nuestro regresor comete un error de {score:.2f} k$\") De media, nuestro regresor comete un error de 0.00 k$  Obtenemos una predicción perfecta sin errores. Esto es demasiado optimista y casi siempre pone de manifiesto un problema metodológico cuando hacemos machine learning. De hecho, entrenamos y predecimos en el mismo dataset. Dado que nuestro árbol de decisión creció por completo, cada instancia del dataset está almacenada en un nodo hoja. Por tanto, nuestro árbol de decisión ha memorizado completamente el dataset durante el fit y, en consecuencia, no comete ningún error cuando predice.\nEste error calculado anteriormente se denomina error empírico o error de entrenamiento.\nEntrenamos un modelo predictivo para minimizar el error de entrenamiento pero nuestro objetivo es minimizar el error en los datos que no se han visto durante el entrenamiento. Este error se llama también error de generalización o el “verdadero” error de prueba.\nDe esta forma, la evaluación más básica supone:\n dividir nuestro dataset en dos subconjuntos: un conjunto de entrenamiento y un conjunto de prueba; entrenar el modelo en el conjunto de entrenamiento; estimar el error de entrenamiento en el conjunto de entrenamiento; estimar el error de prueba en el conjunto de prueba.  Vamos a dividir nuestro dataset.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=42 ) Ahora lo entrenamos.\nregressor.fit(X_train, y_train) DecisionTreeRegressor(random_state=42)  Finalmente, vamos a estimar los diferentes tipos de error. Empecemos calculando el error de entrenamiento.\ny_predicted = regressor.predict(X_train) score = mean_absolute_error(y_train, y_predicted) print(f\"El error de entrenamiento de nuestro modelo es {score:.2f} k$\") El error de entrenamiento de nuestro modelo es 0.00 k$  Observamos el mismo fenómeno que anteriormente: nuestro modelo memoriza el conjunto de entrenamiento. Sin embargo, vamos a calcular el error de prueba.\ny_predicted = regressor.predict(X_test) score = mean_absolute_error(y_test, y_predicted) print(f\"El error de prueba de nuestro modelo es {score:.2f} k$\") El error de prueba de nuestro modelo es 46.33 k$  Este es el error que realmente cabría esperar de nuestro modelo si lo pusiéramos en un entorno de producción.\nEstabilidad de las estimaciones de validación cruzada Cuando hacemos una única división entrenamiento-prueba no damos ninguna indicación de la robustez de la evaluación de nuestro modelo predictivo: en particular, si el conjunto de prueba es pequeño, esta estimación del error de prueba será inestable y podría no reflejar la “verdadera tasa de error” que observaríamos con el mismo modelo en una cantidad ilimitada de datos de prueba.\nPor ejemplo, podríamos haber tenido suerte cuando hicimos nuestra división aleatoria de nuestro limitado dataset y aislar algunos de los casos más fáciles de predecir del conjunto de prueba solo por casualidad: en este caso, la estimación del error de prueba sería demasiado optimista.\nLa validación cruzada permite estimar la solidez de un modelo predictivo repitiendo el procedimiento de división. Proporcionará varios errores de entrenamiento y prueba y, por tanto, alguna estimación de la variabilidad del rendimiento de generalización del modelo.\nExisten diferentes estrategias de validación cruzada. Por el momento, nos centraremos en una llamada “shuffle-split”. En cada iteración de esta estrategia:\n mezclamos aleatoriamente el orden de las instancias de una copia del dataset; dividimos el dataset mezclado en un conjunto de entrenamiento y uno de prueba; entrenamos un nuevo modelo en el conjunto de entrenamiento; evaluamos el error de prueba en el conjunto de prueba.  Repetimos este procedimiento n_splits veces. Tengamos en mente que el coste computacional se incrementa con n_splits.\nEste diagrama muestra el caso particular de la estrategia shuffle-split de validación cruzada usando n_splits=5. Por cada división de validación cruzada el procedimiento entrena un modelo en todos los ejemplo rojos y evalúa la puntuación del modelo en los ejemplos azules.\nEn este caso estableceremos n_splits=40, lo que significa que entrenaremos 40 modelos en total y todos ellos serán descartados: solo registraremos el rendimiento de generalización de cada variante en el conjunto de prueba.\nPara evaluar el rendimiento de generalización de nuestro regresor podemos usar sklearn.model_selection.cross_validate con un objeto sklearn.model_selection.ShuffleSplit:\nfrom sklearn.model_selection import cross_validate, ShuffleSplit cv = ShuffleSplit(n_splits=40, test_size=0.3, random_state=42) cv_results = cross_validate( regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\" ) import pandas as pd cv_results = pd.DataFrame(cv_results) cv_results.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fit_time score_time test_score     0 0.135116 0.003503 -47.329969   1 0.131113 0.003502 -45.871795   2 0.132114 0.003503 -46.721323   3 0.131615 0.003001 -46.637444   4 0.128611 0.003002 -46.978982     Una puntuación es una métrica donde cuanto más grande sea su valor mejores resultados. Por el contrario, un error es una métrica donde cuanto más pequeño sea su valor mejores resultados. El parámetro scoring en cross_validate siempre espera una función que es una puntuación.\nPara hacerlo fácil, todas las métricas de errores en scikit-learn, como mean_absolute_error, se pueden transformar en una puntuación para ser usadas en cross_validate. Para hacerlo necesitamos pasar el nombre de la métrica de error con el prefijo neg_. Por ejemplo, scoring=\"neg_mean_absolute_error\". En este caso, el negativo del error absoluto medio calculado equivaldría a una puntuación.\nVamos a revertir la negación para obtener el error real:\ncv_results[\"test_error\"] = -cv_results[\"test_score\"] cv_results.head(10)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fit_time score_time test_score test_error     0 0.135116 0.003503 -47.329969 47.329969   1 0.131113 0.003502 -45.871795 45.871795   2 0.132114 0.003503 -46.721323 46.721323   3 0.131615 0.003001 -46.637444 46.637444   4 0.128611 0.003002 -46.978982 46.978982   5 0.132614 0.003503 -45.130082 45.130082   6 0.131113 0.003503 -47.191726 47.191726   7 0.131613 0.003504 -45.808697 45.808697   8 0.132113 0.003503 -45.814624 45.814624   9 0.133615 0.003503 -46.106001 46.106001     Obtenemos información del tiempo de entrenamiento y predicción de cada iteración de validación cruzada. También obtenemos la puntuación de prueba que corresponde al error de prueba de cada división.\nlen(cv_results) 40  Obtenemos 40 entradas en nuestro dataframe resultante debido a las 40 divisiones realizadas. Por lo tanto, podemos mostrar la distribución del error de prueba y, así, tener una estimación de su variabilidad.\nimport matplotlib.pyplot as plt cv_results[\"test_error\"].plot.hist(bins=10, edgecolor=\"black\") plt.xlabel(\"error absoluto medio (k$)\") _ = plt.title(\"Distribución del error de prueba\") Observamos que el error de prueba se agrupa en torno a 47 k$ y un rango de entre 45 k$ y 48.5 k$.\nprint(f\"El error medio de validación cruzada es: \" f\"{cv_results['test_error'].mean():.2f} k$\") El error medio de validación cruzada es: 46.53 k$  print(f\"La desviación típica de validación cruzada es: \" f\"{cv_results['test_error'].std():.2f} k$\") La desviación típica de validación cruzada es: 0.83 k$  Observemos que la desviación típica es mucho más pequeña que la media. Podemos resumirlo como que nuestra estimación de validación cruzada del error de prueba es de 46.53 +/- 0.83 k$. Si tuviéramos que entrenar un único modelo en el dataset completo (sin validación cruzada) y luego después tuviéramos acceso a una cantidad ilimitada de datos de prueba, cabría esperar que el error de prueba verdadero cayera dentro de esa región.\nAunque esta información es interesante por sí misma, debería ser contrastada con la escala de la variabilidad natural del vector objetivo de nuestro dataset. Vamos a dibujar la distribución de esta variable objetivo:\ny.plot.hist(bins=20, edgecolor=\"black\") plt.xlabel(\"Valor medio de la vivienda (k$)\") _ = plt.title(\"Distribución del objetivo\") print(f\"La desviación típica del objetivo es: {y.std():.2f} k$\") La desviación típica del objetivo es: 115.40 k$  El rango de la variable objetivo varía desde cercano a 0 hasta 500, con una desviación típica de 115. Remarquemos que la media estimada del error de prueba obtenido por validación cruzada es un poco más pequeño que la escala natural de variación de la variable objetivo. Además, la desviación típica de la validación cruzada estimada del error de prueba es incluso más pequeña. Esto es un buen comienzo, pero no necesariamente suficiente para decidir si el rendimiento de generalización es suficientemente bueno para que nuestra predicción sea útil en la práctica.\nRecordemos que nuestro modelo tiene, de media, un error de alrededor de 47 k$. Con esta información y mirando la distribución del objetivo, tal error podría ser aceptable cuando predecimos viviendas con un valor de 500 k$. Sin embargo, sería un problema con una vivienda con un valor de 50 k$. Por tanto, esto indica que nuestra métrica (Error Absoluto Medio) no es ideal.\nEn su lugar podríamos elegir una métrica relativa al valor del objetivo a predecir: el error porcentual absoluto medio habría sido una mejor opción. Pero en todos los casos, un error de 47 k$ podría ser demasiado grande para usar automáticamente nuestro modelo para etiquetar viviendas sin la supervisión de un experto.\nMás detalles sobre cross_validate Durante la validación cruzada, se entrenan y evalúan muchos modelos. De hecho, el número de elementos de cada matriz de salida de cross_validate es el resultado de uno de estos procedimientos fit / score. Para hacerlo explícito, es posible recuperar estos modelos entrenados para cada una de las divisiones/particiones pasando la opción return_estimator=True en cross_validate.\ncv_results = cross_validate(regressor, X, y, return_estimator=True) cv_results {'fit_time': array([0.15363216, 0.15012908, 0.15063 , 0.15063 , 0.14562511]), 'score_time': array([0.00250292, 0.00250196, 0.00250196, 0.00250268, 0.00250244]), 'estimator': (DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42)), 'test_score': array([0.28326244, 0.4226389 , 0.45552292, 0.23727262, 0.41430376])}  cv_results[\"estimator\"] (DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42), DecisionTreeRegressor(random_state=42))  Los cinco regresores de árbol de decisión corresponden a los cinco árboles de decisión entrenados en las diferentes particiones. Tener acceso a estos regresores es útil porque permite inspeccionar los parametros entrenados internos de estos regresores.\nEn el caso de que solo estemos interesados en la puntuación de prueba, scikit-learn provee una función cross_val_score. Es idéntica a llamar a la función cross_validate y seleccionar solo test_score.\nfrom sklearn.model_selection import cross_val_score scores = cross_val_score(regressor, X, y) scores array([0.28326244, 0.4226389 , 0.45552292, 0.23727262, 0.41430376])  Overfit-generalización-underfit Anteriormente presentamos el frameword de validación cruzada general y cómo nos ayuda a cuantificar los errores de entrenamiento y prueba, así como sus fluctuaciones.\nAhora pondremos estos errores en perspectiva y mostraremos cómo nos pueden ayudar a saber si nuestro modelo generaliza, se produce overfitting o underfitting.\nUsaremos de nuevo el mismo dataset y crearemos el mismo modelo que anteriormente.\nhousing = fetch_california_housing(as_frame=True) X, y = housing.data.copy(), housing.target.copy() y *= 100 regressor = DecisionTreeRegressor() Overfittin vs underfitting Para comprender mejor el rendimiento de generalización de nuestro modelo y encontrar quizás alguna percepción de cómo mejorarlo, compararemos el error de prueba con el error de entrenamiento. Por tanto, necesitamos calcular el error en el conjunto de entrenamiento, lo cual es posible utilizando la función cross_validate.\ncv = ShuffleSplit(n_splits=30, test_size=0.3, random_state=42) cv_results = cross_validate(regressor, X, y, cv=cv, scoring=\"neg_mean_absolute_error\", return_train_score=True, n_jobs=-1 ) cv_results = pd.DataFrame(cv_results) La validación cruzada usa el error absoluto medio negativo. Lo transformamos a positivo:\nscores = pd.DataFrame() scores[[\"train_error\", \"test_error\"]] = -cv_results[ [\"train_score\", \"test_score\"] ] scores.plot.hist(bins=50, edgecolor=\"black\") plt.xlabel(\"Error absoluto medio (k$)\") _ = plt.title(\"Distribución errores entrenamiento y prueba via validación cruzada\") Al dibujar la distribución de los errores de entrenamiento y prueba, obtenemos información sobre si en nuestro modelo se produce overfitting, underfitting o ambos a la vez.\nAquí observamos un pequeño error de entrenamiento (realmente cero), lo que significa que el modelo no realiza underfitting: es lo suficientemente flexible para capturar cualquier variación presente en el conjunto de entrenamiento. Sin embargo, el significativamente grande error de prueba nos dice que sí existe overfitting: el modelo ha memorizado muchas variaciones del conjunto de entrenamiento que podrían considerarse “ruidosas” porque no generalizan para ayudarnos a realizar una buena predicción en el conjunto de prueba.\nCurva de validación Algunos hiperparámetros del modelo suelen ser la clave para evolucionar de un modelo que realiza underfitting a un modelo que hace overfitting, con suerte pasando por una región donde podemos obtener un buen equilibrio entre ambos. Podemos adquirir conocimiento dibujando una curva llamada curva de validación. Esta curva también se puede aplicar al ejemplo anterior para variar el valor de un hiperparámetro.\nPara un árbol de decisión, el paramétro max_depth se usa para controlar el equilibrio entre underfitting y overfitting.\n%%time from sklearn.model_selection import validation_curve max_depth = [1, 5, 10, 15, 20, 25] train_scores, test_scores = validation_curve( regressor, X, y, param_name=\"max_depth\", param_range=max_depth, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) train_errors, test_errors = -train_scores, -test_scores Wall time: 1.86 s  Ahora que hemos coleccionado los resultados mostraremos la curva de validación dibujando los errores de entrenamiento y prueba (así como sus desviaciones).\nplt.plot(max_depth, train_errors.mean(axis=1), label=\"Error entrenamiento\") plt.plot(max_depth, test_errors.mean(axis=1), label=\"Error prueba\") plt.legend() plt.xlabel(\"Máxima profundidad del árbol de decisión\") plt.ylabel(\"Error absoluto medio (k$)\") _ = plt.title(\"Curva de decisión para el árbol de decisión\") La curva de validación se puede dividir en 3 zonas:\n  Para max_depth \u003c 10, el árbol de decisión produce underfitting. Tanto el error de entrenamiento como el de prueba son altos. El modelo es demasiado restrictivo y no puede capturar mucha de la variabilidad de la variable objetivo.\n  La región alrededor de max_depth = 10 corresponde con el parámetro para el cual el árbol de decisión generaliza mejor. Es lo suficientemente flexible para capturar una fracción de la variabilidad del objetivo que se generaliza, mientras que no memoriza todo el ruido en el objetivo.\n  Para max_depth \u003e 10, el árbol de decisión produce overfitting. El error de entrenamiento se convierte en muy pequeño, mientras que el error de prueba aumenta. En esta región, los modelos crean decisiones específicamente para muestras ruidosas que dañan su capacidad para generalizar a los datos de prueba.\n  Observemos que para max_depth = 10 el modelo produce un poco de overfitting ya que hay una brecha entre el error de entrenamiento y el error de prueba. Al mismo tiempo también produce underfitting, ya que el error de entrenamiento aún está lejos de cero (más de 30 k$), lo que significa que el modelo aún podría estar limitado para modelar partes interesantes de los datos. Sin embargo, el error de prueba es mínimo y esto es lo que realmente importa. Este es el mejor compromiso que podemos alcanzar ajustando únicamente este hiperparámetro.\nTengamos en cuenta que mirar los errores medios es bastante limitante. También debemos observar la desviación típica para comprobar la dispersión de la puntuación. Podemos repetir el mismo gráfico de antes, pero añadiendo alguna información para mostrar también la desviación típica de los errores.\nplt.errorbar(max_depth, train_errors.mean(axis=1), yerr=train_errors.std(axis=1), label=\"Error entrenamiento\") plt.errorbar(max_depth, test_errors.mean(axis=1), yerr=test_errors.std(axis=1), label=\"Error prueba\") plt.legend() plt.xlabel(\"Máxima profundidad del árbol de decisión\") plt.ylabel(\"Error absoluto medio (k$)\") _ = plt.title(\"Curva de decisión para el árbol de decisión\") Tuvimos suerte de que la varianza de los errores fuera pequeña en comparación con sus respectivos valores, por tanto las conclusiones anteriores son claras, aunque esto no es necesariamente siempre el caso.\nEfecto del tamaño de la muestra en la validación cruzada Hemos visto anteriormente el framework de validación cruzada general y cómo evaluar si en un modelo se produce underfitting, overfitting o generalización. Además de estos aspectos, también es importante comprender cómo los diferentes errores se ven influenciados por el número de muestras disponibles. Vamos a mostrar este aspecto al observar la variablidad de los diferentes errores.\nPartimos del mismo dataset y modelo que teníamos anteriormente (X, y y regressor)\nCurva de aprendizaje Para comprender el impacto del número de muestras disponibles para entrenamiento en el rendimiento de generalización de un modelo predictivo, es posible reducir sintéticamente el número de muestras usadas para entrenar el modelo predictivo y verificar los errores de entrenamiento y prueba.\nPor tanto, podemos variar el número de muestras del conjunto de entrenamiento y repetir el entrenamiento. Las puntuaciones de entrenamiento y prueba se pueden dibujar de forma similar a la curva de validación, pero en lugar de variar un hiperparámetro, variamos el número de muestras de entrenamiento. Esta curva se llama curva de aprendizaje. Proporciona información sobre el beneficio de añadir nuevas muestras de entrenamiento para mejorar el rendimiento de generalización de un modelo.\nVamos a calcular la curva de aprendizaje de un árbol de decisión y a variar la proporción del conjunto de entrenamiento del 10% al 100%.\nimport numpy as np train_sizes = np.linspace(0.1, 1.0, num=5, endpoint=True) train_sizes array([0.1 , 0.325, 0.55 , 0.775, 1. ])  Usaremos ShuffleSplit de validación cruzada para evaluar nuestro modelo predictivo.\ncv = ShuffleSplit(n_splits=30, test_size=0.2) Ahora ya tenemos todo configurado para comenzar el experimento.\nfrom sklearn.model_selection import learning_curve results = learning_curve( regressor, X, y, train_sizes=train_sizes, cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=-1 ) train_size, train_scores, test_scores = results[:3] # Convierte las puntuaciones en errores train_errors, test_errors = -train_scores, -test_scores plt.errorbar(train_size, train_errors.mean(axis=1), yerr=train_errors.std(axis=1), label=\"Error entrenamiento\") plt.errorbar(train_size, test_errors.mean(axis=1), yerr=test_errors.std(axis=1), label=\"Error prueba\") plt.legend() plt.xscale(\"log\") plt.xlabel(\"Nº de muestras en el conjunto entrenamiento\") plt.ylabel(\"Error absoluto medio (k$)\") _ = plt.title(\"Curva de aprendizaje para el árbol de decisión\") Observando por separado el error de entrenamiento, vemos que obtenemos un error de 0 k$. Lo que significa que en el modelo se produce claramente overfitting de los datos de entrenamiento.\nObservando por separado el error de prueba, vemos que cuantas más muestras se añaden al conjunto de entrenamiento, menor es el error de prueba. Además, estamos buscando la meseta del error de prueba para la cual ya no existe beneficio de seguir añadiendo muestras o evaluar la potencial ganancia de añadir más muestras en el conjunto de entrenamiento. Si alcanzamos una meseta y añadir nuevas muestras al conjunto de entrenamiento no reduce el error de prueba, es posible que hayamos alcanzado la tasa de error de Bayes utilizando el modelo disponible. El uso de un modelo más complejo podría ser la única posibilidad de reducir aún más el error de prueba.\nResumen   El overfitting es causado por el tamaño limitado del conjunto de entrenamiento, el ruido en los datos y la alta flexibilidad de los modelos de machine learning comunes.\n  El underfitting sucede cuando las funciones de predicción aprendidas sufren de errores sistemáticos. Esto se puede producir por la elección de la familia del modelo y los parámetros, los cuales conducen a una carencia de flexibilidad para capturar la estructura repetible del verdadero proceso de generación de datos.\n  Para un conjunto de entrenamiento dado, el objetivo es minimizar el error de prueba ajustando la familia del modelo y sus parámetros para encontrar el mejor equilibrio entre overfitting y underfitting.\n  Para una familia de modelo y parámetros dados, incrementar el tamaño del conjunto de entrenamiento disminuirá el overfitting, pero puede causar un incremento del underfitting.\n  El error de prueba de un modelo que no tiene overfitting ni underfitting puede ser alto todavía si las variaciones de la variable objetivo no pueden ser determinadas completamente por las variables de entrada. Este error irreductible es causado por lo que algunas veces llamamos error de etiqueta. En la práctica, esto sucede a menudo cuando por una razón u otra no tenemos acceso a features importantes.\n  Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Ilustración de los conceptos de underfitting y overfitting. Diferencia entre puntuación de entrenamiento y prueba. Ejemplo de curva de validación  ","description":"","tags":["overfitting","underfitting","validación cruzada","curva de validación","curva de aprendizaje"],"title":"Selección del mejor modelo","uri":"/posts/selecting-best-model/"},{"categories":["tutoriales"],"content":"En este post vamos a presentar un ejemplo de un pipeline de modelado predictivo típico usando datos tabulares, es decir, que pueden ser estructurados en una tabla de 2 dimensiones. En primer lugar, analizaremos el dataset usado y posteriormente entrenaremos un primer pipeline predictivo. Después prestaremos atención a los tipos de datos que tiene que manejar nuestro modelo: numéricos y categóricos. Por último, extenderemos nuestro pipeline para tipos de datos mixtos, es decir, numéricos y categóricos.\nEl objetivo a conseguir es construir intuiciones respecto a un dataset desconocido, identificar y discriminar features numéricas y categóricas y, finalmente, crear un pipeline predictivo avanzado con scikit-learn.\nEn concreto mostraremos los siguientes aspectos:\n identificar datos númericos en un dataset heterogéneo seleccionar el subconjunto de columnas correspondientes a datos numéricos usar la función de scikit-learn train_test_split para dividir los datos en entrenamiento y prueba entrenar y evaluar un modelo de regresión logística la importancia de evaluar el rendimiento de generalización en los datos de prueba usar un predictor dummy para obtener una línea base de referencia ver la importancia de escalar las variables numéricas usar un pipeline para encadenar el escalado y el entrenamiento de una regresión logística evaluar el rendimiento de generalización de nuestro modelo a partir de validación cruzada mostrar las dos estrategias comunes para codificar variables categóricas: ordinal y one-hot usar un pipeline para utilizar un codificador one-hot antes de entrenar un predictor usar un ColumnTransformer para aplicar preprocesamientos diferentes a variables numéricas y categóricas usar un pipeline para encadenar el preprocesamiento ColumnTransformer y entrenar una regresión logística  Primer vistazo al dataset Antes de llevar a cabo cualquier tarea de machine learning hay que realizar un serie de pasos:\n cargar los datos. observar las variables del dataset, diferenciando entre variables numéricas y categóricas, las cuales necesitarán un preprocesamiento diferente en la mayoría de los flujos de machine learning. visualizar la distribución de las variables para obtener algún tipo de conocimiento o idea del dataset.  Usaremos el dataset “credit-g”. Para más detalles sobre dicho dataset puedes acceder al link https://www.openml.org/d/31. El objetivo del dataset es clasificar a las personas por un conjunto de atributos como buenas o malas respecto al riesgo crediticio. Los datos están disponibles en un fichero CSV y usaremos pandas para leerlo.\nimport numpy as np import pandas as pd credit = pd.read_csv(\"credit-g.csv\") Las variables del dataset Los datos se almacenan en un dataframe de pandas. Un dataframe es una estructura de datos de 2 dimensiones. Este tipo de datos también se denominan datos tabulares.\nCada fila representa un “ejemplo”. En el ámbito de machine learning se usan normalmente los términos equivalentes de “registro”, “instancia” u “observación”.\nCada columna representa un tipo de información que ha sido recopilada y se denominan “features”. En el campo de machine learning es normal usar los términos equivalentes de “variable”, “atributo” o “covariable”.\nEchemos un vistazo rápido al dataframe para mostrar las primeras filas:\ncredit.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status duration credit_history purpose credit_amount savings_status employment installment_commitment personal_status other_parties ... property_magnitude age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class     0 '\u003c0' 6 'critical/other existing credit' radio/tv 1169 'no known savings' '\u003e=7' 4 'male single' none ... 'real estate' 67 none own 2 skilled 1 yes yes good   1 '0\u003c=X\u003c200' 48 'existing paid' radio/tv 5951 '\u003c100' '1\u003c=X\u003c4' 2 'female div/dep/mar' none ... 'real estate' 22 none own 1 skilled 1 none yes bad   2 'no checking' 12 'critical/other existing credit' education 2096 '\u003c100' '4\u003c=X\u003c7' 2 'male single' none ... 'real estate' 49 none own 1 'unskilled resident' 2 none yes good   3 '\u003c0' 42 'existing paid' furniture/equipment 7882 '\u003c100' '4\u003c=X\u003c7' 2 'male single' guarantor ... 'life insurance' 45 none 'for free' 1 skilled 2 none yes good   4 '\u003c0' 24 'delayed previously' 'new car' 4870 '\u003c100' '1\u003c=X\u003c4' 3 'male single' none ... 'no known property' 53 none 'for free' 2 skilled 2 none yes bad    5 rows × 21 columns\n credit.shape (1000, 21)  El dataset está compuesto de 1.000 instancias y 21 variables. La columna llamada class es nuestra variable objetivo (es decir, la variable que queremos predecir). Las dos posibles clases son good (bajo riesgo credicitio) y bad (alto riesgo crediticio). El problema de predicción resultante es, por tanto, un problema de clasificación binaria. Usaremos el resto de columnas como variables de entrada para nuestro modelo.\ncredit[\"class\"].value_counts() good 700 bad 300 Name: class, dtype: int64  credit[\"class\"].value_counts().plot.pie(autopct='%1.2f%%'); Vemos que las clases están desbalanceadas, lo que significa que tenemos más instancias de una o más clases comparada con las otras. El desequilibro de clases sucede frecuentemente en la práctica y puede requerir de técnicas especiales al construir el modelo predictivo. Veremos este tipo de técnicas en otros posts.\ncredit.dtypes checking_status object duration int64 credit_history object purpose object credit_amount int64 savings_status object employment object installment_commitment int64 personal_status object other_parties object residence_since int64 property_magnitude object age int64 other_payment_plans object housing object existing_credits int64 job object num_dependents int64 own_telephone object foreign_worker object class object dtype: object  credit.dtypes.value_counts() object 14 int64 7 dtype: int64  Comprobamos que el dataset contiene tanto datos numéricos (7 features) como categóricos (14 features, incluyendo la variable objetivo). En este caso sus tipos son int64 y object, respectivamente.\nInspección visual de los datos Antes de construir cualquier modelo predictivo es buena idea echar un vistazo a los datos:\n quizás la tarea que estamos intentando conseguir se pueda resolver sin utilizar machine learning; debemos comprobar que la información que necesitamos se encuentra presente realmente en el dataset; inspeccionar los datos en una buena forma de encontrar peculiaridades. Estas pueden aparecer durante la recolección de los datos (por ejemplo, debido al malfuncionamiento de sensores o valores faltantes) o en la forma en que los datos son procesados posteriormente (por ejemplo, valores “capados”).  Echemos un vistazo a las distribuciones de las features individualmente para obtener algún conocimiento adicional sobre los datos. Podemos empezar dibujando histogramas, aunque esto solo aplicaría a las features numéricas:\n_ = credit.hist(figsize=(20, 14)) C:\\Program Files\\Python39\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col():  Algunos comentarios sobre estas variables:\n duration: la mayoría de las personas a las que se les concede el crédito su duración está entre aproximadamente 4 y 24 meses, principalmente entre 12 y 24 meses. credit_amount: la mayoría de las personas solicita un crédito menor de 4.000 aproximadamente. age: la mayoría de las personas que solicitan un crédito son menores de 40 años.  Veamos la distribución de algunas variables categóricas:\nimport seaborn as sns _ = sns.countplot(x=\"checking_status\", data=credit) ax = sns.countplot(x=\"credit_history\", data=credit) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\"); Bueno, hasta ahora hemos visto cómo cargar un dataset, calcular su tamaño y visualizar de forma rápida las primeras filas del mismo. En un primer análisis de las variables que lo componen, hemos identificado nuestra variable objetivo y diferenciado las variables numéricas y categóricas. También hemos podido observar cómo se distribuyen sus valores.\nModelo simple con scikit-learn Vamos a crear un primer modelo predictivo, para lo cual solo usaremos las variables numéricas. Los datos numéricos son el tipo de datos más natural en machine learning y (casi) pueden incorporarse directamente a los modelos predictivos.\nComo hemos visto, el archivo CSV contiene toda la información que necesitamos: el objetivo que nos gustaría predecir (es decir, class) y los datos que queremos usar para entrenar nuestro modelo predictivo (es decir, las columnas restantes). El primer paso es separar las columnas para obtener de un lado el objetivo y del otro lado los datos.\nSeparar los datos y el objetivo target_name = \"class\" y = credit[target_name] data = credit.drop(columns=[target_name]) Vamos a usar una función de sklearn que nos permite seleccionar las columnas en función del tipo de dato.\nfrom sklearn.compose import make_column_selector as selector numerical_columns_selector = selector(dtype_include=np.number) numerical_columns = numerical_columns_selector(data) numerical_columns ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']  X = data[numerical_columns] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     0 6 1169 4 4 67 2 1   1 48 5951 2 2 22 1 1   2 12 2096 2 3 49 1 2   3 42 7882 2 4 45 1 2   4 24 4870 3 4 53 2 2     Entrenar un modelo y hacer predicciones Vamos a construir un modelo de clasificación usando regresión logística, que pertenece a la familia de los modelos lineales.\nBrevemente, los modelos lineales buscan un conjunto de pesos para combinar linealmente las features y predecir el objetivo. Por ejemplo, el modelo puede generar un regla como la siguiente:\n  si 0.1 * duration + 3.3 * credit_amount - 15.1 * installment_commitment + 3.2 * residence_since - 0.2 * age + 1.3 * existing_credits - 0.9 * num_dependents + 13.2 \u003e 0, predice good\n  en caso contrario predice bad\n  El metodo fit se llama para entrenar el modelo a partir de los datos de entrada (features) y objetivo.\nfrom sklearn import set_config set_config(display=\"diagram\") from sklearn.linear_model import LogisticRegression model = LogisticRegression(max_iter=500) model.fit(X, y) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}LogisticRegressionLogisticRegression(max_iter=500) El proceso de aprendizaje puede representarse de la siguiente forma:\nEl método fitse compone de dos elementos: un algoritmo de aprendizaje y algunos estados del modelo. El algoritmo de aprendizaje toma los datos y el objetivo de entrenamiento como entrada y establece los estados del modelo. Estos estados del modelo se utilizarán posteriormente para predecir (por clasificadores o regresores) o transformar los datos (por transformadores).\nTanto el algoritmo de aprendizaje como el tipo de estados del modelo son específicos para cada tipo de modelo.\nUsaremos ahora nuestro modelo para llevar a cabo algunas predicciones usando el mismo dataset.\ny_predicted = model.predict(X) El mecanismo de predicción puede representarse de la siguiente forma:\nPara predecir, un modelo usa una función de predicción que utilizará los datos de entrada junto con los estados del modelo. Como el algoritmo de aprendizaje y los estados del modelo, la función de predicción es específica para cada tipo de modelo.\nVamos a revisar las predicciones calculadas. Por simplicidad vamos a echar un vistazo a los primeros cinco objetivos predichos.\ny_predicted[:5] array(['good', 'bad', 'good', 'good', 'good'], dtype=object)  De hecho, podemos comparar estas predicciones con los datos reales:\ny[:5] 0 good 1 bad 2 good 3 good 4 bad Name: class, dtype: object  e incluso podríamos comprobar si las predicciones concuerdan con los objetivos reales:\ny_predicted[:5] == y[:5] 0 True 1 True 2 True 3 True 4 False Name: class, dtype: bool  print(f\"Nº de predicciones correctas: {(y_predicted[:5] == y[:5]).sum()} de las 5 primeras\") Nº de predicciones correctas: 4 de las 5 primeras  En este caso, parece que nuestro modelo comete un error al predecir la quinta instancia. Para obtener un mejor evaluación podemos calcular la tasa promedio de éxito:\n(y_predicted == y).mean() 0.706  ¿Podemos confiar en esta evaluación? ¿Es buena o mala?\nDivisión de los datos en entrenamiento y prueba Cuando construimos un modelo de machine learning es muy importante evaluar el modelo entrenado en datos que no se hayan usado para entrenarlo, ya que la generalización es más que la memorización (significa que queremos una regla que generalice a nuevos datos, sin comparar los datos memorizados). Es más difícil concluir sobre datos nunca vistos que sobre los ya vistos.\nLa evaluación correcta se realiza fácilmente reservando un subconjunto de los datos cuando entrenamos el modelo y usándolos posteriormente para evaluar el modelo. Los datos usados para entrenar un modelo se denominan datos de entrenamiento mientras que los datos usados para evaluar el modelo se denominan datos de prueba.\nEn ocasiones podemos contar con dos datasets separados, uno para el entrenamiento y otro para pruebas. Sin embargo, esto suele ser bastante inusual. La mayoría de las veces tendremos un único archivo que contiene todos los datos y necesitaremos dividirlo una vez cargado en memoria.\nScikit-learn proporciona la función sklearn.model_selection.train_test_split, que usaremos para dividir automáticamente el dataset en dos subconjuntos.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=42, test_size=0.20) Cuando llamamos a la función train_test_split, especificamos que queremos tener el 20% de las instancias en el conjunto de prueba y las instancias restantes (80%) estarán disponibles para el conjunto de entrenamiento.\nEstablecimiento de una línea base Para avaluar el rendimiento de nuestro modelo predictivo resulta de utilidad establecer una línea base simple. La línea base más simple para un clasificador es aquella que predice siempre la misma clase, independientemente de los datos de entrada. Para ello usaremos un DummyClassifier.\nfrom sklearn.dummy import DummyClassifier clf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=42) clf_dummy.fit(X_train, y_train) accuracy_dummy = clf_dummy.score(X_test, y_test) print(f\"Accuracy línea base: {accuracy_dummy}\") Accuracy línea base: 0.705  Este clasificador dummy predice siempre la clase más frecuente (en nuestro caso, la clase good). Como vimos anteriormente la proporción de clase good era del 70%, que coincide con la puntuación obtenida por este clasificador. Bien, ya tenemos una linea base con la que comparar nuestro modelo.\nVamos a entrenar el modelo exactamente de la misma forma que vimos anteriormente, excepto que usaremos para ello los subconjuntos de entrenamiento:\nmodel = LogisticRegression(max_iter=500) model.fit(X_train, y_train) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}LogisticRegressionLogisticRegression(max_iter=500) En lugar de calcular la predicción y calcular manualmente la tasa media de éxito, podemos usar el método score. Cuando se trata de clasificadores este método devuelve su métrica de rendimiento.\naccuracy_lgr = model.score(X_test, y_test) print(f\"Accuracy: {accuracy_lgr:.3f}\") Accuracy: 0.740  Veamos el mecanismo subyacente cuando se llama al método score:\nPara calcular la puntuación, el predictor primero calcula las predicciones (usando el metodo predict) y luego usa una función de puntuación para comparar los objetivos reales y las predicciones. Por último, se devuelve la puntuación.\nPor norma general, nos referimos al rendimiento de generalización de un modelo cuando nos refiramos a la puntuación de prueba o al error de prueba obtenido al comparar la predicción de un modelo con los objetivos reales. También son términos equivalentes rendimiento predictivo y rendimiento estadístico. Nos referimos al rendimiento computacional de un modelo predictivo cuando accedemos al coste computacional de entrenar un modelo predictivo o usarlo para hacer predicciones.\nBueno, la puntuación de nuestro modelo apenas mejora la linea base que establecimos:\nprint(f\"Accuracy línea base = {accuracy_dummy}\") print(f\"Accuracy regresión logística = {accuracy_lgr}\") Accuracy línea base = 0.705 Accuracy regresión logística = 0.74  Seguro que podemos hacerlo mejor. Veamos cómo.\nPreprocesamiento de features numéricas En los siguientes apartados vamos a introducir el uso del preprocesamiento, en este caso del escalado de variables numéricas; y del uso de un pipeline para encadenar el preprocesamiento y el entrenamiento del modelo.\nScikit-learn cuenta con una amplia gama de algoritmos de preprocesamiento que nos permiten transformar los datos de entrada antes de entrenar un modelo. Es este caso, vamos a estandarizar los datos y después entrenaremos un nuevo modelo de regresión logística es esta nueva versión del dataset.\nEn primer lugar, vamos a mostrar algunas estadísticas sobre los datos de entrenamiento.\nX_train.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     count 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000   mean 21.095000 3360.618750 2.965000 2.846250 35.558750 1.411250 1.143750   std 11.807211 2898.174863 1.122653 1.105277 11.411587 0.578828 0.351056   min 4.000000 250.000000 1.000000 1.000000 19.000000 1.000000 1.000000   25% 12.000000 1380.000000 2.000000 2.000000 27.000000 1.000000 1.000000   50% 18.000000 2333.000000 3.000000 3.000000 33.000000 1.000000 1.000000   75% 24.000000 4154.500000 4.000000 4.000000 42.000000 2.000000 1.000000   max 60.000000 18424.000000 4.000000 4.000000 75.000000 4.000000 2.000000     Observamos que las features del dataset abarcan diferentes rangos. Algunos algoritmos hacen suposiciones con respecto a las distribuciones de las features y, en general, la normalización de estas features resultará de utilidad para abordar estas suposiciones. Algunas razones para el escalado de features son las siguientes:\n  Los modelos basados en distancias entre pares de instancias, por ejemplo, k-nearest neighbors, deben ser entrenados con features normalizadas para hacer que cada feature contribuya aproximadamente por igual a los cálculos de distancias.\n  Muchos modelos, como la regresión logística usan solucionadores numéricos (basados en descenso de gradiente) para encontrar sus parámetros óptimos. Este solucionador converge más rápido cuando las features están escaladas.\n  El hecho de que un modelo de machile learning requiera o no de escalado de features depende de la familia del modelo. Los modelos lineales como la regresión logística generalmente se benefician del escalado mientras que otros modelos como los árboles de decisión no necesitan de este preprocesado (pero tampoco se verían penalizados).\nVamos a mostrar cómo aplicar tal normalización usando un transformador de scikit-learn llamado StandardScaler. Este transformador escala cada feature individualmente para que todas tengan media 0 y desviación estándar 1. Analizaremos los diferentes pasos usados por scikit-learn para conseguir esta transformación de los datos.\nLo primero que necesitamos es llamar al método fit para que aprenda el escalado de los datos.\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}StandardScalerStandardScaler() El método fit de los transformadores es similar al método fit de los predictores. La diferencia principal es que el transformador tiene un único argumento, la matriz de datos, mientras que el último tiene dos argumentos, la matriz de datos y el objetivo. En este caso, el algoritmo necesita calcular la media y la desviación típica de cada feature y almacenarla en algunas arrays de Numpy. Aquí, estos estadísticos son los estados del modelo. El hecho de que los estados del modelo de este scaler sean arrays de medias y desviaciones típicas es específico del StandardScale. Otros transformadores de scikit-learn calcularán diferentes estadísticos y los almacenarán como estados del modelo de la misma forma.\nVamos a inspeccionar las medias y desviaciones típicas calculadas.\nscaler.mean_ array([2.10950000e+01, 3.36061875e+03, 2.96500000e+00, 2.84625000e+00, 3.55587500e+01, 1.41125000e+00, 1.14375000e+00])  scaler.scale_ array([1.17998294e+01, 2.89636294e+03, 1.12195142e+00, 1.10458632e+00, 1.14044530e+01, 5.78466453e-01, 3.50836055e-01])  Ya sabemos, por convención de scikit-learn, que si un atributo aprende de los datos, su nombre termina con _, como mean_ y scale_ para el StandardScaler.\nEl escalado de los datos se aplica a cada feature de forma individual. Para cada feature, restamos su media y dividimos por su desviación típica. Una vez que hemos llamado al método fit podemos ejecutar la transformación llamando al método transform.\nX_train_scaled = scaler.transform(X_train) X_train_scaled array([[ 3.29708155, 1.19991221, 0.03119565, ..., 2.4061873 , 1.01777726, -0.40973554], [-0.00805096, -0.35962991, -0.86010854, ..., -0.22436411, -0.71093146, -0.40973554], [-1.27925578, -0.73354714, -0.86010854, ..., 1.26628169, -0.71093146, -0.40973554], ..., [ 0.24619 , 0.84360327, 0.92249983, ..., -0.7504744 , 1.01777726, -0.40973554], [-0.77077385, -0.64792251, -0.86010854, ..., -0.92584449, -0.71093146, -0.40973554], [-1.27925578, -0.83850636, 0.03119565, ..., -1.01352954, -0.71093146, -0.40973554]])  Vamos a analizar el mecanismo interno del método transform y lo pondremos en perspectiva con lo que ya vimos con los predictores.\nEl método transform para los transformadores es similar al método predict para los predictores. Usa una función predefinida, llamada función de transformación, y usa los estados del modelo y los datos de entrada. Sin embargo, en lugar de devolver predicciones, el trabajo del método transform es devolver una versión transformada de los datos de entrada.\nPor último, el método fit_transform es un método abreviado para llamar sucesivamente a fity después a transform.\nX_train_scaled = scaler.fit_transform(X_train) X_train_scaled array([[ 3.29708155, 1.19991221, 0.03119565, ..., 2.4061873 , 1.01777726, -0.40973554], [-0.00805096, -0.35962991, -0.86010854, ..., -0.22436411, -0.71093146, -0.40973554], [-1.27925578, -0.73354714, -0.86010854, ..., 1.26628169, -0.71093146, -0.40973554], ..., [ 0.24619 , 0.84360327, 0.92249983, ..., -0.7504744 , 1.01777726, -0.40973554], [-0.77077385, -0.64792251, -0.86010854, ..., -0.92584449, -0.71093146, -0.40973554], [-1.27925578, -0.83850636, 0.03119565, ..., -1.01352954, -0.71093146, -0.40973554]])  X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns) X_train_scaled.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  duration credit_amount installment_commitment residence_since age existing_credits num_dependents     count 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02 8.000000e+02   mean 9.769963e-17 -1.776357e-17 1.465494e-16 2.664535e-17 -3.075318e-16 2.242651e-16 -8.437695e-17   std 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00 1.000626e+00   min -1.448750e+00 -1.073974e+00 -1.751413e+00 -1.671440e+00 -1.451955e+00 -7.109315e-01 -4.097355e-01   25% -7.707739e-01 -6.838296e-01 -8.601085e-01 -7.661239e-01 -7.504744e-01 -7.109315e-01 -4.097355e-01   50% -2.622919e-01 -3.547963e-01 3.119565e-02 1.391924e-01 -2.243641e-01 -7.109315e-01 -4.097355e-01   75% 2.461900e-01 2.740959e-01 9.224998e-01 1.044509e+00 5.648013e-01 1.017777e+00 -4.097355e-01   max 3.297082e+00 5.200792e+00 9.224998e-01 1.044509e+00 3.458408e+00 4.475195e+00 2.440599e+00     Observemos que la media de todas las columnas es cercana a 0 y la desviación típica en todos los casos es cercano a 1. Podemos visualizar el efecto de StandarScaler usando un joinplot para mostrar ambos histogramas de distribución y un scatterplot de cada par de features numéricas al mismo tiempo. Observamos que StandardScaler no cambia la estructura de los datos en si mismos sino que los ejes han sido desplazados y escalados.\nimport matplotlib.pyplot as plt # number of points to visualize to have a clearer plot num_points_to_plot = 300 sns.jointplot(data=X_train[:num_points_to_plot], x=\"age\", y=\"credit_amount\", marginal_kws=dict(bins=15)) plt.suptitle(\"Jointplot de 'age' vs 'credit_amount' \\nantes de StandardScaler\", y=1.1) sns.jointplot(data=X_train_scaled[:num_points_to_plot], x=\"age\", y=\"credit_amount\", marginal_kws=dict(bins=15)) _ = plt.suptitle(\"Jointplot de 'age' vs 'credit_amount' \\ndespués de StandardScaler\", y=1.1) Podemos combinar fácilmente operaciones secuenciales con un pipeline de scikit-learn, que encadena juntas operaciones y se usa como cualquier otro clasificador o regresor. La función make_pipeline creará un Pipeline: toma como argumentos las sucesivas transformaciones a ejecutar, seguido por el modelo clasificador o regresor.\nimport time from sklearn.pipeline import make_pipeline model = make_pipeline(StandardScaler(), LogisticRegression()) model div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}PipelinePipeline(steps=[('standardscaler', StandardScaler()),('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression() La función make_pipeline no requiere que demos un nombre a cada paso. De hecho, se lo asigna automáticamente basado en el nombre de la clase suministrada; un StandardScaler tendrá un paso llamado \"standardscaler\" en el pipeline resultante. Podemos comprobar el nombre de cada paso del modelo:\nmodel.named_steps {'standardscaler': StandardScaler(), 'logisticregression': LogisticRegression()}  Este pipeline predictivo expone los mismos métodos que el predictor final: fit y predict (y adicionalmente predict_proba, decision_function o score).\nstart = time.time() model.fit(X_train, y_train) elapsed_time = time.time() - start Cuando llamamos a model.fit, se llamará al método fit_transform para cada transformador subyacente (en este caso, un único transformador) para:\n aprender sus estados de modelo internos transformar los datos de entrenamiento. Finalmente, los datos preprocesados se suministrarán para entrenar el predictor.  Para predecir los objetivos dado un conjunto de prueba se usa el método predict.\ny_predicted = model.predict(X_test) y_predicted[:5] array(['good', 'good', 'good', 'good', 'good'], dtype=object)  Mostremos el mecanismo subyacente:\nSe llama al método transform de cada transformador (en este caso, un único transformador) para preprocesar los datos. Tengamos en cuenta que no es necesario llamar al método fit de esos transformadores porque estamos usando los estados de modelo internos calculados cuando llamamos a model.fil. Los datos preprocesados son entonces proporcionados al predictor que devolverá los objetivos predichos llamando al método predict.\nComo atajo, podemos comprobar la puntuación del pipeline predictivo completo llamando al método model.score. Por tanto, vamos a verificar el rendimiento computacional y de generalización de este pipeline predictivo.\nmodel_name = model.__class__.__name__ score = model.score(X_test, y_test) print(f\"La precisión usando un {model_name} es {score:.3f} \" f\"con un tiempo de entrenamiento de {elapsed_time:.3f} segundos \" f\"en {model[-1].n_iter_[0]} iteraciones\") La precisión usando un Pipeline es 0.730 con un tiempo de entrenamiento de 0.009 segundos en 10 iteraciones  model = LogisticRegression() start = time.time() model.fit(X_train, y_train) elapsed_time = time.time() - start model_name = model.__class__.__name__ score = model.score(X_test, y_test) print(f\"La precisión usando {model_name} es {score:.3f} \" f\"con un tiempo de entrenamiento de {elapsed_time:.3f} segundos \" f\"en {model.n_iter_[0]} iteraciones\") La precisión usando LogisticRegression es 0.740 con un tiempo de entrenamiento de 0.025 segundos en 88 iteraciones  Vemos que escalar los datos antes de entrenar la regresión logística fue beneficioso en términos de rendimiento computacional. De hecho, el número de iteraciones decrece así como el tiempo de entrenamiento. El rendimiento de generalización no cambió dado que ambos modelos comvergen.\nTrabajar con datos no escalados forzará potencialmente al algoritmo a iterar más como hemos visto. También existe el escenario catastrófico donde el número de iteraciones requeridas sea mayor que el número de iteraciones permitidas por el parámetro del predictor (controlado por max_iter). Por lo tanto, antes de incrementar max_iter, asegurémosnos de que los datos están escalados.\nEvaluación del modelo usando validación cruzada Vamos a discutir algunos aspectos prácticos de evaluar el rendimiento de generalización de nuestro modelo a través de la validación cruzada, en lugar de usar una única división entrenamiento-prueba.\nLa necesidad de validación cruzada Anteriormente dividimos los datos originales en un conjunto de entrenamiento y en un conjunto de pruebas. En general, la puntuación de un modelo dependerá de la forma en que hacemos esta división. Un inconveniente de hacer una única división es que no proporciona ninguna información sobre su variablidad. Otro inconveniente, en una configuración donde la cantidad de datos es pequeña, es que la cantidad de datos disponibles para entrenamiento y prueba será incluso más pequeña después de la división.\nEn su lugar, podemos usar validación cruzada. La validación cruzada consiste en repetir el procedimiento de modo que los conjuntos de entrenamiento y prueba sean diferentes cada vez. Las métricas de rendimiento de generalización se recopilan en cada repetición y luego se agregan. Como resultado, podemos evaluar la variabilidad de nuestra medida del rendimiento de generalización del modelo.\nExisten varias estrategias de validación cruzada, cada una de ellas define cómo repetir el procedimiento de fit / score. En nuestro caso, usaremos la estrategia K-fold: el dataset completo se divide en K particiones. El procesimiento fit / score se repite K veces, donde en cada iteración se usan K-1 particiones para entrenar el modelo y 1 partición para prueba. El siguiente diagrama muestra esta estrategia de K-fold.\nEl diagrama muestra el caso particular de K-fold. Para cada división de validación cruzada, el procedimiento entrena un clon del modelo en todas las instancias rojas y evalúa la puntuación del modelo en las instancias azules.\nLa validación cruzada es, por tanto, computacionalmente intensiva porque requiere entrenar varios modelos, en lugar de uno solo. En scikit-learn, la función cross_validate permite hacer validación cruzada y necesitamos pasar el modelo, los datos y el objetivo. Dado que existen varias estrategias, cross_validate toma un parámetro cv que define la estrategia de división.\n%%time from sklearn.model_selection import cross_validate model = make_pipeline(StandardScaler(), LogisticRegression()) cv_result = cross_validate(model, X, y, cv=5) cv_result Wall time: 50.5 ms {'fit_time': array([0.00750613, 0.00700498, 0.00700617, 0.00750685, 0.00700593]), 'score_time': array([0.002002 , 0.00150108, 0.00200152, 0.00200224, 0.00150156]), 'test_score': array([0.715, 0.71 , 0.69 , 0.715, 0.735])}  La salida de cross_validate es un diccionario de Python, que contiene tres entradas por defecto:\n el tiempo de entrenamiento del modelo en los datos de entrenamiento en cada una de las particiones el tiempo de predicción con el modelo en los datos de prueba en cada una de las particiones la puntuación por defecto en los datos de prueba en cada una de las particiones  Establecer cv=5 crea 5 divisiones distintas para obtener 5 variaciones distintas para los conjuntos de entrenamiento y prueba. Cada conjunto de entrenamiento se usa para entrenar un modelo que después se evalúa en el conjunto de prueba. La estrategia por defecto cuando se establece cv=int es la validación cruzada K-fold, donde K corresponde al número (entero) de divisiones. Establecer cv=5 o cv=10 es una práctica común, ya que mantiene un equilibrio entre el tiempo de cálculo y la estabilidad de la variabilidad estimada.\nHay que tener en cuenta que la función cross-validate, por defecto, descarta los K modelos que se entrenaron en los diferentes subconjuntos superpuestos del dataset. El objetivo de la validación cruzada no es entrenar un modelo sino estimar aproximadamente el rendimiento de generalización de un modelo que se habría entrenado en el conjunto completo de entrenamiento, junto con una estimación de la variabilidad (incertidumbre sobre la precisión de la generalización).\nPodemos pasar parámetros adiciones a sklearn.mode_selection.cross_validate para recopilar información adicional, tales como las puntuaciones de entrenamiento de los modelos obtenidos en cada ciclo o incluso devolver los propios modelos en lugar de descartarlos.\nVamos a extraer las puntuaciones calculadas en las particiones de prueba de cada ciclo de validación cruzada a partir del diccionario cv_result y calcular la precisión media y la variación de la precisión a lo largo de las particiones.\nscores = cv_result[\"test_score\"] print( \"La precisión media de validación cruzada es: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\" ) La precisión media de validación cruzada es: 0.713 +/- 0.014  Hay que tener en cuenta que al calcular la desviación típica de las puntuaciones de validación cruzada, podemos estimar la incertidumbre del rendimiento de generalización de nuestro modelo. Esta es la principal ventaja de la validación cruzada y puede ser crucial en la práctica, por ejemplo cuando comparamos diferentes modelos para averiguar cuál de ellos es mejor que los demás o si nuestras medidas del rendimiento de generalización de cada modelo están dentro de las barras de error de uno u otro.\nCodificación de variables categóricas Bien, hasta ahora hemos visto cómo manejarnos con variables numéricas y codificarlas. Vamos a ver cómo codificar variables categóricas usando codificación ordinal y one-hot.\nYa vimos anteriormente que una variable numérica es una cantidad representada por un número entero o real. Estas variables se manejan de forma natural por los algoritmos de machine learning, que generalmente se componen de una secuencia de instrucciones aritméticas, como sumas y multiplicaciones.\nPor el contrario, las variables categóricas están representadas normalmente por etiquetas de texto (pero no solo) tomadas de entre una lista finita de opciones posibles. Por ejemplo, la variable personal_status de nuestro dataset es una variable categórica porque codifica los datos usando una lista finita de posibles estados:\ncredit[\"personal_status\"].value_counts() 'male single' 548 'female div/dep/mar' 310 'male mar/wid' 92 'male div/sep' 50 Name: personal_status, dtype: int64  ¿Cómo podemos reconocer las columnas categóricas dentro del dataset? Una parte de la respuesta tiene que ver con el tipo de dato de las columnas:\ncredit.dtypes checking_status object duration int64 credit_history object purpose object credit_amount int64 savings_status object employment object installment_commitment int64 personal_status object other_parties object residence_since int64 property_magnitude object age int64 other_payment_plans object housing object existing_credits int64 job object num_dependents int64 own_telephone object foreign_worker object class object dtype: object  Si observamos la columna personal_status podemos comprobar que su tipo de dato es object, lo que significar que contiene valores de texto.\nSeleccionar features en función de su tipo de dato Para seleccionar columnas basadas en su tipo de dato podemos usar la función make_column_selector de scikit-learn, como vimos anteriormente.\ntarget_name = \"class\" y = credit[target_name] data = credit.drop(columns=[target_name]) categorical_columns_selector = selector(dtype_include=object) categorical_columns = categorical_columns_selector(data) categorical_columns ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']  Hemos creado el selector pasándole el tipo de datos que queremos incluir; hemos pasado el dataset de entrada al objeto selector, que devuelve una listado de nombres de columnas que tienen el tipo de datos requerido. Ahora podemos filtrar las columnas que no queremos:\nX = data[categorical_columns] X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status credit_history purpose savings_status employment personal_status other_parties property_magnitude other_payment_plans housing job own_telephone foreign_worker     0 '\u003c0' 'critical/other existing credit' radio/tv 'no known savings' '\u003e=7' 'male single' none 'real estate' none own skilled yes yes   1 '0\u003c=X\u003c200' 'existing paid' radio/tv '\u003c100' '1\u003c=X\u003c4' 'female div/dep/mar' none 'real estate' none own skilled none yes   2 'no checking' 'critical/other existing credit' education '\u003c100' '4\u003c=X\u003c7' 'male single' none 'real estate' none own 'unskilled resident' none yes   3 '\u003c0' 'existing paid' furniture/equipment '\u003c100' '4\u003c=X\u003c7' 'male single' guarantor 'life insurance' none 'for free' skilled none yes   4 '\u003c0' 'delayed previously' 'new car' '\u003c100' '1\u003c=X\u003c4' 'male single' none 'no known property' none 'for free' skilled none yes     Vamos a presentar diferentes estrategias de codificación de datos categóricos a datos numéricos que puedan ser usados por un algoritmo de machine learning.\nEstrategias para codificar categorías Codificando categorías ordinales La estrategia más intuitiva es codificar cada categoría con un número diferente. OrdinalEncoder transforma los datos de esta forma. Empezaremos codificando una única columna para comprender cómo funciona:\nfrom sklearn.preprocessing import OrdinalEncoder personal_status_column = X[[\"personal_status\"]] encoder = OrdinalEncoder() personal_status_encoded = encoder.fit_transform(personal_status_column) personal_status_encoded[:10] array([[3.], [0.], [3.], [3.], [3.], [3.], [3.], [3.], [1.], [2.]])  Vemos que cada categoría de personal_status ha sido reemplazada por un valor numérico. Podemos comprobar el mapeo entre las categorías y los valoras numéricos comprobando el atributo entrenado categories_.\nencoder.categories_ [array([\"'female div/dep/mar'\", \"'male div/sep'\", \"'male mar/wid'\", \"'male single'\"], dtype=object)]  Ahora podemos comprobar la codificación aplicada en todas las categorías.\nX_encoded = encoder.fit_transform(X) X_encoded[:5] array([[1., 1., 7., 4., 3., 3., 2., 2., 1., 1., 3., 1., 1.], [0., 3., 7., 2., 0., 0., 2., 2., 1., 1., 3., 0., 1.], [3., 1., 4., 2., 1., 3., 2., 2., 1., 1., 2., 0., 1.], [1., 3., 5., 2., 1., 3., 1., 0., 1., 0., 3., 0., 1.], [1., 2., 1., 2., 0., 3., 2., 1., 1., 0., 3., 0., 1.]])  Vemos que todas las categorías se han codificado en cada feature (columna) de forma independiente. También podemos comprobar que el número de features antes y después de la codificación sigue siendo el mismo.\nSin embargo, debemos ser cuidadosos cuando apliquemos esta estrategia de codificación: usar esta representación de enteros conduce a los modelos predictivos posteriores a asumir que los valores están ordenados (es decir, 0 \u003c 1 \u003c 2 \u003c 3 …).\nPor defecto, OrdinalEncoder usa una estrategia lexicográfica para mapear etiquetas categóricas de texto a enteros. Esta estrategia es arbitraria y a menudo sin sentido. Por ejemplo, supongamos que el dataset tiene una variable categórica llamada size con categorías como “S”, “M”, “L” y “XL”. Nos gustaría que la representación numérica respetase el significado de los tamaños mapeandolos incrementalmente con enteros, tal que 0, 1, 2, 3. Sin embargo, la estrategia lexicográfica usada por defecto podría mapear las etiquetas “S”, “M”, “L”, “XL” como 2, 1, 0, 3, siguiendo un orden alfabético.\nLa clase OrdinalEncoder acepta un argumento constructor categories para pasar explícitamente las categorías en el orden esperado.\nSi una variable categórica no contiene ninguna información significativa de orden esta codificación podría se engañosa para los modelos estadísticos posteriores y deberíamos considerar una codificación one-hot en su lugar.\nCodificando categorías nominales (sin asumir ningún orden) OneHotEncoder es un codificador alternativo que previene que los modelos posteriores hagan una falsa asunción sobre el orden de las categorías. Para una feature dada, crea tantas columnas como categorías posibles. Para una instancia dada, el valor de la columna correspondiente a la categoría se establecerá a 1 mientras que las columnas de las otras categorías se establecerán a 0.\nEmpecemos codificando una única feature (por ejemplo personal_status):\nfrom sklearn.preprocessing import OneHotEncoder encoder = OneHotEncoder(sparse=False) personal_status_encoded = encoder.fit_transform(personal_status_column) personal_status_encoded array([[0., 0., 0., 1.], [1., 0., 0., 0.], [0., 0., 0., 1.], ..., [0., 0., 0., 1.], [0., 0., 0., 1.], [0., 0., 0., 1.]])  sparse=False se usa en OneHotEncoder a modo didáctico, para tener un visualización más fácil de los datos. Las matrices dispersas son estructuras eficientes de datos donde la mayoría de los elementos de la matriz son ceros.\nVemos que codificar una única columna nos dará una matriz NumPy repleta de ceros y unos. Lo comprenderemos mejor usando los nombres asociados de las features resultado de la transformación.\nfeature_names = encoder.get_feature_names(input_features=[\"personal_status\"]) personal_status_encoded = pd.DataFrame(personal_status_encoded, columns=feature_names) personal_status_encoded  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  personal_status_'female div/dep/mar' personal_status_'male div/sep' personal_status_'male mar/wid' personal_status_'male single'     0 0.0 0.0 0.0 1.0   1 1.0 0.0 0.0 0.0   2 0.0 0.0 0.0 1.0   3 0.0 0.0 0.0 1.0   4 0.0 0.0 0.0 1.0   ... ... ... ... ...   995 1.0 0.0 0.0 0.0   996 0.0 1.0 0.0 0.0   997 0.0 0.0 0.0 1.0   998 0.0 0.0 0.0 1.0   999 0.0 0.0 0.0 1.0    1000 rows × 4 columns\n Como podemos ver, cada categoría se convierte en una columna; la codificación devolvió, para cada ejemplo, un 1 para especificar a qué categoría pertenece. Vamos a aplicarlo a todo el dataset.\nX_encoded = encoder.fit_transform(X) X_encoded[:5] array([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.], [0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.], [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.], [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.]])  columns_encoded = encoder.get_feature_names(X.columns) pd.DataFrame(X_encoded, columns=columns_encoded).head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status_'0\u003c=X\u003c200' checking_status_'\u003c0' checking_status_'\u003e=200' checking_status_'no checking' credit_history_'all paid' credit_history_'critical/other existing credit' credit_history_'delayed previously' credit_history_'existing paid' credit_history_'no credits/all paid' purpose_'domestic appliance' ... housing_own housing_rent job_'high qualif/self emp/mgmt' job_'unemp/unskilled non res' job_'unskilled resident' job_skilled own_telephone_none own_telephone_yes foreign_worker_no foreign_worker_yes     0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 1.0   1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0   2 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 ... 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0   3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0   4 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 1.0    5 rows × 54 columns\n Echemos un vistazo a cómo la variable purpose ha sido codificada y comparémosla con su representación original. El número de features después del codificado es 10 veces mayor que su representación original, debido al elevado número de posibles categorías.\nEligiendo una estrategia de codificación La elección de una estrategia de codificación dependerá de los modelos subyacentes y del tipo de categorías (es decir, ordinales vs nominales). En general, OneHotEncoder es la estrategia usada cuando los modelos posteriores son modelos lineales mientras que OrdinalEncoder es frecuentemente una buena estrategia con modelos basados en árboles.\nUsar un OrdinalEncoder devolverá categorias ordinales. Esto significa que existe un orden en las categorías resultantes (es decir, 0 \u003c 1 \u003c 2). El impacto de violar esta asunción de ordenación realmente depende de los modelos posteriores. Los modelos lineales se verán impactados por categorías desordenadas mientras que los modelos basados en árbol no.\nAun así podemos usar un OrdinalEncoder con modelos lineales pero necesitamos asegurarnos de que:\n las categorías originales (antes de codificar) tienen un orden; las categorías codificadas siguen el mismo orden que las categorías originales.  La codificación One-hot de variables categóricas con una alta cardinalidad pueden provocar ineficiencia computacional en modelos basados en árbol. Debido a esto, no es recomendable usar OneHotEncoder en tales casos, incluso aunque las categorías originales no tengan un orden dado.\nEvaluar nuestro pipeline predictivo Ahora podemos integrar este codificador dentro de un pipeline de machine learning como hicimos con los datos numéricos: entrenemos un clasificador lineal en los datos codificados y verifiquemos el rendimiento de generalización desde este pipeline de machine learning usando validación cruzada.\nAntes de crear el pipeline veamos algunas situaciones que pueden ocurrir en la validación cruzada. Puede ocurrir que algunos valores de una variable categórica sucedan de forma muy infrecuente. Por ejemplo, con el ejemplo anterior de la variable size podemos tener que para la categoría S ocurra 24 veces, M ocurra 25 veces, L ocurra 21 veces y XL ocurra 1 vez. Esto puede ser un problema durante la validación cruzada: si la muestra termina en el conjunto de prueba durante la división, entonces el clasificador no vería esta categoría durante el entrenamiento y no sería capaz de codificarla. En scikit-learn, existen dos soluciones para solventar este problema:\n enumenar todos las posible categorías y proporcionarlas al codificador a través del parámetro categories; usar el parámetro handle_unknown.  Por simplicidad, usaremos esta última solución.\nTengamos en cuenta que OrdinalEncoder también expone un parámetro handle_unknown. Puede ser establecido en use_encoded_value y estableciendo unknown_value para manejar categorías raras.\nAhora podemos crear nuestro pipeline de machine learning.\nmodel = make_pipeline( OneHotEncoder(handle_unknown=\"ignore\"), LogisticRegression(max_iter=500) ) Aquí necesitamos incrementar el número máximo de iteraciones para obtener una covergencia plena de LogisticRegresion y evitar un ConvergenceWarning. Al contrario que las features numéricas, las features categóricas codificadas one-hot tienen todas la misma escala (los valores son 0 o 1), por lo que no obtenemos ningún beneficio del escalado. En este caso, incrementar max_iter es la forma correcta de proceder.\nFinalmente, vamos a comprobar el rendimiento de generalización del modelo usando únicamente las columnas categóricas.\ncv_results = cross_validate(model, X, y) cv_results {'fit_time': array([0.03252769, 0.02702308, 0.02702284, 0.02552271, 0.02652216]), 'score_time': array([0.00300288, 0.00350356, 0.00300264, 0.00300264, 0.00300312]), 'test_score': array([0.74 , 0.75 , 0.765, 0.72 , 0.73 ])}  scores = cv_results[\"test_score\"] print(f\"La precisión es: {scores.mean():.3f} +/- {scores.std():.3f}\") La precisión es: 0.741 +/- 0.016  En este caso, esta representación de las variables categóricas es ligeramente más predictiva del riesgo crediticio que las variables numéricas usadas anteriormente.\nUsando juntas variables numéricas y categóricas Hasta el momento hemos visto el preprocesamiento requerido cuando manejamos variables numéricas y categóricas. Sin embargo, desvinculamos el proceso para tratar cada tipo individualmente. Vamos a mostrar cómo combinar estos pasos de preprocesamiento.\nSelección basada en tipos de datos Repitamos de nuevo el procedimiento para separar las variables categóricas y numéricas según sus tipos de dato:\ntarget_name = \"class\" y = credit[target_name] X = credit.drop(columns=[target_name]) numerical_columns_selector = selector(dtype_exclude=object) categorical_columns_selector = selector(dtype_include=object) numerical_columns = numerical_columns_selector(X) categorical_columns = categorical_columns_selector(X) Atención, en este ejemplo sabemos que el tipo de datos object se usa para representar textos y, por tanto, features categóricas. Tengamos precaución porque esto no es siempre el caso. Algunas veces el tipo de dados object podría contener otros tipos de información, como fechas que no tenían el formato adecuado y, sin embargo, se relacionan a una cantidad de tiempo transcurrido.\nEn un escenario más general, deberíamos inspeccionar manualmente el contenido de nuestro dataframe para no usar equivocadamente make_column_selector.\nEnviar columnas a un procesador específico Ya vimos anteriormente que necesitamos tratar los datos de forma diferente dependiendo de su naturaleza (numérica o categórica). Scikit-learn proporciona una clase ColumnTransformer que enviará columnas específicas a transformadores específicos, haciendo fácil entrenar un único modelo predictivo en un dataset que combina ambos tipos de variables juntas (datos tabulares tipados heterogéneamente).\nEn primer lugar definimos las columnas dependiendo de su tipo de dato:\n La codificación one-hot se aplicará a las columnas categóricas. Además, usaremos handle_unknown=\"ignore\" para solventar el potencial problema debido a categorías raras. El escalado numérico de las features numéricas será estandarizado.  Ahora creamos nuestro ColumnTransformer especificando los tres valores: el nombre del preprocesador, el transformador y las columnas. En primer lugar, vamos a crear los preprocesadores para las partes numéricas y categóricas.\ncategorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\") numerical_preprocessor = StandardScaler() Ahora creamos el transformador y asociamos cada uno de los preprocesadores con sus respectivas columnas.\nfrom sklearn.compose import ColumnTransformer preprocessor = ColumnTransformer([ (\"one-hot-encoder\", categorical_preprocessor, categorical_columns), (\"standard_scaler\", numerical_preprocessor, numerical_columns) ]) Vamos a representar gráficamente la estructura de ColumnTransformer:\nUn ColumnTransformer hace lo siguiente:\n divide las columnas del dataset original basándose en los nombres de las columnas o índices proporcionados. Obtendremos tantos subconjuntos como números de transformadores pasados al ColumnTransformer. transforma cada subconjunto. Se aplica un transformador específico a cada subconjunto: internamente llamará a fit_transform o transform. La salida de este paso es un conjunto de datasets transformados. por último, concatena los datasets transformados en un único dataset.  Lo importante es que ColumnTransformer es como cualquier otro transformador de scikit-learn. Puede ser combinado con un clasificador en un Pipeline:\nmodel = make_pipeline(preprocessor, LogisticRegression(max_iter=500)) set_config(display=\"diagram\") model div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}PipelinePipeline(steps=[('columntransformer', ColumnTransformer(transformers=[('one-hot-encoder', OneHotEncoder(handle_unknown='ignore'),['checking_status','credit_history', 'purpose','savings_status','employment','personal_status','other_parties','property_magnitude','other_payment_plans','housing', 'own_telephone','foreign_worker']),('standard_scaler',StandardScaler(),['duration', 'credit_amount','installment_commitment','residence_since', 'age','existing_credits','num_dependents'])])),('logisticregression', LogisticRegression(max_iter=500))])columntransformer: ColumnTransformerColumnTransformer(transformers=[('one-hot-encoder',OneHotEncoder(handle_unknown='ignore'),['checking_status', 'credit_history','purpose', 'employment','personal_status', 'other_parties','property_magnitude', 'other_payment_plans','housing', 'job', 'own_telephone','foreign_worker']),('standard_scaler', StandardScaler(),['duration', 'credit_amount','installment_commitment', 'residence_since','age', 'existing_credits','num_dependents'])])one-hot-encoder['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']OneHotEncoderOneHotEncoder(handle_unknown='ignore')standard_scaler['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']StandardScalerStandardScaler()LogisticRegressionLogisticRegression(max_iter=500) El modelo final es más complejo que los que hemos visto previamente pero aún sigue la misma API (el mismo conjunto de métodos que pueden ser llamados por el usuario):\n el metodo fit es llamado para preprocesar los datos y luego entrenar el clasificador en los datos preprocesados; el metodo predict hace predicciones en datos nuevos; el metodo score es usado para predecir en los datos de prueba y comparar las predicciones con las etiquetas de prueba esperadas para calcular la precisión.  Empecemos dividiendo nuestros datos en los conjuntos de entrenamiento y prueba:\nX_train, X_test, y_train, y_test = train_test_split( X, y, random_state=42) _ = model.fit(X_train, y_train) Luego, podemos enviar el dataset en bruto directamente al pipeline. De hecho, no necesitamos hacer ningún preprocesamiento manual (llamando a los métodos transform o fit_transform) ya que será manejado cuando llamemos al método predict. Como ejemplo, predeciremos en los primeros cinco ejemplos del conjunto de prueba.\nX_test.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  checking_status duration credit_history purpose credit_amount savings_status employment installment_commitment personal_status other_parties residence_since property_magnitude age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker     521 '\u003c0' 18 'existing paid' radio/tv 3190 '\u003c100' '1\u003c=X\u003c4' 2 'female div/dep/mar' none 2 'real estate' 24 none own 1 skilled 1 none yes   737 '\u003c0' 18 'existing paid' 'new car' 4380 '100\u003c=X\u003c500' '1\u003c=X\u003c4' 3 'male single' none 4 car 35 none own 1 'unskilled resident' 2 yes yes   740 '\u003c0' 24 'all paid' 'new car' 2325 '100\u003c=X\u003c500' '4\u003c=X\u003c7' 2 'male single' none 3 car 32 bank own 1 skilled 1 none yes   660 '\u003e=200' 12 'existing paid' radio/tv 1297 '\u003c100' '1\u003c=X\u003c4' 3 'male mar/wid' none 4 'real estate' 23 none rent 1 skilled 1 none yes   411 'no checking' 33 'critical/other existing credit' 'used car' 7253 '\u003c100' '4\u003c=X\u003c7' 3 'male single' none 2 car 35 none own 2 'high qualif/self emp/mgmt' 1 yes yes     model.predict(X_test)[:5] array(['good', 'bad', 'bad', 'good', 'good'], dtype=object)  y_test[:5] 521 bad 737 good 740 good 660 good 411 good Name: class, dtype: object  Para obtener directamente la puntuación de precisión, necesitamos llamar al método score. Calculemos la puntuación de precisión del conjunto de pruebas completo.\nmodel.score(X_test, y_test) 0.768  Evaluación del modelo con validación cruzada Como vimos anteriormente, un modelo predictivo debe ser evaluado con validación cruzada. Nuestro modelo es utilizable con herramientas de validación cruzada de scikit-learn como cualquier otro predictor:\ncv_results = cross_validate(model, X, y, cv=5) cv_results {'fit_time': array([0.04668927, 0.04341388, 0.04553866, 0.04053569, 0.03930879]), 'score_time': array([0.00600529, 0.00600529, 0.006495 , 0.00568366, 0.00550437]), 'test_score': array([0.75, 0.76, 0.76, 0.74, 0.75])}  scores = cv_results[\"test_score\"] print(\"La precisión media por validación cruzada es: \" f\"{scores.mean():.3f} +/- {scores.std():.3f}\") La precisión media por validación cruzada es: 0.752 +/- 0.007  El modelo compuesto tiene una mayor precisión predicitiva que los dos modelos que utilizan variables numéricas y categóricas por separado.\nResumen A modo de resumen hemos visto:\n cómo crear un modelo predictivo con scikit-learn; la API de scikit-learn para entrenar y probar un modelo predictivo; procesar datos numéricos, en particular usando un Pipeline; procesar datos categóricos, en particular usando OneHotEncoder y OrdinalEncoder; manejar y procesar tipos de datos mixtos (es decir, datos numéricos y categóricos), en particular usando ColumnTransformer.  Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:\n Pipeline predictivo de machine learning con tipos de datos mixtos. Importancia del escalado de features.  ","description":"","tags":["pipeline","regresión logística","validación cruzada","baseline","one-hot encoding","ordinal encoding","ColumnTransformer","escalado"],"title":"Pipeline de modelado predictivo","uri":"/posts/predictive-modeling-pipeline/"},{"categories":["tutoriales"],"content":"Durante el mes de agosto he participado en el evento organizado por Kaggle denominado 30 Days of ML. Las dos primeras semanas consistieron en un repaso a los conceptos básicos de python y machine learning. Las últimas dos semanas participamos en una competición creada para todos los concursantes del evento.\nPara la competición disponíamos de una dataset sintético, pero basado en datos reales. El objetivo era predecir la cantidad de una reclamación del seguro. Las features estaban anonimizadas, pero relacionadas con features del mundo real. Las columnas de features cat0 a cat9 eran categóricas, y las columnas de features cont0 a cont13 continuas.\nNos proporcionan los siguientes archivos:\n train.csv - los datos de entrenamiento con la columna target test.csv - el conjuto de prueba; tendremos que predecir el target para cada una de las filas de este archivo sample_submission.csv - un archivo de envío de ejemplo con el formato correcto  Las semanas previas a la competición, durante el curso de machine learning, trabajamos principalmente con dos modelos:\n Random Forest Uso y optimización de modelos con gradient boosting. En concreto, hacemos uso de la librería XGBoost.  Por tanto, para esta competición seguí las líneas marcadas durantes las semanas de aprendizaje teórico y utilicé ambos modelos. A continuación detallo los pasos seguidos durante los días que trabajé en la competición.\nImportación de librerías necesarias import numpy as np import pandas as pd from sklearn.preprocessing import OrdinalEncoder from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.ensemble import RandomForestRegressor from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error Carga de los datasets # Carga de los datos de entrenamiento y prueba train = pd.read_csv(\"input/train.csv\", index_col=0) test = pd.read_csv(\"input/test.csv\", index_col=0) # Previsualización del dataset train.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13 target   id                          1 B B B C B B A E C N ... 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850 8.113634   2 B B A A B D A F A O ... 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083 8.481233   3 A A A C B D A D A F ... 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846 8.364351   4 B B A C B D A E C K ... 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682 8.049253   6 A A A C B D A E A N ... 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823 7.972260    5 rows × 25 columns\n Preprocesamiento # Separamos el target de las features y = train['target'] features = train.drop(['target'], axis=1) # Previsualización de las features features.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont4 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13   id                          1 B B B C B B A E C N ... 0.610706 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850   2 B B A A B D A F A O ... 0.276853 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083   3 A A A C B D A D A F ... 0.285074 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846   4 B B A C B D A E C K ... 0.284667 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682   6 A A A C B D A E A N ... 0.287595 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823    5 rows × 24 columns\n Seleccionamos y transformamos las variables categóricas a valores numéricos, antes de entrenar y evaluar nuestro modelo. Para ello usamos Ordinal Encoding.\n# Lista de columnas categóricas object_cols = [col for col in features.columns if 'cat' in col] # Aplicamos ordinal-encode a las columnas categóricas X = features.copy() X_test = test.copy() ordinal_encoder = OrdinalEncoder() X[object_cols] = ordinal_encoder.fit_transform(features[object_cols]) X_test[object_cols] = ordinal_encoder.transform(test[object_cols]) # Previsualización de las features con ordinal-encoded X.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 ... cont4 cont5 cont6 cont7 cont8 cont9 cont10 cont11 cont12 cont13   id                          1 1.0 1.0 1.0 2.0 1.0 1.0 0.0 4.0 2.0 13.0 ... 0.610706 0.400361 0.160266 0.310921 0.389470 0.267559 0.237281 0.377873 0.322401 0.869850   2 1.0 1.0 0.0 0.0 1.0 3.0 0.0 5.0 0.0 14.0 ... 0.276853 0.533087 0.558922 0.516294 0.594928 0.341439 0.906013 0.921701 0.261975 0.465083   3 0.0 0.0 0.0 2.0 1.0 3.0 0.0 3.0 0.0 5.0 ... 0.285074 0.650609 0.375348 0.902567 0.555205 0.843531 0.748809 0.620126 0.541474 0.763846   4 1.0 1.0 0.0 2.0 1.0 3.0 0.0 4.0 2.0 10.0 ... 0.284667 0.668980 0.239061 0.732948 0.679618 0.574844 0.346010 0.714610 0.540150 0.280682   6 0.0 0.0 0.0 2.0 1.0 3.0 0.0 4.0 0.0 13.0 ... 0.287595 0.686964 0.420667 0.648182 0.684501 0.956692 1.000773 0.776742 0.625849 0.250823    5 rows × 24 columns\n Extraemos un conjunto de validación a partir de los datos de entrenamiento:\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0) Intento 1 - Entrenamiento de un modelo Random Forest # Definimos el modelo model_rf = RandomForestRegressor(random_state=1, n_jobs=-1) # Entrenamiento del modelo (puede tarder unos minutos en terminar) model_rf.fit(X_train, y_train) preds_valid = model_rf.predict(X_valid) rmse_rf = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_rf) 0.7375392165180452  Bien, ya tenemos nuestro primer resultado. Estoy ansioso por realizar el proceso completo y comprobar mi posición en la clasificación. Así que sin más demora me lanzo a realizar mi primer submit.\n# Usamos el modelo entrenado para realizar predicciones predictions = model_rf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) Cuando enviamos dicho archivo nos indican cuál es la puntuación obtenida (public score). Kaggle calcula dicha puntuación usando solo una parte de los datos de prueba. La puntuación final (private score) se calculará usando el conjunto completo de prueba. La puntuación privada no será visible para nosotros ni para ninguno de los competidores y solo la conoceremos al final de la competición.\nLa puntuación pública obtenida es de 0.73845. Esta puntuación es resultado de entrenar un modelo Random Forest con los hiperparámetros por defecto, por lo tanto, nuestra posición en la clasificación se ubica en la parte baja de la tabla, igualada a la de otros miles de competidores (en total participamos 7.500 equipos). Por tanto, todavía tenemos mucho margen para seguir mejorando.\nIntento 2 - Entrenamiento de un modelo XGBoost Vamos a entrenar unos de los modelos “estrella” en muchas de las competiciones de Kaggle: XGBoost.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1, n_jobs=-1) # Entrenamiento del modelo model_xgb.fit(X_train, y_train) preds_valid = model_xgb.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7268784689736293  Bueno, hemos mejorado ligeramente respecto al uso de Random Forest. Así que como hicimos anteriormente, generamos nuestra predicciones, exportamos nuestro archivo de envío y lo subimos a Kaggle para ver nuestra puntuación.\n# Usamos el modelo entrenado para realizar predicciones predictions = model_xgb.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72613. Son solo unas décimas respecto al envío previo, pero suficientes para escalar a la zona media de la tabla. Seguro que podemos hacerlo mejor… por ejemplo, afinar algunos hiperparámetros. Vamos a ello.\nIntento 3 - Entrenamiento de un modelo XGBoost - Refinamiento usando Grid Search Para este refinamiento, vamos a usar GridSearch para encontrar cuál es la mejor combinación de algunos hiperparámetros.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1) clf = GridSearchCV(model_xgb, {'max_depth': [2, 4, 6], 'n_estimators': [50, 100, 200, 500]}, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=1) clf.fit(X_train, y_train) print(-clf.best_score_) print(clf.best_params_) Fitting 5 folds for each of 12 candidates, totalling 60 fits 0.7201387194775795 {'max_depth': 2, 'n_estimators': 500}  # Entrenamiento del modelo con los mejores parámetros preds_valid = clf.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7221846356377921  Bien, otra ligera mejora. Así que como hicimos anteriormente, generamos nuestra predicciones, exportamos nuestro archivo de envío y lo subimos a Kaggle para ver nuestra puntuación.\n# Usamos el modelo entrenado para realizar predicciones predictions = clf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72181. Igualmente, son solo unas décimas respecto al envío previo, pero suficientes para seguir escalando posiciones. Hemos superado la zona media de la tabla Sigamos afinando algunos hiperparámetros.\nIntento 4 (y último) - Entrenamiento de un modelo XGBoost - Refinamiento usando Grid Search Seguimos ajustando hiperparámetros. Dado que finalmente el mejor valor para n_estimators era 500, lo que representaba el limite superior de la lista proporcionada, vamos a seguir probando más alla de este límite.\n# Definimos el modelo model_xgb = XGBRegressor(random_state=1) clf = GridSearchCV(model_xgb, {'max_depth': [2], 'n_estimators': [500, 1000, 2000, 3000]}, scoring='neg_root_mean_squared_error', verbose=2, n_jobs=1) clf.fit(X_train, y_train) print(-clf.best_score_) print(clf.best_params_) Fitting 5 folds for each of 4 candidates, totalling 20 fits [CV] END ......................max_depth=2, n_estimators=500; total time= 16.7s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.8s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.6s [CV] END ......................max_depth=2, n_estimators=500; total time= 15.7s [CV] END ......................max_depth=2, n_estimators=500; total time= 16.0s [CV] END .....................max_depth=2, n_estimators=1000; total time= 32.1s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.3s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.4s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.2s [CV] END .....................max_depth=2, n_estimators=1000; total time= 31.1s [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.0min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=2000; total time= 1.1min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.6min [CV] END .....................max_depth=2, n_estimators=3000; total time= 1.7min 0.7194840350716621 {'max_depth': 2, 'n_estimators': 1000}  # Entrenamiento del modelo con los mejores parámetros preds_valid = clf.predict(X_valid) rmse_xgb = mean_squared_error(y_valid, preds_valid, squared=False) print(rmse_xgb) 0.7211765635584879  # Usamos el modelo entrenado para realizar predicciones predictions = clf.predict(X_test) # Guardamos la predicciones en un archivo CSV, según las instrucciones de la competición output = pd.DataFrame({'Id': X_test.index, 'target': predictions}) output.to_csv('output/submission.csv', index=False) La puntuación pública obtenida es de 0.72028. Mejoramos ligeramente y subimos posiciones en la clasificación. Finalmente no puedo dedicarle más tiempo (los ciclos de entrenamiento llevan su tiempo) y Kaggle comunica la finalización del evento. Publica las puntuaciones privadas, calculadas sobre la totalidad de los datos de prueba: el score final obtenido es 0.71874. La posición final en la clasificación es 2780 sobre un total de 7572 concursantes. Los diez primeros clasificados se encuentran en una horquilla de 0.71533 a 0.71547.\nEn fin, no está mal. Sin tener más información sobre las features y su significado, podríamos seguir empleando fuerza bruta, potencia de cálculo y tiempo para seguir afinando hiperparámetros con el objetivo de seguir disminuyendo algunas milésimas a la métrica.\n","description":"","tags":["kaggle","regresión"],"title":"30 Days of ML","uri":"/posts/30daysofml/"},{"categories":["tutoriales"],"content":"ANALISIS DE LA CALIDAD DEL VINO - Clasificación multiclase En la primera parte de este análisis enfocamos el problema como aprendizaje supervisado - regresión. El modelo resultante no podemos considerarlo satisfactorio. Vamos a considerar el problema como aprendizaje supervisado - clasificación, concretamente clasificación multiclase.\nCarga de datos Importamos las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.model_selection import RandomizedSearchCV, cross_val_score from sklearn.model_selection import cross_validate, cross_val_predict from sklearn.linear_model import SGDClassifier from sklearn.dummy import DummyClassifier from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier from sklearn import metrics import xgboost as xgb %matplotlib inline import warnings warnings.filterwarnings('ignore') Leemos los datos y creamos un DataFrame\nwine = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Exploración de los datos wine.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     No vamos a profundizar en la exploración de datos, puesto que ya lo hicimos en la primera parte de este análisis (Calidad del vino - Un problema de regresión).\nPreparación de los datos El único preprocesamiento que vamos a realizar es convertir la variable objetivo \"quality\" a categórica.\nwine[\"quality_cat\"] = wine[\"quality\"].astype(\"category\") wine[\"quality_cat\"].value_counts() 5 681 6 638 7 199 4 53 8 18 3 10 Name: quality_cat, dtype: int64  print(f\"Porcentaje de cada una de las puntuaciones de calidad\") wine[\"quality_cat\"].value_counts(normalize=True)*100 Porcentaje de cada una de las puntuaciones de calidad 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality_cat, dtype: float64  Como ya vimos, el dataset se encuentra significativamente desbalanceado. La mayoría de las instancias (82%) tienen puntuaciones de 6 ó 5.\nA continuación creamos el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = wine.columns[:-2] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = wine[predict_columns] y = wine[\"quality_cat\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) Línea base Una pregunta que nos podemos hacer es si está justificado el uso del aprendizaje automático, si nos aporta valor respecto a predecir el azar. Por tanto, lo siguiente que haremos será entrenar un clasificador dummy que utilizaremos como línea base con el que comparar.\nEn primer lugar, entrenaremos un clasificador que genera predicciones uniformemente al azar.\nclf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Predice al azar clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='uniform')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.16108673901331486  Un clasificador que prediga al azar obtiene una puntuación accuracy del 16%.\nProbemos con otro clasificador, pero en este caso, que prediga siempre la clase más frecuente:\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) # Predice siempre la clase más frecuente clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='most_frequent')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.4308052321213254  Un clasificador que siempre prediga la clase más frecuente (en nuestro caso la puntuación de calidad 6) obtiene una accuracy del 43%. Vamos a tomar como línea base la predicción de este clasificador dummy.\npreds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1) Dibujemos su matriz de confusión:\nconf_mx = metrics.confusion_matrix(y_train, preds) conf_mx array([[ 0, 0, 9, 0, 0, 0], [ 0, 0, 43, 0, 0, 0], [ 0, 0, 551, 0, 0, 0], [ 0, 0, 506, 0, 0, 0], [ 0, 0, 157, 0, 0, 0], [ 0, 0, 13, 0, 0, 0]], dtype=int64)  fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=clf_dummy.classes_, yticklabels=clf_dummy.classes_,) accuracy_base = metrics.accuracy_score(y_train, preds) precision_base = metrics.precision_score(y_train, preds, average='weighted', zero_division=0) recall_base = metrics.recall_score(y_train, preds, average='weighted') f1_base = metrics.f1_score(y_train, preds, average='weighted') print(f\"Accuracy: {accuracy_base}\") print(f\"Precision: {precision_base}\") print(f\"Recall: {recall_base}\") print(f\"f1: {f1_base}\") Accuracy: 0.43080531665363564 Precision: 0.18559322085703928 Recall: 0.43080531665363564 f1: 0.25942484095754453  print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.43 1.00 0.60 551 6 0.00 0.00 0.00 506 7 0.00 0.00 0.00 157 8 0.00 0.00 0.00 13 accuracy 0.43 1279 macro avg 0.07 0.17 0.10 1279 weighted avg 0.19 0.43 0.26 1279  Nuestro clasificador dummy es correcto solo el 19% de las veces (precision) y detecta el 43% de las puntuaciones reales (recall). A menudo es conveniente combinar precisión y sensibilidad en una sola métrica llamada puntuación F1, en particular si necesitamos una forma sencilla de comparar dos clasificadores. La puntuación F1 es la media armónica de precisión y sensibilidad. Mientras que la media regular trata a todos los valores por igual, la media armónica otorga mucho más peso a los valores bajos. Como resultado, el clasificador solo obtendrá una puntuación alta en F1 si tanto la sensibilidad como la precisión son altas. En nuestro caso, F1 = 0,26. Bien, tomemos estas tres métricas como nuestra línea base inicial.\nPor tanto, nuestra línea base será:\n Precision: 0.1855 Recall: 0.4308 F1: 0.2594  Entrenamiento de diversos modelos def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = {\"accuracy\": \"accuracy\", \"precision\": \"precision_weighted\", \"recall\": \"recall_weighted\", \"f1\": \"f1_weighted\"} scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring) accuracy, accuracy_std = scores['test_accuracy'].mean(), \\ scores['test_accuracy'].std() precision, precision_std = scores['test_precision'].mean(), \\ scores['test_precision'].std() recall, recall_std = scores['test_recall'].mean(), \\ scores['test_recall'].std() f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std() result = { \"Accuracy\": accuracy, \"Accuracy std\": accuracy_std, \"Precision\": precision, \"Precision std\": precision_std, \"Recall\": recall, \"Recall std\": recall_std, \"f1\": f1, \"f1 std\": f1_std, } if verbose: print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\") print(f\"Precision: {precision} - (std: {precision_std})\") print(f\"Recall: {recall} - (std: {recall_std})\") print(f\"f1: {f1} - (std: {f1_std})\") return result models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed), DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), xgb.XGBClassifier()] model_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\", \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \"Gradient Boosting\", \"XGBoost\"] accuracy = [] precision = [] recall = [] f1 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") clf_scores = evaluate_model(models[model], X_train, y_train) accuracy.append(clf_scores[\"Accuracy\"]) precision.append(clf_scores[\"Precision\"]) recall.append(clf_scores[\"Recall\"]) f1.append(clf_scores[\"f1\"]) Paso 1 de 8 ...running Naive Bayes Gaussian Accuracy: 0.55125 - (std: 0.027102056829233452) Precision: 0.5646348802130249 - (std: 0.020745595731671666) Recall: 0.55125 - (std: 0.027102056829233452) f1: 0.5541082295110215 - (std: 0.023545313928114795) Paso 2 de 8 ...running K Neighbors Classifier Accuracy: 0.4964828431372549 - (std: 0.013777320430796238) Precision: 0.472985448646598 - (std: 0.015072330289309464) Recall: 0.4964828431372549 - (std: 0.013777320430796238) f1: 0.4749703234382818 - (std: 0.01350721905804416) Paso 3 de 8 ...running Random Forest Accuracy: 0.6826194852941176 - (std: 0.03746156433885403) Precision: 0.6585977991402794 - (std: 0.0406774341137893) Recall: 0.6826194852941176 - (std: 0.03746156433885403) f1: 0.6642629277794576 - (std: 0.03850557708999431) Paso 4 de 8 ...running Decision Tree Accuracy: 0.6012714460784314 - (std: 0.028539445741031087) Precision: 0.5978218408820158 - (std: 0.025874687130953537) Recall: 0.6012714460784314 - (std: 0.028539445741031087) f1: 0.5978989958450711 - (std: 0.0264307770802976) Paso 5 de 8 ...running Extra Tree Accuracy: 0.5676348039215686 - (std: 0.032774267548303905) Precision: 0.5697402861119303 - (std: 0.030789932683965727) Recall: 0.5676348039215686 - (std: 0.032774267548303905) f1: 0.5668315018481278 - (std: 0.031722387303563124) Paso 6 de 8 ...running Ada Boost Accuracy: 0.5504748774509804 - (std: 0.03954230035312734) Precision: 0.48457698009594374 - (std: 0.05118366184736229) Recall: 0.5504748774509804 - (std: 0.03954230035312734) f1: 0.5052214324230416 - (std: 0.03764434709325329) Paso 7 de 8 ...running Gradient Boosting Accuracy: 0.6474325980392157 - (std: 0.03472028817662461) Precision: 0.6218203966653049 - (std: 0.03370831758409691) Recall: 0.6474325980392157 - (std: 0.03472028817662461) f1: 0.6328837599218248 - (std: 0.03442412231869498) Paso 8 de 8 ...running XGBoost [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. Accuracy: 0.6560079656862745 - (std: 0.023339659857252816) Precision: 0.6346626310195044 - (std: 0.028312439862179448) Recall: 0.6560079656862745 - (std: 0.023339659857252816) f1: 0.6420686275076488 - (std: 0.024663282704859676)  df_result = pd.DataFrame({\"Model\": model_names, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}) df_result.sort_values(by=\"f1\", ascending=False)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model accuracy precision recall f1     2 Random Forest 0.682619 0.658598 0.682619 0.664263   7 XGBoost 0.656008 0.634663 0.656008 0.642069   6 Gradient Boosting 0.647433 0.621820 0.647433 0.632884   3 Decision Tree 0.601271 0.597822 0.601271 0.597899   4 Extra Tree 0.567635 0.569740 0.567635 0.566832   0 Naive Bayes Gaussian 0.551250 0.564635 0.551250 0.554108   5 Ada Boost 0.550475 0.484577 0.550475 0.505221   1 K Neighbors Classifier 0.496483 0.472985 0.496483 0.474970     Vamos a visualizar la comparativa de los diferentes modelos / métricas:\nmetrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"] for metric in metrics_list: df_result.sort_values(by=metric).plot.barh(\"Model\", metric) plt.title(f\"Model by {metric}\") plt.show() Obtenemos que el modelo que tiene mejor rendimiento es Random Forest. Examinemos un poco más en detalle la ejecución de Random Forest:\nclf_rf = RandomForestClassifier(random_state=seed) preds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1) clf_rf.get_params() {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 7 1 0 0   4 1 0 32 9 1 0   5 0 2 434 108 7 0   6 0 1 116 364 25 0   7 0 0 14 70 73 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.79 0.75 551 6 0.65 0.72 0.68 506 7 0.66 0.46 0.54 157 8 1.00 0.15 0.27 13 accuracy 0.68 1279 macro avg 0.50 0.35 0.37 1279 weighted avg 0.66 0.68 0.66 1279  El modelo es correcto el 66% de las veces (precision) y detecta el 68% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,66. Bueno, ha mejorado significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nEl % de mejora del indicador F1 respecto a la línea base es:\n% diferencia F1= (0.66 - 0.26) / 0.66 * 100 = 60.6%  Realmente la mejora respecto a la línea base es considerable, un 60%. Podemos concluir que está justificado el uso de aprendizaje automático para predecir la puntuación de calidad del vino.\n En general, si el porcentaje de mejora respecto a nuestra línea base no es mayor que un 5% deberíamos reconsiderar el uso de aprendizaje automático.\n Al examinar en detalle el resultado de las predicciones, podemos observar que es pésimo en las puntuaciones extremas (3, 4 y 8) y bastante malo en la puntuación 7.\nAjuste fino de hiperparámetros Vamos a realizar un ajuste de hiperparámetros a ver si se consigue alguna mejora.\nparam_grid = [ {\"n_estimators\": range(20, 200, 20), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) Ajuste inicial con Randomize Search En primer lugar hacemos un barrido rápido aleatorio:\nclf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, scoring=\"f1_weighted\", verbose=2, random_state=seed, n_jobs = -1) clf_random.fit(X_train, y_train) Fitting 5 folds for each of 200 candidates, totalling 1000 fits RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_iter=200, n_jobs=-1, param_distributions=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(20, 200, 20)}], random_state=42, scoring='f1_weighted', verbose=2)  clf_random.best_params_ {'n_estimators': 40, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': False}  preds = cross_val_predict(clf_random.best_estimator_, X_train, y_train, cv=5, n_jobs=-1) print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.81 0.76 551 6 0.66 0.71 0.68 506 7 0.66 0.46 0.55 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.51 0.36 0.38 1279 weighted avg 0.66 0.69 0.67 1279  Ajuste final con GridSearch Proseguimos con un ajuste final usando GridSearch:\nparam_grid = [ {\"n_estimators\": range(130, 200, 10), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) grid_search = GridSearchCV(clf_rf, param_grid, cv=5, scoring=\"f1_weighted\", verbose=2, n_jobs=-1) grid_search.fit(X_train, y_train) Fitting 5 folds for each of 6048 candidates, totalling 30240 fits GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_jobs=-1, param_grid=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(130, 200, 10)}], scoring='f1_weighted', verbose=2)  grid_search.best_params_ {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 170}  final_model = grid_search.best_estimator_ preds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1) pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 6 2 0 0   4 1 0 31 10 1 0   5 0 0 451 94 6 0   6 0 0 113 365 28 0   7 0 0 10 78 69 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.74 0.82 0.78 551 6 0.66 0.72 0.69 506 7 0.63 0.44 0.52 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.50 0.36 0.37 1279 weighted avg 0.67 0.69 0.67 1279  Tras el ajuste de hiperparámetros se consigue una muy ligera mejora respecto a los hiperparámetros por defecto. Es correcto el 67% de las veces (precision) y detecta el 69% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,67. Lo que mejora significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nPor último veamos cómo se ejecuta en el conjunto de prueba:\ny_pred = final_model.predict(X_test) pd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 5 6 7 8   Actual         3 1 0 0 0   4 6 4 0 0   5 101 28 1 0   6 37 89 6 0   7 0 22 19 1   8 0 1 4 0     print(metrics.classification_report(y_test, y_pred, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 1 4 0.00 0.00 0.00 10 5 0.70 0.78 0.73 130 6 0.62 0.67 0.64 132 7 0.63 0.45 0.53 42 8 0.00 0.00 0.00 5 accuracy 0.65 320 macro avg 0.32 0.32 0.32 320 weighted avg 0.62 0.65 0.63 320  Es correcto el 62% de las veces (precision) y detecta el 65% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,65.\naccuracy_best = metrics.accuracy_score(y_test, y_pred) precision_best = metrics.precision_score(y_test, y_pred, average='weighted', zero_division=0) recall_best = metrics.recall_score(y_test, y_pred, average='weighted') f1_best = metrics.f1_score(y_test, y_pred, average='weighted') Matriz de confusión conf_mx = metrics.confusion_matrix(y_test, y_pred) fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=final_model.classes_, yticklabels=final_model.classes_,) Feature importances feature_importances = final_model.feature_importances_ feature_importances array([0.06970454, 0.10304422, 0.07397403, 0.06774786, 0.07530372, 0.06051697, 0.09785917, 0.0830556 , 0.06881937, 0.12760515, 0.17236938])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.17236937962448678, 'alcohol'), (0.12760514906291182, 'sulphates'), (0.10304421805642286, 'volatile acidity'), (0.09785917335424621, 'total sulfur dioxide'), (0.0830555965951595, 'density'), (0.0753037227200391, 'chlorides'), (0.07397402652373279, 'citric acid'), (0.06970454021889655, 'fixed acidity'), (0.06881936733049614, 'pH'), (0.06774786106526597, 'residual sugar'), (0.06051696544834242, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances'); Observamos que las características que más influencia tienen en nuestro modelo son alcohol y sulphates, seguidas por volatile acidity y total sulfur dioxide.\nSelección de características Vamos a usar RFECV para determinar el nº de características válidas con cross-validation.\nfrom sklearn.feature_selection import RFECV from sklearn.model_selection import StratifiedKFold selector = RFECV(final_model, step=1, cv=StratifiedKFold()) selector = selector.fit(X_train, y_train) pd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Support     0 fixed acidity True   1 volatile acidity True   2 citric acid True   3 residual sugar True   4 chlorides True   5 free sulfur dioxide True   6 total sulfur dioxide True   7 density True   8 pH True   9 sulphates True   10 alcohol True     pd.DataFrame({\"Feature\": predict_columns, \"Ranking\": selector.ranking_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Ranking     0 fixed acidity 1   1 volatile acidity 1   2 citric acid 1   3 residual sugar 1   4 chlorides 1   5 free sulfur dioxide 1   6 total sulfur dioxide 1   7 density 1   8 pH 1   9 sulphates 1   10 alcohol 1     # Dibuja el número de features vs la puntuación a través de cross-validation plt.figure() plt.xlabel(\"Nº de features seleccionadas\") plt.ylabel(\"Puntuación cross validation\") plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_) plt.show() Observando la gráfica se concluye que todas las variables son importantes para el modelo, ya que se obtiene la máxima puntuación con las 10 características seleccionadas.\nselector.grid_scores_ array([0.49564951, 0.59737132, 0.65132353, 0.6661826 , 0.6739951 , 0.67869792, 0.67790135, 0.68573223, 0.68025123, 0.68808211, 0.69354167])  Guardado del modelo Por último, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model_clf.joblib\", compress=True) #final_model = joblib.load(\"final_model_clf.joblib\") ['final_model_clf.joblib']  Comentarios finales a los resultados Nuestra línea de base de partida, obtenida a partir de un clasificador que siempre predice la clase más frecuente, es la siguiente:\n Precision: 19% Recall: 43% Accuracy: 43% f1: 0.26  Una vez entrenados diversos modelos, el que mejores resultados ha proporcionados es RandomForest. Después de realizar un ajuste fino de hiperparámetros obtenemos las siguientes métricas:\n Precision: 67% Recall: 69% Accuracy: 69% f1: 0.67  La evaluación en el conjunto de prueba es la siguiente:\n Precision: 62% Recall: 65% Accuracy: 65% f1: 0.63  Al ser multiclase, estamos hablando de puntuaciones ponderadas. Sin embargo, las puntuaciones obtenidas por cada clase son muy dispares. Se puede observar que el resultado es pésimo en las puntuaciones extremas (3, 4 y 8). Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nTodas las variables predictoras son relevantes para el modelo. Las tres que más afectan en la predicción son las siguientes:\n alcohol sulphates volatile acidity.  Podría ser interesante evaluar el modelo segmentando nuestra variable objetivo en rangos de calidad (por ejemplo, baja, media y alta) y comprobar si obtenemos mejores resultados.\n ","description":"","tags":["clasificación","clasificación multiclase","random forest"],"title":"Calidad del vino - Clasificación multiclase","uri":"/posts/wine-quality-clasificacion-multiclase/"},{"categories":["tutoriales"],"content":"En este post repasaremos las principales fases que componen un proyecto de Machine Learning.\nExisten ocho pasos principales:\n  Encuadrar el problema y disponer de la visión global.\n  Obtener los datos.\n  Explorar los datos para obtener ideas.\n  Preparar los datos para exponer lo mejor posible los patrones de datos subyacentes a los algoritmos de Machine Learning.\n  Explorar muchos modelos diferentes y preseleccionar los mejores.\n  Afinar nuestros modelos y combinarlos en una gran solución.\n  Presentar nuestra solución.\n  Implantar, monitorizar y mantener nuestro sistema.\n  Disponemos un conjunto de datos que contiene diversas características de variantes de tinto y blanco del vino portugués “Vinho Verde”. Disponemos de variables químicas, como son la cantidad de alcohol, ácido cítrico, acidez, densidad, pH, etc; así como de una variable sensorial y subjetiva como es la puntuación con la que un grupo de expertos calificaron la calidad del vino: entre 0 (muy malo) y 10 (muy excelente).\nEl objetivo es desarrollar un modelo que pueda predecir la puntuación de calidad dados dichos indicadores bioquímicos.\nLo primero que nos viene a la mente son una serie de preguntas básicas:\n  ¿Cómo se enmarcaría este problema (supervisado, no supervisado, etc.)?\n  ¿Cuál es la variable objetivo? ¿Cuáles son los predictores?\n  ¿Cómo vamos a medir el rendimiento de nuestro modelo?\n  El codigo python utilizado en este artículo está disponible en mi repositorio github\nEn primer lugar importamos todas las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet, Ridge from sklearn.dummy import DummyRegressor from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor from sklearn.svm import SVR from sklearn import metrics %matplotlib inline Obtención de los datos El dataset se encuentra igualmente disponible en Kaggle o en UCI\nPodemos cargarlo directamente desde la url o una vez descargado desde nuestra carpeta data.\nred = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Verificamos el tamaño y el tipo de los datos\nred.shape (1599, 12)  red.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     red.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 1599 entries, 0 to 1598 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 fixed acidity 1599 non-null float64 1 volatile acidity 1599 non-null float64 2 citric acid 1599 non-null float64 3 residual sugar 1599 non-null float64 4 chlorides 1599 non-null float64 5 free sulfur dioxide 1599 non-null float64 6 total sulfur dioxide 1599 non-null float64 7 density 1599 non-null float64 8 pH 1599 non-null float64 9 sulphates 1599 non-null float64 10 alcohol 1599 non-null float64 11 quality 1599 non-null int64 dtypes: float64(11), int64(1) memory usage: 150.0 KB  Realizamos una serie de comprobaciones para conocer la naturaleza de los datos con los que vamos a trabajar: tipo, valores únicos, número de valores únicos y su porcentaje, valores medios y desviación estándar.\npd.DataFrame({\"Type\": red.dtypes, \"Unique\": red.nunique(), \"Null\": red.isnull().sum(), \"Null percent\": red.isnull().sum() / len(red), \"Mean\": red.mean(), \"Std\": red.std()})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Type Unique Null Null percent Mean Std     fixed acidity float64 96 0 0.0 8.319637 1.741096   volatile acidity float64 143 0 0.0 0.527821 0.179060   citric acid float64 80 0 0.0 0.270976 0.194801   residual sugar float64 91 0 0.0 2.538806 1.409928   chlorides float64 153 0 0.0 0.087467 0.047065   free sulfur dioxide float64 60 0 0.0 15.874922 10.460157   total sulfur dioxide float64 144 0 0.0 46.467792 32.895324   density float64 436 0 0.0 0.996747 0.001887   pH float64 89 0 0.0 3.311113 0.154386   sulphates float64 96 0 0.0 0.658149 0.169507   alcohol float64 65 0 0.0 10.422983 1.065668   quality int64 6 0 0.0 5.636023 0.807569     Mmmmm, no existen valores nulos, ¡qué buen dataset!\nred.describe().T  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count mean std min 25% 50% 75% max     fixed acidity 1599.0 8.319637 1.741096 4.60000 7.1000 7.90000 9.200000 15.90000   volatile acidity 1599.0 0.527821 0.179060 0.12000 0.3900 0.52000 0.640000 1.58000   citric acid 1599.0 0.270976 0.194801 0.00000 0.0900 0.26000 0.420000 1.00000   residual sugar 1599.0 2.538806 1.409928 0.90000 1.9000 2.20000 2.600000 15.50000   chlorides 1599.0 0.087467 0.047065 0.01200 0.0700 0.07900 0.090000 0.61100   free sulfur dioxide 1599.0 15.874922 10.460157 1.00000 7.0000 14.00000 21.000000 72.00000   total sulfur dioxide 1599.0 46.467792 32.895324 6.00000 22.0000 38.00000 62.000000 289.00000   density 1599.0 0.996747 0.001887 0.99007 0.9956 0.99675 0.997835 1.00369   pH 1599.0 3.311113 0.154386 2.74000 3.2100 3.31000 3.400000 4.01000   sulphates 1599.0 0.658149 0.169507 0.33000 0.5500 0.62000 0.730000 2.00000   alcohol 1599.0 10.422983 1.065668 8.40000 9.5000 10.20000 11.100000 14.90000   quality 1599.0 5.636023 0.807569 3.00000 5.0000 6.00000 6.000000 8.00000     Exploración de los datos El siguiente paso será realizar un análisis exploratorio de los datos. ¿Cómo se distribuyen las características?\nred.hist(bins=50, figsize=(15,12)); Verifiquemos ahora cómo se distribuye nuestra variable objetivo (la puntuación de calidad):\nprint(f\"Percentage of quality scores\") red[\"quality\"].value_counts(normalize=True) * 100 Percentage of quality scores 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality, dtype: float64  Podemos comprobar que se encuentra significativamente desbalanceada. La mayoría de las instancias (82%) tienen puntuaciones de 5 ó 6.\nVamos a verificar las correlaciones entre las características del dataset:\ncorr_matrix = red.corr() corr_matrix  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     fixed acidity 1.000000 -0.256131 0.671703 0.114777 0.093705 -0.153794 -0.113181 0.668047 -0.682978 0.183006 -0.061668 0.124052   volatile acidity -0.256131 1.000000 -0.552496 0.001918 0.061298 -0.010504 0.076470 0.022026 0.234937 -0.260987 -0.202288 -0.390558   citric acid 0.671703 -0.552496 1.000000 0.143577 0.203823 -0.060978 0.035533 0.364947 -0.541904 0.312770 0.109903 0.226373   residual sugar 0.114777 0.001918 0.143577 1.000000 0.055610 0.187049 0.203028 0.355283 -0.085652 0.005527 0.042075 0.013732   chlorides 0.093705 0.061298 0.203823 0.055610 1.000000 0.005562 0.047400 0.200632 -0.265026 0.371260 -0.221141 -0.128907   free sulfur dioxide -0.153794 -0.010504 -0.060978 0.187049 0.005562 1.000000 0.667666 -0.021946 0.070377 0.051658 -0.069408 -0.050656   total sulfur dioxide -0.113181 0.076470 0.035533 0.203028 0.047400 0.667666 1.000000 0.071269 -0.066495 0.042947 -0.205654 -0.185100   density 0.668047 0.022026 0.364947 0.355283 0.200632 -0.021946 0.071269 1.000000 -0.341699 0.148506 -0.496180 -0.174919   pH -0.682978 0.234937 -0.541904 -0.085652 -0.265026 0.070377 -0.066495 -0.341699 1.000000 -0.196648 0.205633 -0.057731   sulphates 0.183006 -0.260987 0.312770 0.005527 0.371260 0.051658 0.042947 0.148506 -0.196648 1.000000 0.093595 0.251397   alcohol -0.061668 -0.202288 0.109903 0.042075 -0.221141 -0.069408 -0.205654 -0.496180 0.205633 0.093595 1.000000 0.476166   quality 0.124052 -0.390558 0.226373 0.013732 -0.128907 -0.050656 -0.185100 -0.174919 -0.057731 0.251397 0.476166 1.000000     plt.figure(figsize=(15,10)) sns.heatmap(red.corr(), annot=True, cmap='coolwarm') plt.show() Existen correlaciones positivas entre las características:\n fixed acidity con citric acid y densidad, free sulfur dioxide con total sulfur dioxide, alcohol con quality  y correlaciones negativas entre las caracteríticas:\n fixed acidity con pH, volatile acidity con citric acid, citric acid con pH, density con alcohol  Mostremos sólo las correlaciones de la variable objetivo con el resto de características:\ncorr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False) alcohol 0.476166 sulphates 0.251397 citric acid 0.226373 fixed acidity 0.124052 residual sugar 0.013732 free sulfur dioxide -0.050656 pH -0.057731 chlorides -0.128907 density -0.174919 total sulfur dioxide -0.185100 volatile acidity -0.390558 Name: quality, dtype: float64  plt.figure(figsize=(8,5)) corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False).plot(kind='bar') plt.title(\"Attribute correlations with quality\") plt.show() Podemos observar una correlación positiva con el atributo alcohol y negativa con volatile acidity.\nPreparación de los datos En primer lugar vamos a crear el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = red.columns[:-1] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = red[predict_columns] y = red[\"quality\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) X_train.shape, y_train.shape ((1279, 11), (1279,))  X_test.shape, y_test.shape ((320, 11), (320,))  Línea base Para determinar adecuadamente si nuestro modelo es mejor o peor, primero tenemos que definir una línea base con la que poder comparar. Para ello vamos a entrenar algunos regresores dummy cuyos resultados usaremos como línea base de comparación.\nEste regresor dummy predice de manera constante la puntuación 5, la más frecuente:\nrg_dummy = DummyRegressor(strategy=\"constant\", constant=5) rg_dummy.fit(X_train, y_train) DummyRegressor(constant=array(5), strategy='constant')  Nos creamos una función que nos permitirá evaluar nuestro modelo a lo largo de este análisis:\ndef evaluate_model(estimator, X_train, y_train, cv=10, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"] scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring) val_mae_mean, val_mae_std = -scores['test_neg_mean_absolute_error'].mean(), \\ -scores['test_neg_mean_absolute_error'].std() train_mae_mean, train_mae_std = -scores['train_neg_mean_absolute_error'].mean(), \\ -scores['train_neg_mean_absolute_error'].std() val_mse_mean, val_mse_std = -scores['test_neg_mean_squared_error'].mean(), \\ -scores['test_neg_mean_squared_error'].std() train_mse_mean, train_mse_std = -scores['train_neg_mean_squared_error'].mean(), \\ -scores['train_neg_mean_squared_error'].std() val_rmse_mean, val_rmse_std = np.sqrt(-scores['test_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['test_neg_mean_squared_error']).std() train_rmse_mean, train_rmse_std = np.sqrt(-scores['train_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['train_neg_mean_squared_error']).std() val_r2_mean, val_r2_std = scores['test_r2'].mean(), scores['test_r2'].std() train_r2_mean, train_r2_std = scores['train_r2'].mean(), scores['train_r2'].std() result = { \"Val MAE\": val_mae_mean, \"Val MAE std\": val_mae_std, \"Train MAE\": train_mae_mean, \"Train MAE std\": train_mae_std, \"Val MSE\": val_mse_mean, \"Val MSE std\": val_mse_std, \"Train MSE\": train_mse_mean, \"Train MSE std\": train_mse_std, \"Val RMSE\": val_rmse_mean, \"Val RMSE std\": val_rmse_std, \"Train RMSE\": train_rmse_mean, \"Train RMSE std\": train_rmse_std, \"Val R2\": val_r2_mean, \"Val R2 std\": val_r2_std, \"Train R2\": train_rmse_mean, \"Train R2 std\": train_r2_std, } if verbose: print(f\"val_MAE_mean: {val_mae_mean} - (std: {val_mae_std})\") print(f\"train_MAE_mean: {train_mae_mean} - (std: {train_mae_std})\") print(f\"val_MSE_mean: {val_mse_mean} - (std: {val_mse_std})\") print(f\"train_MSE_mean: {train_mse_mean} - (std: {train_mse_std})\") print(f\"val_RMSE_mean: {val_rmse_mean} - (std: {val_rmse_std})\") print(f\"train_RMSE_mean: {train_rmse_mean} - (std: {train_rmse_std})\") print(f\"val_R2_mean: {val_r2_mean} - (std: {val_r2_std})\") print(f\"train_R2_mean: {train_r2_mean} - (std: {train_r2_std})\") return result rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.719365157480315 - (std: -0.06352462970037416) train_MAE_mean: 0.7193126146346173 - (std: -0.007057414168822716) val_MSE_mean: 1.0398868110236221 - (std: -0.12176257291946108) train_MSE_mean: 1.0398750482672072 - (std: -0.01354074583910719) val_RMSE_mean: 1.0180017820772593 - (std: 0.05965888627141756) train_RMSE_mean: 1.0197209977802941 - (std: 0.006643414270421584) val_R2_mean: -0.6192850555554466 - (std: 0.14799333040101653) train_R2_mean: -0.5986022943608599 - (std: 0.01598456942915052)  Un regresor que siempre predice la puntuación de calidad más frecuente (en nuestro caso, la puntuación 5) obtiene un RMSE = 1.01.\nProbemos ahora con un regresor dummy que predice la media de las puntuaciones de calidad:\nrg_dummy = DummyRegressor(strategy=\"mean\") # Mean prediction rg_dummy.fit(X_train, y_train) DummyRegressor()  rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.6842639509806605 - (std: -0.039939453843720794) train_MAE_mean: 0.6836374055181736 - (std: -0.004461928774514038) val_MSE_mean: 0.6515564887161005 - (std: -0.08938937463665708) train_MSE_mean: 0.6505431870574859 - (std: -0.009928873673332832) val_RMSE_mean: 0.8052590895459458 - (std: 0.05580580095057208) train_RMSE_mean: 0.8065390950374436 - (std: 0.006154285796714715) val_R2_mean: -0.007632943779434287 - (std: 0.010684535533448955) train_R2_mean: 0.0 - (std: 0.0)  Un regresor que predice siempre la puntuación media de calidad obtiene un RMSE = 0.80. Vamos a tomar la predicción de este regresor dummy como nuestra línea base.\nEntrenamiento de diversos modelos OK, ya estamos listos para entrenar varios modelos de forma rápida de diferente tipología y usando los parámetros estándar. Seleccionamos algunos modelos de regresión: Linear Regression, Lasso, ElasticNet, Ridge, Extre Trees, y RandomForest.\nmodels = [LinearRegression(), Lasso(alpha=0.1), ElasticNet(), Ridge(), ExtraTreesRegressor(), RandomForestRegressor()] model_names = [\"Lineal Regression\", \"Lasso\", \"ElasticNet\", \"Ridge\", \"Extra Tree\", \"Random Forest\"] mae = [] mse = [] rmse = [] r2 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") rg_scores = evaluate_model(models[model], X_train, y_train) mae.append(rg_scores[\"Val MAE\"]) mse.append(rg_scores[\"Val MSE\"]) rmse.append(rg_scores[\"Val RMSE\"]) r2.append(rg_scores[\"Val R2\"]) Paso 1 de 6 ...running Lineal Regression val_MAE_mean: 0.5054157041773433 - (std: -0.046264972549372924) train_MAE_mean: 0.49951141240221786 - (std: -0.005396834677886112) val_MSE_mean: 0.4363366846653876 - (std: -0.0713599197838867) train_MSE_mean: 0.423559916011364 - (std: -0.007783364048942027) val_RMSE_mean: 0.6578988186927084 - (std: 0.059210041615646476) train_RMSE_mean: 0.6507877560250832 - (std: 0.005934022177307515) val_R2_mean: 0.32302131635332426 - (std: 0.0972958323285871) train_R2_mean: 0.34888336017832816 - (std: 0.008988207786517072) Paso 2 de 6 ...running Lasso val_MAE_mean: 0.5542159398138832 - (std: -0.044044881537899525) train_MAE_mean: 0.551926769360105 - (std: -0.005222359881914205) val_MSE_mean: 0.5011613158962728 - (std: -0.07980261731926688) train_MSE_mean: 0.49648903729654775 - (std: -0.00886434349442919) val_RMSE_mean: 0.7054560563903938 - (std: 0.05910218607112876) train_RMSE_mean: 0.7045920060170998 - (std: 0.006256385006291075) val_R2_mean: 0.22550457016915199 - (std: 0.06858817248045986) train_R2_mean: 0.23679715721911138 - (std: 0.008061051196907644) Paso 3 de 6 ...running ElasticNet val_MAE_mean: 0.6484828644185054 - (std: -0.03858618665902155) train_MAE_mean: 0.6472074434172257 - (std: -0.004861676284701619) val_MSE_mean: 0.6260699925252777 - (std: -0.08837053843631361) train_MSE_mean: 0.6236958050351286 - (std: -0.009753039023728842) val_RMSE_mean: 0.7891968495348196 - (std: 0.056906284447264595) train_RMSE_mean: 0.7897200517246066 - (std: 0.0061680579774354895) val_R2_mean: 0.032300440343033296 - (std: 0.027013749786509673) train_R2_mean: 0.041268269123349036 - (std: 0.0034334107542665303) Paso 4 de 6 ...running Ridge val_MAE_mean: 0.5052017417711606 - (std: -0.04639189777979148) train_MAE_mean: 0.5000120146851917 - (std: -0.00538293390792397) val_MSE_mean: 0.4353611411950837 - (std: -0.07150445371257734) train_MSE_mean: 0.4243933932521361 - (std: -0.007774091981744382) val_RMSE_mean: 0.6571341500690723 - (std: 0.05946301378236467) train_RMSE_mean: 0.6514279204128516 - (std: 0.0059209592739344254) val_R2_mean: 0.32476443307512515 - (std: 0.09605257129964452) train_R2_mean: 0.3476024511130947 - (std: 0.0089301257345918) Paso 5 de 6 ...running Extra Tree val_MAE_mean: 0.3767233021653543 - (std: -0.048411131876621855) train_MAE_mean: -0.0 - (std: -0.0) val_MSE_mean: 0.33849758981299216 - (std: -0.07037684927470149) train_MSE_mean: -0.0 - (std: -0.0) val_RMSE_mean: 0.5784725891678845 - (std: 0.062185636560190514) train_RMSE_mean: 0.0 - (std: 0.0) val_R2_mean: 0.4753582472917177 - (std: 0.09435328966382882) train_R2_mean: 1.0 - (std: 0.0) Paso 6 de 6 ...running Random Forest val_MAE_mean: 0.421939406988189 - (std: -0.03848180259232641) train_MAE_mean: 0.15720688154624 - (std: -0.0024955091475250693) val_MSE_mean: 0.3536394728100393 - (std: -0.06315688035394738) train_MSE_mean: 0.049982221460505356 - (std: -0.0012897300801719821) val_RMSE_mean: 0.5921558699969636 - (std: 0.05468910712544724) train_RMSE_mean: 0.22354850027354933 - (std: 0.0028791467403151403) val_R2_mean: 0.450229047801475 - (std: 0.08970981370698214) train_R2_mean: 0.9231573754360927 - (std: 0.0020715859571618753)  Veamos cuál es el rendimiento de cada uno de ellos:\ndf_result = pd.DataFrame({\"Model\": model_names, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}) df_result  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model MAE MSE RMSE R2     0 Lineal Regression 0.505416 0.436337 0.657899 0.323021   1 Lasso 0.554216 0.501161 0.705456 0.225505   2 ElasticNet 0.648483 0.626070 0.789197 0.032300   3 Ridge 0.505202 0.435361 0.657134 0.324764   4 Extra Tree 0.375012 0.335985 0.576599 0.479648   5 Random Forest 0.422140 0.356897 0.594764 0.445618     df_result.sort_values(by=\"RMSE\", ascending=False).plot.barh(\"Model\", \"RMSE\"); df_result.sort_values(by=\"R2\").plot.barh(\"Model\", \"R2\"); Analizando los resultados vemos que extra trees es el modelo que mejores resultados obtiene. RMSE = 0.576599 and R2 = 0.479648. OK, este será nuestro modelo candidato. Vamos a realizar el ajuste fino.\nFine-Tune param_grid = [ {'n_estimators': range(10, 300, 10), 'max_features': [2, 3, 4, 5, 8, \"auto\"], 'bootstrap': [True, False]} ] xtree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1) grid_search = GridSearchCV(xtree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(X_train, y_train) GridSearchCV(cv=5, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=42), param_grid=[{'bootstrap': [True, False], 'max_features': [2, 3, 4, 5, 8, 'auto'], 'n_estimators': range(10, 300, 10)}], return_train_score=True, scoring='neg_mean_squared_error')  grid_search.best_params_ {'bootstrap': False, 'max_features': 5, 'n_estimators': 160}  ¡Es el momento de la verdad! Veamos su rendimiento en el conjunto de prueba:\nfinal_model = grid_search.best_estimator_ y_pred = final_model.predict(X_test) print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\") print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\") print(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\") print(f\"R2: {final_model.score(X_test, y_test)}\") MAE: 0.38298828124999995 MSE: 0.28038391113281247 RMSE: 0.5295128998738486 R2: 0.5709542506612473  Bueno, ¡un poco mejor! Obtenemos un error de +/- 0.5295.\nPodemos visualizar cómo han sido sus predicciones:\nplt.figure(figsize=(10,8)) plt.scatter(y_test, y_pred, alpha=0.1) plt.xlabel(\"Real\") plt.ylabel(\"Predicted\") plt.show() Se observa una mayor concentración de predicciones en las puntuaciones centrales (5 y 6), debido a un mayor número de instancias en el dataset respecto a las demás. También podemos comprobar que las predicciones sobre las puntuaciones extremas son pésimas. Las puntuaciones 5 y 6 son las que mejores resultados ofrecen.\n¿Cuáles son las características más relevantes?:\nfeature_importances = final_model.feature_importances_ feature_importances array([0.06242878, 0.12054219, 0.07478461, 0.06697772, 0.06670251, 0.05944129, 0.07925392, 0.07148382, 0.06178626, 0.12593217, 0.21066673])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.2106667292131454, 'alcohol'), (0.12593217102849735, 'sulphates'), (0.1205421943281732, 'volatile acidity'), (0.07925392046422035, 'total sulfur dioxide'), (0.07478461494308856, 'citric acid'), (0.07148382305429932, 'density'), (0.06697771630809285, 'residual sugar'), (0.06670250522733821, 'chlorides'), (0.062428775664599805, 'fixed acidity'), (0.061786258397281676, 'pH'), (0.05944129137126337, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances') La gráfica nos muestra que las características más importantes son: alcohol, sulphates y volatile acidity, algo que también nos anticipaba el análisis de correlaciones que vimos anteriormente.\nVeamos ahora cómo se distribuyen los errores:\ndf_resul = pd.DataFrame({\"Pred\": y_pred, \"Real\": y_test, \"error\": y_pred - y_test, \"error_abs\": abs(y_pred - y_test)}) df_resul[\"error\"].plot.hist(bins=40, density=True) plt.title(\"Error distribution\") plt.xlabel(\"Error\"); Parece que los errores siguen una distribución gaussiana.\n¿Cuál es el MAE que se produce en la puntuación de calidad 6?\ndf_resul[df_resul[\"Real\"].isin([6])][\"error\"].abs().mean() 0.3437013037105609  Más en general ¿Cuál es el MAE que se produce en cada puntuación de calidad?\ndf_resul.groupby(\"Real\")[\"error_abs\"].mean() Real 3 2.268966 4 1.286657 5 0.462774 6 0.343701 7 0.617315 8 1.597190 9 3.434483 Name: error_abs, dtype: float64  df_resul.groupby(\"Real\")[\"error_abs\"].mean().plot.bar() plt.title(\"MAE distribution\") plt.ylabel(\"MAE\") plt.xlabel(\"Quality\"); Se comprueba que en las puntuaciones de calidad extremas el error es elevado, sobre todo en la puntuación 8 y 3. Las puntuaciones 5 y 6 es donde menos error se produce.\nGuardado del modelo Como paso final, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model.joblib\", compress=True) ['final_model.joblib']  Conclusiones Después de probar diversos modelos, el que mejores resultados arroja es ExtraTrees. Tras un ajuste fino del mismo conseguimos una importante mejora:\n Nuestra línea base teníamos un MAE: 0.684263 y RMSE: 0.805259. El modelo de Extra Tree obtiene un MAE: 0.382988, RMSE: 0.529512 y R2:0.570954.  Sin embargo, la puntuación de R2 sigue siendo muy baja. Según dicho valor, nuestro modelo apenas puede explicar un 57% de la varianza. Es decir, el porcentaje de relación entre las variables que puede explicarse mediante nuestro modelo lineal es del 57,09%.\n Como sabemos, R2 varía entre 0 y 1. Es la proporción de la varianza en la variable dependiente (nuestra variable objetivo) que es predecible a partir de las variables independientes (nuestros predictores). Si la predicción fuera exactamente igual a lo real, R2 = 1 (es decir, 100%).\n El RMSE = 0,529. Es decir, tenemos un error típico de predicción de 0,529.\nSegún la gráfica de distribución de MAE podemos observar que nuestro modelo no es nada bueno para valores extremos de puntuación. De hecho no es capaz de predecir ninguna puntuación de 3 y apenas alguna de 8. Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nComo ejercicio final, podríamos probar a enfocar el modelado como un problema de clasificación, para evaluar si ofrece mejores resultados que un problema de regresión. Lo veremos en futuros posts.\n","description":"","tags":["regresión","ExtraTrees"],"title":"Calidad del vino - Un problema de regresión","uri":"/posts/wine-quality-un-problema-de-regresion/"},{"categories":["taxonomía"],"content":"Lee Sedol tenía solo 12 años cuando se convirtió en uno de los jugadores profesionales de Go más jóvenes de la historia. Cuando el 9 de marzo de 2016 cruzó las puertas del Hotel Four Seasons de Seúl tenía 33 años y era 18 veces campeón del mundo. Le esperaban cinco intensas partidas contra un duro contrincante. Ante el asombro general perdió 4-1. Ese día pasaría a la historia como el día en que el campeón del mundo de Go perdió contra AlphaGo, un programa informático perteneciente a la división DeepMind de Google. Lee Sedol también pasaría a la historia como el único humano que ha ganado una partida a AlphaGo (aunque posteriormente reconocería que fue debido a un error en su programa).\nGran parte de la magia negra de AlphaGo proviene del uso de técnicas y sistemas de Machine Learning e Inteligencia Artificial. Los sistemas de Machine learning (ML) o aprendizaje automático, están detrás de muchos de los productos de alta tecnología que nos rodean, de los motores de búsqueda de webs, del reconocimiento de habla de nuestros dispositivos, nos recomienda películas y series en nuestras plataformas de streaming favoritas, detecta el spam de nuestros correos, etc.\nPero ¿qué es machine Learning y qué significa que una máquina pueda aprender algo? Según la definición académica de Arthur Samuel, que popularizó dicho término en 1959, machine Learning es el campo de estudio que proporciona a los ordenadores la habilidad de aprender sin ser explícitamente programados. Como informáticos que somos, aquí va otra definición más “ingenieril”: Un programa de ordenador se dice que aprende de una experiencia E con respecto a alguna tarea T y alguna medida de la ejecución P, si su ejecución en T, medida por P, mejora con la experiencia E. (Tom Mitchell, 1997)\nEn vista de esto ¿si nos descargamos una copia de Wikipedia o la Hemeroteca Digital, nuestros ordenadores están aprendiendo algo? Evidentemente no. Dispondremos de una cantidad enorme de datos, pero de repente nuestras máquinas no serán mejores en ninguna tarea.\n¿Qué ventajas ofrece el uso de machine learning sobre otras técnicas de programación tradicionales? Utilizando el caso de uso del spam de correo que mencionamos anteriormente, con un enfoque tradicional haríamos lo siguiente:\n  Observaríamos que en los correos de spam aparecen palabras del tipo “para ti”, “gratis”, “increíble”, etc.\n  Codificaríamos un procedimiento que detectara estas palabras y etiquetaríamos como spam aquellos correos que contuvieran estos patrones.\n  Iteraríamos tantas veces por los dos pasos anteriores para codificar tantas reglas como patrones detectemos.\n  Un enfoque basado en técnicas de machine learning se centraría en aprender qué palabras o frases aparecen con mayor frecuencia en correos etiquetados como spam en comparación con correos “buenos”. Es lo que se denomina “entrenar” nuestro modelo, con el objetivo de que pueda clasificar los nuevos correos que nos lleguen.\nAdemás, supongamos que nuestro inteligente spammer compulsivo detecta que le bloqueamos aquellos correos donde aparece la palabra “gratis” y empieza a sustituirla por la palabra “gratuito”, y así sucesivamente cambiando las reglas. Un enfoque tradicional nos obligaría a estar constantemente cambiando nuestros patrones de detección y haciendo re-entregas. Un enfoque basado en ML detectaría automáticamente estos patrones inusualmente frecuentes en los correos marcados como spam y los marcaría en el futuro sin intervención humana.\nOtro campo donde realmente brilla machine learning es en el reconocimiento de escritura manual (o del habla). Podríamos escribir un programa que detectara determinados trazos o incluso el alfabeto completo, pero esto no escalaría a los miles de combinaciones escritas por millones de personas en el mundo. La mejor forma sería entrenar un modelo de ML proporcionándole muchos ejemplos de diferentes tipos de letras y patrones escritos a mano.\nComo vemos, machine learning es ideal para procesos donde tengamos mucho ajuste manual o un gran número de reglas, soluciones donde haya que adaptarse a nuevos datos, tratamiento de información no estructurada (sonidos, imágenes) y un largo etcétera de casos de uso.\nClasificación de los sistemas de machine learning Existen formas muy diversas de clasificar los sistemas de machine learning. Las más comunes serían las siguientes:\n  Si son entrenados con supervisión humana se pueden clasificar en: supervisados, no supervisados, semisupervisados y aprendizaje por reforzamiento.\n  Si pueden aprender incrementalmente al vuelo: aprendizaje online y aprendizaje por lotes.\n  Aprendizaje basado en instancia (donde los sistemas aprenden ejemplos “de memoria” y después generalizan a nuevos ejemplos usando medidas de similitud) vs aprendizaje basado en modelo (el sistema crea un modelo a partir de ejemplos de entrenamiento que usará posteriormente para realizar predicciones).\n  Esta tipología no es excluyente. Nuestro sistema de spam podría ser un ejemplo de aprendizaje supervisado online basado en modelo si lo entrenamos con una red neuronal.\nVeamos un poco más cerca nuestra primera categorización. Una mañana cualquiera nos acercamos a nuestro “banco amigo” a pedir un préstamo para montar nuestro soñado puesto de castañas. Después de rellenar varios formularios con datos de todo tipo, el director de la sucursal nos convoca para la semana siguiente, donde nos comunicará si nos concede dicho préstamo. ¿Cómo sabe el banco si devolveremos el préstamo? El banco tiene información de otros cientos de miles de operaciones similares a la nuestra y conoce si el cliente devolvió el préstamo o no (es decir, tiene datos etiquetados, aprendizaje supervisado). Con los datos que les hemos proporcionado y con sus modelos de clasificación, el banco puede predecir con un nivel de probabilidad en qué medida seremos capaces de devolver el préstamo. Queda a criterio del director de la sucursal si confiar ciegamente en lo que pronostican dichos modelos.\nEn el aprendizaje no supervisado no disponemos de datos etiquetados, por lo que el sistema debe aprender sin contar con un profesor. Los algoritmos no supervisados son muy útiles para detectar relaciones o agrupaciones entre los datos, algo que a una persona le resultaría muy difícil detectar. Por ejemplo, los modelos detrás de las empresas de venta online pueden detectar que las personas que compran un determinado producto X también suelen comprar el producto Z, por lo que nos los suelen sugerir (“Tal vez le interese…”, “Otros clientes también compraron…”, etc.) durante el proceso de compra. Este tipo de algoritmos no supervisados también se usan para la detección de anomalías (muy útil en la prevención del fraude bancario o en la detección de defectos de fabricación). El sistema está entrenado con ejemplos normales, por lo que es capaz de determinar si una nueva instancia es o no una anomalía.\nAlgunos sistemas de clasificación de imágenes serían un ejemplo de aprendizaje semisupervisado: son capaces de detectar personas y probablemente determinará que la persona X aparece en el siguiente grupo de imágenes. Tan solo hay que ayudarle indicándole quién es esa persona para que a la siguiente ocasión sepa etiquetarla correctamente.\nPor último, el aprendizaje por reforzamiento es un tipo muy diferente a los anteriores. El sistema obtiene recompensas o penalizaciones en función de sus acciones. Debe aprender a partir de ellas, eligiendo cuál sería la mejor estrategia (denominada política) para obtener la mayor recompensa a lo largo del tiempo. AlphaGo sería un ejemplo de aprendizaje por refuerzo. Aprendió su política ganadora estudiando millones de partidas. Durante su combate con el campeón del mundo aplicó las políticas que había aprendido.\nEn posteriores artículos hablaremos también de algunos de los lenguajes más idóneos y la combinación de herramientas que tenemos a nuestra disposición para trabajar de forma inmediata en machine learning: Python, Jupyter Notebook, Scikit-Learn, Tensor-Flow, Keras, etc.\nRevisaremos cuáles son las fases principales de un proyecto típico de machine learning con ejemplos prácticos.\n¡Bienvenidos al mundo de machine learning!\nPor cierto, apenas 3 años después de su derrota por AlphaGo, Lee Sedol se retiró de las competiciones oficiales. Si tenéis oportunidad no dejéis de ver el documental AlphaGo – The movie, que narra la apasionante crónica del combate entre ambas “mentes”.\n","description":"","tags":["aprendizaje automático","machine learning"],"title":"Breve Introducción a Machine Learning","uri":"/posts/breve-introduccion-machine-learning/"}]
