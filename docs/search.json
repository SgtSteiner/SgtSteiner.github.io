[{"categories":["tutoriales"],"content":"ANALISIS DE LA CALIDAD DEL VINO - Clasificación multiclase En la primera parte de este análisis enfocamos el problema como aprendizaje supervisado - regresión. El modelo resultante no podemos considerarlo satisfactorio. Vamos a considerar el problema como aprendizaje supervisado - clasificación, concretamente clasificación multiclase.\nCarga de datos Importamos las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.model_selection import RandomizedSearchCV, cross_val_score from sklearn.model_selection import cross_validate, cross_val_predict from sklearn.linear_model import SGDClassifier from sklearn.dummy import DummyClassifier from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier from sklearn import metrics import xgboost as xgb %matplotlib inline import warnings warnings.filterwarnings('ignore') Leemos los datos y creamos un DataFrame\nwine = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Exploración de los datos wine.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     No vamos a profundizar en la exploración de datos, puesto que ya lo hicimos en la primera parte de este análisis (Calidad del vino - Un problema de regresión).\nPreparación de los datos El único preprocesamiento que vamos a realizar es convertir la variable objetivo \"quality\" a categórica.\nwine[\"quality_cat\"] = wine[\"quality\"].astype(\"category\") wine[\"quality_cat\"].value_counts() 5 681 6 638 7 199 4 53 8 18 3 10 Name: quality_cat, dtype: int64  print(f\"Porcentaje de cada una de las puntuaciones de calidad\") wine[\"quality_cat\"].value_counts(normalize=True)*100 Porcentaje de cada una de las puntuaciones de calidad 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality_cat, dtype: float64  Como ya vimos, el dataset se encuentra significativamente desbalanceado. La mayoría de las instancias (82%) tienen puntuaciones de 6 ó 5.\nA continuación creamos el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = wine.columns[:-2] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = wine[predict_columns] y = wine[\"quality_cat\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) Línea base Una pregunta que nos podemos hacer es si está justificado el uso del aprendizaje automático, si nos aporta valor respecto a predecir el azar. Por tanto, lo siguiente que haremos será entrenar un clasificador dummy que utilizaremos como línea base con el que comparar.\nEn primer lugar, entrenaremos un clasificador que genera predicciones uniformemente al azar.\nclf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Predice al azar clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='uniform')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.16108673901331486  Un clasificador que prediga al azar obtiene una puntuación accuracy del 16%.\nProbemos con otro clasificador, pero en este caso, que prediga siempre la clase más frecuente:\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) # Predice siempre la clase más frecuente clf_dummy.fit(X_train, y_train) DummyClassifier(random_state=42, strategy='most_frequent')  cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean() 0.4308052321213254  Un clasificador que siempre prediga la clase más frecuente (en nuestro caso la puntuación de calidad 6) obtiene una accuracy del 43%. Vamos a tomar como línea base la predicción de este clasificador dummy.\npreds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1) Dibujemos su matriz de confusión:\nconf_mx = metrics.confusion_matrix(y_train, preds) conf_mx array([[ 0, 0, 9, 0, 0, 0], [ 0, 0, 43, 0, 0, 0], [ 0, 0, 551, 0, 0, 0], [ 0, 0, 506, 0, 0, 0], [ 0, 0, 157, 0, 0, 0], [ 0, 0, 13, 0, 0, 0]], dtype=int64)  fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=clf_dummy.classes_, yticklabels=clf_dummy.classes_,) accuracy_base = metrics.accuracy_score(y_train, preds) precision_base = metrics.precision_score(y_train, preds, average='weighted', zero_division=0) recall_base = metrics.recall_score(y_train, preds, average='weighted') f1_base = metrics.f1_score(y_train, preds, average='weighted') print(f\"Accuracy: {accuracy_base}\") print(f\"Precision: {precision_base}\") print(f\"Recall: {recall_base}\") print(f\"f1: {f1_base}\") Accuracy: 0.43080531665363564 Precision: 0.18559322085703928 Recall: 0.43080531665363564 f1: 0.25942484095754453  print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.43 1.00 0.60 551 6 0.00 0.00 0.00 506 7 0.00 0.00 0.00 157 8 0.00 0.00 0.00 13 accuracy 0.43 1279 macro avg 0.07 0.17 0.10 1279 weighted avg 0.19 0.43 0.26 1279  Nuestro clasificador dummy es correcto solo el 19% de las veces (precision) y detecta el 43% de las puntuaciones reales (recall). A menudo es conveniente combinar precisión y sensibilidad en una sola métrica llamada puntuación F1, en particular si necesitamos una forma sencilla de comparar dos clasificadores. La puntuación F1 es la media armónica de precisión y sensibilidad. Mientras que la media regular trata a todos los valores por igual, la media armónica otorga mucho más peso a los valores bajos. Como resultado, el clasificador solo obtendrá una puntuación alta en F1 si tanto la sensibilidad como la precisión son altas. En nuestro caso, F1 = 0,26. Bien, tomemos estas tres métricas como nuestra línea base inicial.\nPor tanto, nuestra línea base será:\n Precision: 0.1855 Recall: 0.4308 F1: 0.2594  Entrenamiento de diversos modelos def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = {\"accuracy\": \"accuracy\", \"precision\": \"precision_weighted\", \"recall\": \"recall_weighted\", \"f1\": \"f1_weighted\"} scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring) accuracy, accuracy_std = scores['test_accuracy'].mean(), \\ scores['test_accuracy'].std() precision, precision_std = scores['test_precision'].mean(), \\ scores['test_precision'].std() recall, recall_std = scores['test_recall'].mean(), \\ scores['test_recall'].std() f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std() result = { \"Accuracy\": accuracy, \"Accuracy std\": accuracy_std, \"Precision\": precision, \"Precision std\": precision_std, \"Recall\": recall, \"Recall std\": recall_std, \"f1\": f1, \"f1 std\": f1_std, } if verbose: print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\") print(f\"Precision: {precision} - (std: {precision_std})\") print(f\"Recall: {recall} - (std: {recall_std})\") print(f\"f1: {f1} - (std: {f1_std})\") return result models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed), DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), xgb.XGBClassifier()] model_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\", \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \"Gradient Boosting\", \"XGBoost\"] accuracy = [] precision = [] recall = [] f1 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") clf_scores = evaluate_model(models[model], X_train, y_train) accuracy.append(clf_scores[\"Accuracy\"]) precision.append(clf_scores[\"Precision\"]) recall.append(clf_scores[\"Recall\"]) f1.append(clf_scores[\"f1\"]) Paso 1 de 8 ...running Naive Bayes Gaussian Accuracy: 0.55125 - (std: 0.027102056829233452) Precision: 0.5646348802130249 - (std: 0.020745595731671666) Recall: 0.55125 - (std: 0.027102056829233452) f1: 0.5541082295110215 - (std: 0.023545313928114795) Paso 2 de 8 ...running K Neighbors Classifier Accuracy: 0.4964828431372549 - (std: 0.013777320430796238) Precision: 0.472985448646598 - (std: 0.015072330289309464) Recall: 0.4964828431372549 - (std: 0.013777320430796238) f1: 0.4749703234382818 - (std: 0.01350721905804416) Paso 3 de 8 ...running Random Forest Accuracy: 0.6826194852941176 - (std: 0.03746156433885403) Precision: 0.6585977991402794 - (std: 0.0406774341137893) Recall: 0.6826194852941176 - (std: 0.03746156433885403) f1: 0.6642629277794576 - (std: 0.03850557708999431) Paso 4 de 8 ...running Decision Tree Accuracy: 0.6012714460784314 - (std: 0.028539445741031087) Precision: 0.5978218408820158 - (std: 0.025874687130953537) Recall: 0.6012714460784314 - (std: 0.028539445741031087) f1: 0.5978989958450711 - (std: 0.0264307770802976) Paso 5 de 8 ...running Extra Tree Accuracy: 0.5676348039215686 - (std: 0.032774267548303905) Precision: 0.5697402861119303 - (std: 0.030789932683965727) Recall: 0.5676348039215686 - (std: 0.032774267548303905) f1: 0.5668315018481278 - (std: 0.031722387303563124) Paso 6 de 8 ...running Ada Boost Accuracy: 0.5504748774509804 - (std: 0.03954230035312734) Precision: 0.48457698009594374 - (std: 0.05118366184736229) Recall: 0.5504748774509804 - (std: 0.03954230035312734) f1: 0.5052214324230416 - (std: 0.03764434709325329) Paso 7 de 8 ...running Gradient Boosting Accuracy: 0.6474325980392157 - (std: 0.03472028817662461) Precision: 0.6218203966653049 - (std: 0.03370831758409691) Recall: 0.6474325980392157 - (std: 0.03472028817662461) f1: 0.6328837599218248 - (std: 0.03442412231869498) Paso 8 de 8 ...running XGBoost [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. [15:57:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior. Accuracy: 0.6560079656862745 - (std: 0.023339659857252816) Precision: 0.6346626310195044 - (std: 0.028312439862179448) Recall: 0.6560079656862745 - (std: 0.023339659857252816) f1: 0.6420686275076488 - (std: 0.024663282704859676)  df_result = pd.DataFrame({\"Model\": model_names, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}) df_result.sort_values(by=\"f1\", ascending=False)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model accuracy precision recall f1     2 Random Forest 0.682619 0.658598 0.682619 0.664263   7 XGBoost 0.656008 0.634663 0.656008 0.642069   6 Gradient Boosting 0.647433 0.621820 0.647433 0.632884   3 Decision Tree 0.601271 0.597822 0.601271 0.597899   4 Extra Tree 0.567635 0.569740 0.567635 0.566832   0 Naive Bayes Gaussian 0.551250 0.564635 0.551250 0.554108   5 Ada Boost 0.550475 0.484577 0.550475 0.505221   1 K Neighbors Classifier 0.496483 0.472985 0.496483 0.474970     Vamos a visualizar la comparativa de los diferentes modelos / métricas:\nmetrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"] for metric in metrics_list: df_result.sort_values(by=metric).plot.barh(\"Model\", metric) plt.title(f\"Model by {metric}\") plt.show() Obtenemos que el modelo que tiene mejor rendimiento es Random Forest. Examinemos un poco más en detalle la ejecución de Random Forest:\nclf_rf = RandomForestClassifier(random_state=seed) preds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1) clf_rf.get_params() {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}  pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 7 1 0 0   4 1 0 32 9 1 0   5 0 2 434 108 7 0   6 0 1 116 364 25 0   7 0 0 14 70 73 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.79 0.75 551 6 0.65 0.72 0.68 506 7 0.66 0.46 0.54 157 8 1.00 0.15 0.27 13 accuracy 0.68 1279 macro avg 0.50 0.35 0.37 1279 weighted avg 0.66 0.68 0.66 1279  El modelo es correcto el 66% de las veces (precision) y detecta el 68% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,66. Bueno, ha mejorado significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nEl % de mejora del indicador F1 respecto a la línea base es:\n% diferencia F1= (0.66 - 0.26) / 0.66 * 100 = 60.6%  Realmente la mejora respecto a la línea base es considerable, un 60%. Podemos concluir que está justificado el uso de aprendizaje automático para predecir la puntuación de calidad del vino.\n En general, si el porcentaje de mejora respecto a nuestra línea base no es mayor que un 5% deberíamos reconsiderar el uso de aprendizaje automático.\n Al examinar en detalle el resultado de las predicciones, podemos observar que es pésimo en las puntuaciones extremas (3, 4 y 8) y bastante malo en la puntuación 7.\nAjuste fino de hiperparámetros Vamos a realizar un ajuste de hiperparámetros a ver si se consigue alguna mejora.\nparam_grid = [ {\"n_estimators\": range(20, 200, 20), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) Ajuste inicial con Randomize Search En primer lugar hacemos un barrido rápido aleatorio:\nclf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, scoring=\"f1_weighted\", verbose=2, random_state=seed, n_jobs = -1) clf_random.fit(X_train, y_train) Fitting 5 folds for each of 200 candidates, totalling 1000 fits RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_iter=200, n_jobs=-1, param_distributions=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(20, 200, 20)}], random_state=42, scoring='f1_weighted', verbose=2)  clf_random.best_params_ {'n_estimators': 40, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy', 'bootstrap': False}  preds = cross_val_predict(clf_random.best_estimator_, X_train, y_train, cv=5, n_jobs=-1) print(metrics.classification_report(y_train, preds, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.72 0.81 0.76 551 6 0.66 0.71 0.68 506 7 0.66 0.46 0.55 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.51 0.36 0.38 1279 weighted avg 0.66 0.69 0.67 1279  Ajuste final con GridSearch Proseguimos con un ajuste final usando GridSearch:\nparam_grid = [ {\"n_estimators\": range(130, 200, 10), \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None], \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"min_samples_split\": [2, 5, 10], \"min_samples_leaf\": [1, 2, 4], } ] clf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1) grid_search = GridSearchCV(clf_rf, param_grid, cv=5, scoring=\"f1_weighted\", verbose=2, n_jobs=-1) grid_search.fit(X_train, y_train) Fitting 5 folds for each of 6048 candidates, totalling 30240 fits GridSearchCV(cv=5, estimator=RandomForestClassifier(n_jobs=-1, random_state=42), n_jobs=-1, param_grid=[{'bootstrap': [True, False], 'criterion': ['gini', 'entropy'], 'max_depth': [2, 4, 6, 8, 10, 12, 14, None], 'max_features': ['auto', 'sqrt', 'log2'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': range(130, 200, 10)}], scoring='f1_weighted', verbose=2)  grid_search.best_params_ {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 170}  final_model = grid_search.best_estimator_ preds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1) pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 3 4 5 6 7 8   Actual           3 0 1 6 2 0 0   4 1 0 31 10 1 0   5 0 0 451 94 6 0   6 0 0 113 365 28 0   7 0 0 10 78 69 0   8 0 0 0 6 5 2     print(metrics.classification_report(y_train, preds))  precision recall f1-score support 3 0.00 0.00 0.00 9 4 0.00 0.00 0.00 43 5 0.74 0.82 0.78 551 6 0.66 0.72 0.69 506 7 0.63 0.44 0.52 157 8 1.00 0.15 0.27 13 accuracy 0.69 1279 macro avg 0.50 0.36 0.37 1279 weighted avg 0.67 0.69 0.67 1279  Tras el ajuste de hiperparámetros se consigue una muy ligera mejora respecto a los hiperparámetros por defecto. Es correcto el 67% de las veces (precision) y detecta el 69% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,67. Lo que mejora significativamente nuestra línea base (recordemos, precision=19%, recall=43% y F1=0,26).\nPor último veamos cómo se ejecuta en el conjunto de prueba:\ny_pred = final_model.predict(X_test) pd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicción'])  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n Predicción 5 6 7 8   Actual         3 1 0 0 0   4 6 4 0 0   5 101 28 1 0   6 37 89 6 0   7 0 22 19 1   8 0 1 4 0     print(metrics.classification_report(y_test, y_pred, zero_division=0))  precision recall f1-score support 3 0.00 0.00 0.00 1 4 0.00 0.00 0.00 10 5 0.70 0.78 0.73 130 6 0.62 0.67 0.64 132 7 0.63 0.45 0.53 42 8 0.00 0.00 0.00 5 accuracy 0.65 320 macro avg 0.32 0.32 0.32 320 weighted avg 0.62 0.65 0.63 320  Es correcto el 62% de las veces (precision) y detecta el 65% de las puntuaciones reales (recall). Siendo la puntuación F1 de 0,65.\naccuracy_best = metrics.accuracy_score(y_test, y_pred) precision_best = metrics.precision_score(y_test, y_pred, average='weighted', zero_division=0) recall_best = metrics.recall_score(y_test, y_pred, average='weighted') f1_best = metrics.f1_score(y_test, y_pred, average='weighted') Matriz de confusión conf_mx = metrics.confusion_matrix(y_test, y_pred) fig = plt.figure(figsize=(8,8)) ax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", xticklabels=final_model.classes_, yticklabels=final_model.classes_,) Feature importances feature_importances = final_model.feature_importances_ feature_importances array([0.06970454, 0.10304422, 0.07397403, 0.06774786, 0.07530372, 0.06051697, 0.09785917, 0.0830556 , 0.06881937, 0.12760515, 0.17236938])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.17236937962448678, 'alcohol'), (0.12760514906291182, 'sulphates'), (0.10304421805642286, 'volatile acidity'), (0.09785917335424621, 'total sulfur dioxide'), (0.0830555965951595, 'density'), (0.0753037227200391, 'chlorides'), (0.07397402652373279, 'citric acid'), (0.06970454021889655, 'fixed acidity'), (0.06881936733049614, 'pH'), (0.06774786106526597, 'residual sugar'), (0.06051696544834242, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances'); Observamos que las características que más influencia tienen en nuestro modelo son alcohol y sulphates, seguidas por volatile acidity y total sulfur dioxide.\nSelección de características Vamos a usar RFECV para determinar el nº de características válidas con cross-validation.\nfrom sklearn.feature_selection import RFECV from sklearn.model_selection import StratifiedKFold selector = RFECV(final_model, step=1, cv=StratifiedKFold()) selector = selector.fit(X_train, y_train) pd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Support     0 fixed acidity True   1 volatile acidity True   2 citric acid True   3 residual sugar True   4 chlorides True   5 free sulfur dioxide True   6 total sulfur dioxide True   7 density True   8 pH True   9 sulphates True   10 alcohol True     pd.DataFrame({\"Feature\": predict_columns, \"Ranking\": selector.ranking_})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Feature Ranking     0 fixed acidity 1   1 volatile acidity 1   2 citric acid 1   3 residual sugar 1   4 chlorides 1   5 free sulfur dioxide 1   6 total sulfur dioxide 1   7 density 1   8 pH 1   9 sulphates 1   10 alcohol 1     # Dibuja el número de features vs la puntuación a través de cross-validation plt.figure() plt.xlabel(\"Nº de features seleccionadas\") plt.ylabel(\"Puntuación cross validation\") plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_) plt.show() Observando la gráfica se concluye que todas las variables son importantes para el modelo, ya que se obtiene la máxima puntuación con las 10 características seleccionadas.\nselector.grid_scores_ array([0.49564951, 0.59737132, 0.65132353, 0.6661826 , 0.6739951 , 0.67869792, 0.67790135, 0.68573223, 0.68025123, 0.68808211, 0.69354167])  Guardado del modelo Por último, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model_clf.joblib\", compress=True) #final_model = joblib.load(\"final_model_clf.joblib\") ['final_model_clf.joblib']  Comentarios finales a los resultados Nuestra línea de base de partida, obtenida a partir de un clasificador que siempre predice la clase más frecuente, es la siguiente:\n Precision: 19% Recall: 43% Accuracy: 43% f1: 0.26  Una vez entrenados diversos modelos, el que mejores resultados ha proporcionados es RandomForest. Después de realizar un ajuste fino de hiperparámetros obtenemos las siguientes métricas:\n Precision: 67% Recall: 69% Accuracy: 69% f1: 0.67  La evaluación en el conjunto de prueba es la siguiente:\n Precision: 62% Recall: 65% Accuracy: 65% f1: 0.63  Al ser multiclase, estamos hablando de puntuaciones ponderadas. Sin embargo, las puntuaciones obtenidas por cada clase son muy dispares. Se puede observar que el resultado es pésimo en las puntuaciones extremas (3, 4 y 8). Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nTodas las variables predictoras son relevantes para el modelo. Las tres que más afectan en la predicción son las siguientes:\n alcohol sulphates volatile acidity.  Podría ser interesante evaluar el modelo segmentando nuestra variable objetivo en rangos de calidad (por ejemplo, baja, media y alta) y comprobar si obtenemos mejores resultados.\n ","description":"","tags":["clasificación","clasificación multiclase","random forest"],"title":"Calidad del vino - Clasificación multiclase","uri":"/posts/wine-quality-clasificacion-multiclase/"},{"categories":["tutoriales"],"content":"En este post repasaremos las principales fases que componen un proyecto de Machine Learning.\nExisten ocho pasos principales:\n  Encuadrar el problema y disponer de la visión global.\n  Obtener los datos.\n  Explorar los datos para obtener ideas.\n  Preparar los datos para exponer lo mejor posible los patrones de datos subyacentes a los algoritmos de Machine Learning.\n  Explorar muchos modelos diferentes y preseleccionar los mejores.\n  Afinar nuestros modelos y combinarlos en una gran solución.\n  Presentar nuestra solución.\n  Implantar, monitorizar y mantener nuestro sistema.\n  Disponemos un conjunto de datos que contiene diversas características de variantes de tinto y blanco del vino portugués “Vinho Verde”. Disponemos de variables químicas, como son la cantidad de alcohol, ácido cítrico, acidez, densidad, pH, etc; así como de una variable sensorial y subjetiva como es la puntuación con la que un grupo de expertos calificaron la calidad del vino: entre 0 (muy malo) y 10 (muy excelente).\nEl objetivo es desarrollar un modelo que pueda predecir la puntuación de calidad dados dichos indicadores bioquímicos.\nLo primero que nos viene a la mente son una serie de preguntas básicas:\n  ¿Cómo se enmarcaría este problema (supervisado, no supervisado, etc.)?\n  ¿Cuál es la variable objetivo? ¿Cuáles son los predictores?\n  ¿Cómo vamos a medir el rendimiento de nuestro modelo?\n  El codigo python utilizado en este artículo está disponible en mi repositorio github\nEn primer lugar importamos todas las librerías necesarias:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate from sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, ElasticNet, Ridge from sklearn.dummy import DummyRegressor from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor from sklearn.svm import SVR from sklearn import metrics %matplotlib inline Obtención de los datos El dataset se encuentra igualmente disponible en Kaggle o en UCI\nPodemos cargarlo directamente desde la url o una vez descargado desde nuestra carpeta data.\nred = pd.read_csv(\"data/wine-quality/winequality-red.csv\") Verificamos el tamaño y el tipo de los datos\nred.shape (1599, 12)  red.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     0 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5   1 7.8 0.88 0.00 2.6 0.098 25.0 67.0 0.9968 3.20 0.68 9.8 5   2 7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.9970 3.26 0.65 9.8 5   3 11.2 0.28 0.56 1.9 0.075 17.0 60.0 0.9980 3.16 0.58 9.8 6   4 7.4 0.70 0.00 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 9.4 5     red.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 1599 entries, 0 to 1598 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 fixed acidity 1599 non-null float64 1 volatile acidity 1599 non-null float64 2 citric acid 1599 non-null float64 3 residual sugar 1599 non-null float64 4 chlorides 1599 non-null float64 5 free sulfur dioxide 1599 non-null float64 6 total sulfur dioxide 1599 non-null float64 7 density 1599 non-null float64 8 pH 1599 non-null float64 9 sulphates 1599 non-null float64 10 alcohol 1599 non-null float64 11 quality 1599 non-null int64 dtypes: float64(11), int64(1) memory usage: 150.0 KB  Realizamos una serie de comprobaciones para conocer la naturaleza de los datos con los que vamos a trabajar: tipo, valores únicos, número de valores únicos y su porcentaje, valores medios y desviación estándar.\npd.DataFrame({\"Type\": red.dtypes, \"Unique\": red.nunique(), \"Null\": red.isnull().sum(), \"Null percent\": red.isnull().sum() / len(red), \"Mean\": red.mean(), \"Std\": red.std()})  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Type Unique Null Null percent Mean Std     fixed acidity float64 96 0 0.0 8.319637 1.741096   volatile acidity float64 143 0 0.0 0.527821 0.179060   citric acid float64 80 0 0.0 0.270976 0.194801   residual sugar float64 91 0 0.0 2.538806 1.409928   chlorides float64 153 0 0.0 0.087467 0.047065   free sulfur dioxide float64 60 0 0.0 15.874922 10.460157   total sulfur dioxide float64 144 0 0.0 46.467792 32.895324   density float64 436 0 0.0 0.996747 0.001887   pH float64 89 0 0.0 3.311113 0.154386   sulphates float64 96 0 0.0 0.658149 0.169507   alcohol float64 65 0 0.0 10.422983 1.065668   quality int64 6 0 0.0 5.636023 0.807569     Mmmmm, no existen valores nulos, ¡qué buen dataset!\nred.describe().T  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count mean std min 25% 50% 75% max     fixed acidity 1599.0 8.319637 1.741096 4.60000 7.1000 7.90000 9.200000 15.90000   volatile acidity 1599.0 0.527821 0.179060 0.12000 0.3900 0.52000 0.640000 1.58000   citric acid 1599.0 0.270976 0.194801 0.00000 0.0900 0.26000 0.420000 1.00000   residual sugar 1599.0 2.538806 1.409928 0.90000 1.9000 2.20000 2.600000 15.50000   chlorides 1599.0 0.087467 0.047065 0.01200 0.0700 0.07900 0.090000 0.61100   free sulfur dioxide 1599.0 15.874922 10.460157 1.00000 7.0000 14.00000 21.000000 72.00000   total sulfur dioxide 1599.0 46.467792 32.895324 6.00000 22.0000 38.00000 62.000000 289.00000   density 1599.0 0.996747 0.001887 0.99007 0.9956 0.99675 0.997835 1.00369   pH 1599.0 3.311113 0.154386 2.74000 3.2100 3.31000 3.400000 4.01000   sulphates 1599.0 0.658149 0.169507 0.33000 0.5500 0.62000 0.730000 2.00000   alcohol 1599.0 10.422983 1.065668 8.40000 9.5000 10.20000 11.100000 14.90000   quality 1599.0 5.636023 0.807569 3.00000 5.0000 6.00000 6.000000 8.00000     Exploración de los datos El siguiente paso será realizar un análisis exploratorio de los datos. ¿Cómo se distribuyen las características?\nred.hist(bins=50, figsize=(15,12)); Verifiquemos ahora cómo se distribuye nuestra variable objetivo (la puntuación de calidad):\nprint(f\"Percentage of quality scores\") red[\"quality\"].value_counts(normalize=True) * 100 Percentage of quality scores 5 42.589118 6 39.899937 7 12.445278 4 3.314572 8 1.125704 3 0.625391 Name: quality, dtype: float64  Podemos comprobar que se encuentra significativamente desbalanceada. La mayoría de las instancias (82%) tienen puntuaciones de 5 ó 6.\nVamos a verificar las correlaciones entre las características del dataset:\ncorr_matrix = red.corr() corr_matrix  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality     fixed acidity 1.000000 -0.256131 0.671703 0.114777 0.093705 -0.153794 -0.113181 0.668047 -0.682978 0.183006 -0.061668 0.124052   volatile acidity -0.256131 1.000000 -0.552496 0.001918 0.061298 -0.010504 0.076470 0.022026 0.234937 -0.260987 -0.202288 -0.390558   citric acid 0.671703 -0.552496 1.000000 0.143577 0.203823 -0.060978 0.035533 0.364947 -0.541904 0.312770 0.109903 0.226373   residual sugar 0.114777 0.001918 0.143577 1.000000 0.055610 0.187049 0.203028 0.355283 -0.085652 0.005527 0.042075 0.013732   chlorides 0.093705 0.061298 0.203823 0.055610 1.000000 0.005562 0.047400 0.200632 -0.265026 0.371260 -0.221141 -0.128907   free sulfur dioxide -0.153794 -0.010504 -0.060978 0.187049 0.005562 1.000000 0.667666 -0.021946 0.070377 0.051658 -0.069408 -0.050656   total sulfur dioxide -0.113181 0.076470 0.035533 0.203028 0.047400 0.667666 1.000000 0.071269 -0.066495 0.042947 -0.205654 -0.185100   density 0.668047 0.022026 0.364947 0.355283 0.200632 -0.021946 0.071269 1.000000 -0.341699 0.148506 -0.496180 -0.174919   pH -0.682978 0.234937 -0.541904 -0.085652 -0.265026 0.070377 -0.066495 -0.341699 1.000000 -0.196648 0.205633 -0.057731   sulphates 0.183006 -0.260987 0.312770 0.005527 0.371260 0.051658 0.042947 0.148506 -0.196648 1.000000 0.093595 0.251397   alcohol -0.061668 -0.202288 0.109903 0.042075 -0.221141 -0.069408 -0.205654 -0.496180 0.205633 0.093595 1.000000 0.476166   quality 0.124052 -0.390558 0.226373 0.013732 -0.128907 -0.050656 -0.185100 -0.174919 -0.057731 0.251397 0.476166 1.000000     plt.figure(figsize=(15,10)) sns.heatmap(red.corr(), annot=True, cmap='coolwarm') plt.show() Existen correlaciones positivas entre las características:\n fixed acidity con citric acid y densidad, free sulfur dioxide con total sulfur dioxide, alcohol con quality  y correlaciones negativas entre las caracteríticas:\n fixed acidity con pH, volatile acidity con citric acid, citric acid con pH, density con alcohol  Mostremos sólo las correlaciones de la variable objetivo con el resto de características:\ncorr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False) alcohol 0.476166 sulphates 0.251397 citric acid 0.226373 fixed acidity 0.124052 residual sugar 0.013732 free sulfur dioxide -0.050656 pH -0.057731 chlorides -0.128907 density -0.174919 total sulfur dioxide -0.185100 volatile acidity -0.390558 Name: quality, dtype: float64  plt.figure(figsize=(8,5)) corr_matrix[\"quality\"].drop(\"quality\").sort_values(ascending=False).plot(kind='bar') plt.title(\"Attribute correlations with quality\") plt.show() Podemos observar una correlación positiva con el atributo alcohol y negativa con volatile acidity.\nPreparación de los datos En primer lugar vamos a crear el conjunto de predictores y el conjunto con la variable objetivo:\npredict_columns = red.columns[:-1] predict_columns Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'], dtype='object')  X = red[predict_columns] y = red[\"quality\"] Posteriormente, creamos los conjuntos de entrenamiento y prueba, siendo el conjunto de entrenamiento un 80% del dataset completo y el 20% restante el conjunto de prueba:\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2) X_train.shape, y_train.shape ((1279, 11), (1279,))  X_test.shape, y_test.shape ((320, 11), (320,))  Línea base Para determinar adecuadamente si nuestro modelo es mejor o peor, primero tenemos que definir una línea base con la que poder comparar. Para ello vamos a entrenar algunos regresores dummy cuyos resultados usaremos como línea base de comparación.\nEste regresor dummy predice de manera constante la puntuación 5, la más frecuente:\nrg_dummy = DummyRegressor(strategy=\"constant\", constant=5) rg_dummy.fit(X_train, y_train) DummyRegressor(constant=array(5), strategy='constant')  Nos creamos una función que nos permitirá evaluar nuestro modelo a lo largo de este análisis:\ndef evaluate_model(estimator, X_train, y_train, cv=10, verbose=True): \"\"\"Print and return cross validation of model \"\"\" scoring = [\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"] scores = cross_validate(estimator, X_train, y_train, return_train_score=True, cv=cv, scoring=scoring) val_mae_mean, val_mae_std = -scores['test_neg_mean_absolute_error'].mean(), \\ -scores['test_neg_mean_absolute_error'].std() train_mae_mean, train_mae_std = -scores['train_neg_mean_absolute_error'].mean(), \\ -scores['train_neg_mean_absolute_error'].std() val_mse_mean, val_mse_std = -scores['test_neg_mean_squared_error'].mean(), \\ -scores['test_neg_mean_squared_error'].std() train_mse_mean, train_mse_std = -scores['train_neg_mean_squared_error'].mean(), \\ -scores['train_neg_mean_squared_error'].std() val_rmse_mean, val_rmse_std = np.sqrt(-scores['test_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['test_neg_mean_squared_error']).std() train_rmse_mean, train_rmse_std = np.sqrt(-scores['train_neg_mean_squared_error']).mean(), \\ np.sqrt(-scores['train_neg_mean_squared_error']).std() val_r2_mean, val_r2_std = scores['test_r2'].mean(), scores['test_r2'].std() train_r2_mean, train_r2_std = scores['train_r2'].mean(), scores['train_r2'].std() result = { \"Val MAE\": val_mae_mean, \"Val MAE std\": val_mae_std, \"Train MAE\": train_mae_mean, \"Train MAE std\": train_mae_std, \"Val MSE\": val_mse_mean, \"Val MSE std\": val_mse_std, \"Train MSE\": train_mse_mean, \"Train MSE std\": train_mse_std, \"Val RMSE\": val_rmse_mean, \"Val RMSE std\": val_rmse_std, \"Train RMSE\": train_rmse_mean, \"Train RMSE std\": train_rmse_std, \"Val R2\": val_r2_mean, \"Val R2 std\": val_r2_std, \"Train R2\": train_rmse_mean, \"Train R2 std\": train_r2_std, } if verbose: print(f\"val_MAE_mean: {val_mae_mean} - (std: {val_mae_std})\") print(f\"train_MAE_mean: {train_mae_mean} - (std: {train_mae_std})\") print(f\"val_MSE_mean: {val_mse_mean} - (std: {val_mse_std})\") print(f\"train_MSE_mean: {train_mse_mean} - (std: {train_mse_std})\") print(f\"val_RMSE_mean: {val_rmse_mean} - (std: {val_rmse_std})\") print(f\"train_RMSE_mean: {train_rmse_mean} - (std: {train_rmse_std})\") print(f\"val_R2_mean: {val_r2_mean} - (std: {val_r2_std})\") print(f\"train_R2_mean: {train_r2_mean} - (std: {train_r2_std})\") return result rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.719365157480315 - (std: -0.06352462970037416) train_MAE_mean: 0.7193126146346173 - (std: -0.007057414168822716) val_MSE_mean: 1.0398868110236221 - (std: -0.12176257291946108) train_MSE_mean: 1.0398750482672072 - (std: -0.01354074583910719) val_RMSE_mean: 1.0180017820772593 - (std: 0.05965888627141756) train_RMSE_mean: 1.0197209977802941 - (std: 0.006643414270421584) val_R2_mean: -0.6192850555554466 - (std: 0.14799333040101653) train_R2_mean: -0.5986022943608599 - (std: 0.01598456942915052)  Un regresor que siempre predice la puntuación de calidad más frecuente (en nuestro caso, la puntuación 5) obtiene un RMSE = 1.01.\nProbemos ahora con un regresor dummy que predice la media de las puntuaciones de calidad:\nrg_dummy = DummyRegressor(strategy=\"mean\") # Mean prediction rg_dummy.fit(X_train, y_train) DummyRegressor()  rg_scores = evaluate_model(rg_dummy, X_train, y_train) val_MAE_mean: 0.6842639509806605 - (std: -0.039939453843720794) train_MAE_mean: 0.6836374055181736 - (std: -0.004461928774514038) val_MSE_mean: 0.6515564887161005 - (std: -0.08938937463665708) train_MSE_mean: 0.6505431870574859 - (std: -0.009928873673332832) val_RMSE_mean: 0.8052590895459458 - (std: 0.05580580095057208) train_RMSE_mean: 0.8065390950374436 - (std: 0.006154285796714715) val_R2_mean: -0.007632943779434287 - (std: 0.010684535533448955) train_R2_mean: 0.0 - (std: 0.0)  Un regresor que predice siempre la puntuación media de calidad obtiene un RMSE = 0.80. Vamos a tomar la predicción de este regresor dummy como nuestra línea base.\nEntrenamiento de diversos modelos OK, ya estamos listos para entrenar varios modelos de forma rápida de diferente tipología y usando los parámetros estándar. Seleccionamos algunos modelos de regresión: Linear Regression, Lasso, ElasticNet, Ridge, Extre Trees, y RandomForest.\nmodels = [LinearRegression(), Lasso(alpha=0.1), ElasticNet(), Ridge(), ExtraTreesRegressor(), RandomForestRegressor()] model_names = [\"Lineal Regression\", \"Lasso\", \"ElasticNet\", \"Ridge\", \"Extra Tree\", \"Random Forest\"] mae = [] mse = [] rmse = [] r2 = [] for model in range(len(models)): print(f\"Paso {model+1} de {len(models)}\") print(f\"...running {model_names[model]}\") rg_scores = evaluate_model(models[model], X_train, y_train) mae.append(rg_scores[\"Val MAE\"]) mse.append(rg_scores[\"Val MSE\"]) rmse.append(rg_scores[\"Val RMSE\"]) r2.append(rg_scores[\"Val R2\"]) Paso 1 de 6 ...running Lineal Regression val_MAE_mean: 0.5054157041773433 - (std: -0.046264972549372924) train_MAE_mean: 0.49951141240221786 - (std: -0.005396834677886112) val_MSE_mean: 0.4363366846653876 - (std: -0.0713599197838867) train_MSE_mean: 0.423559916011364 - (std: -0.007783364048942027) val_RMSE_mean: 0.6578988186927084 - (std: 0.059210041615646476) train_RMSE_mean: 0.6507877560250832 - (std: 0.005934022177307515) val_R2_mean: 0.32302131635332426 - (std: 0.0972958323285871) train_R2_mean: 0.34888336017832816 - (std: 0.008988207786517072) Paso 2 de 6 ...running Lasso val_MAE_mean: 0.5542159398138832 - (std: -0.044044881537899525) train_MAE_mean: 0.551926769360105 - (std: -0.005222359881914205) val_MSE_mean: 0.5011613158962728 - (std: -0.07980261731926688) train_MSE_mean: 0.49648903729654775 - (std: -0.00886434349442919) val_RMSE_mean: 0.7054560563903938 - (std: 0.05910218607112876) train_RMSE_mean: 0.7045920060170998 - (std: 0.006256385006291075) val_R2_mean: 0.22550457016915199 - (std: 0.06858817248045986) train_R2_mean: 0.23679715721911138 - (std: 0.008061051196907644) Paso 3 de 6 ...running ElasticNet val_MAE_mean: 0.6484828644185054 - (std: -0.03858618665902155) train_MAE_mean: 0.6472074434172257 - (std: -0.004861676284701619) val_MSE_mean: 0.6260699925252777 - (std: -0.08837053843631361) train_MSE_mean: 0.6236958050351286 - (std: -0.009753039023728842) val_RMSE_mean: 0.7891968495348196 - (std: 0.056906284447264595) train_RMSE_mean: 0.7897200517246066 - (std: 0.0061680579774354895) val_R2_mean: 0.032300440343033296 - (std: 0.027013749786509673) train_R2_mean: 0.041268269123349036 - (std: 0.0034334107542665303) Paso 4 de 6 ...running Ridge val_MAE_mean: 0.5052017417711606 - (std: -0.04639189777979148) train_MAE_mean: 0.5000120146851917 - (std: -0.00538293390792397) val_MSE_mean: 0.4353611411950837 - (std: -0.07150445371257734) train_MSE_mean: 0.4243933932521361 - (std: -0.007774091981744382) val_RMSE_mean: 0.6571341500690723 - (std: 0.05946301378236467) train_RMSE_mean: 0.6514279204128516 - (std: 0.0059209592739344254) val_R2_mean: 0.32476443307512515 - (std: 0.09605257129964452) train_R2_mean: 0.3476024511130947 - (std: 0.0089301257345918) Paso 5 de 6 ...running Extra Tree val_MAE_mean: 0.3767233021653543 - (std: -0.048411131876621855) train_MAE_mean: -0.0 - (std: -0.0) val_MSE_mean: 0.33849758981299216 - (std: -0.07037684927470149) train_MSE_mean: -0.0 - (std: -0.0) val_RMSE_mean: 0.5784725891678845 - (std: 0.062185636560190514) train_RMSE_mean: 0.0 - (std: 0.0) val_R2_mean: 0.4753582472917177 - (std: 0.09435328966382882) train_R2_mean: 1.0 - (std: 0.0) Paso 6 de 6 ...running Random Forest val_MAE_mean: 0.421939406988189 - (std: -0.03848180259232641) train_MAE_mean: 0.15720688154624 - (std: -0.0024955091475250693) val_MSE_mean: 0.3536394728100393 - (std: -0.06315688035394738) train_MSE_mean: 0.049982221460505356 - (std: -0.0012897300801719821) val_RMSE_mean: 0.5921558699969636 - (std: 0.05468910712544724) train_RMSE_mean: 0.22354850027354933 - (std: 0.0028791467403151403) val_R2_mean: 0.450229047801475 - (std: 0.08970981370698214) train_R2_mean: 0.9231573754360927 - (std: 0.0020715859571618753)  Veamos cuál es el rendimiento de cada uno de ellos:\ndf_result = pd.DataFrame({\"Model\": model_names, \"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}) df_result  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Model MAE MSE RMSE R2     0 Lineal Regression 0.505416 0.436337 0.657899 0.323021   1 Lasso 0.554216 0.501161 0.705456 0.225505   2 ElasticNet 0.648483 0.626070 0.789197 0.032300   3 Ridge 0.505202 0.435361 0.657134 0.324764   4 Extra Tree 0.375012 0.335985 0.576599 0.479648   5 Random Forest 0.422140 0.356897 0.594764 0.445618     df_result.sort_values(by=\"RMSE\", ascending=False).plot.barh(\"Model\", \"RMSE\"); df_result.sort_values(by=\"R2\").plot.barh(\"Model\", \"R2\"); Analizando los resultados vemos que extra trees es el modelo que mejores resultados obtiene. RMSE = 0.576599 and R2 = 0.479648. OK, este será nuestro modelo candidato. Vamos a realizar el ajuste fino.\nFine-Tune param_grid = [ {'n_estimators': range(10, 300, 10), 'max_features': [2, 3, 4, 5, 8, \"auto\"], 'bootstrap': [True, False]} ] xtree_reg = ExtraTreesRegressor(random_state=42, n_jobs=-1) grid_search = GridSearchCV(xtree_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(X_train, y_train) GridSearchCV(cv=5, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=42), param_grid=[{'bootstrap': [True, False], 'max_features': [2, 3, 4, 5, 8, 'auto'], 'n_estimators': range(10, 300, 10)}], return_train_score=True, scoring='neg_mean_squared_error')  grid_search.best_params_ {'bootstrap': False, 'max_features': 5, 'n_estimators': 160}  ¡Es el momento de la verdad! Veamos su rendimiento en el conjunto de prueba:\nfinal_model = grid_search.best_estimator_ y_pred = final_model.predict(X_test) print(f\"MAE: {metrics.mean_absolute_error(y_test, y_pred)}\") print(f\"MSE: {metrics.mean_squared_error(y_test, y_pred)}\") print(f\"RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred))}\") print(f\"R2: {final_model.score(X_test, y_test)}\") MAE: 0.38298828124999995 MSE: 0.28038391113281247 RMSE: 0.5295128998738486 R2: 0.5709542506612473  Bueno, ¡un poco mejor! Obtenemos un error de +/- 0.5295.\nPodemos visualizar cómo han sido sus predicciones:\nplt.figure(figsize=(10,8)) plt.scatter(y_test, y_pred, alpha=0.1) plt.xlabel(\"Real\") plt.ylabel(\"Predicted\") plt.show() Se observa una mayor concentración de predicciones en las puntuaciones centrales (5 y 6), debido a un mayor número de instancias en el dataset respecto a las demás. También podemos comprobar que las predicciones sobre las puntuaciones extremas son pésimas. Las puntuaciones 5 y 6 son las que mejores resultados ofrecen.\n¿Cuáles son las características más relevantes?:\nfeature_importances = final_model.feature_importances_ feature_importances array([0.06242878, 0.12054219, 0.07478461, 0.06697772, 0.06670251, 0.05944129, 0.07925392, 0.07148382, 0.06178626, 0.12593217, 0.21066673])  sorted(zip(feature_importances, X_test.columns), reverse=True) [(0.2106667292131454, 'alcohol'), (0.12593217102849735, 'sulphates'), (0.1205421943281732, 'volatile acidity'), (0.07925392046422035, 'total sulfur dioxide'), (0.07478461494308856, 'citric acid'), (0.07148382305429932, 'density'), (0.06697771630809285, 'residual sugar'), (0.06670250522733821, 'chlorides'), (0.062428775664599805, 'fixed acidity'), (0.061786258397281676, 'pH'), (0.05944129137126337, 'free sulfur dioxide')]  feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False) feature_imp.plot(kind='bar') plt.title('Feature Importances') La gráfica nos muestra que las características más importantes son: alcohol, sulphates y volatile acidity, algo que también nos anticipaba el análisis de correlaciones que vimos anteriormente.\nVeamos ahora cómo se distribuyen los errores:\ndf_resul = pd.DataFrame({\"Pred\": y_pred, \"Real\": y_test, \"error\": y_pred - y_test, \"error_abs\": abs(y_pred - y_test)}) df_resul[\"error\"].plot.hist(bins=40, density=True) plt.title(\"Error distribution\") plt.xlabel(\"Error\"); Parece que los errores siguen una distribución gaussiana.\n¿Cuál es el MAE que se produce en la puntuación de calidad 6?\ndf_resul[df_resul[\"Real\"].isin([6])][\"error\"].abs().mean() 0.3437013037105609  Más en general ¿Cuál es el MAE que se produce en cada puntuación de calidad?\ndf_resul.groupby(\"Real\")[\"error_abs\"].mean() Real 3 2.268966 4 1.286657 5 0.462774 6 0.343701 7 0.617315 8 1.597190 9 3.434483 Name: error_abs, dtype: float64  df_resul.groupby(\"Real\")[\"error_abs\"].mean().plot.bar() plt.title(\"MAE distribution\") plt.ylabel(\"MAE\") plt.xlabel(\"Quality\"); Se comprueba que en las puntuaciones de calidad extremas el error es elevado, sobre todo en la puntuación 8 y 3. Las puntuaciones 5 y 6 es donde menos error se produce.\nGuardado del modelo Como paso final, guardamos nuestro modelo entrenado para futuras predicciones.\nimport joblib joblib.dump(final_model, \"final_model.joblib\", compress=True) ['final_model.joblib']  Conclusiones Después de probar diversos modelos, el que mejores resultados arroja es ExtraTrees. Tras un ajuste fino del mismo conseguimos una importante mejora:\n Nuestra línea base teníamos un MAE: 0.684263 y RMSE: 0.805259. El modelo de Extra Tree obtiene un MAE: 0.382988, RMSE: 0.529512 y R2:0.570954.  Sin embargo, la puntuación de R2 sigue siendo muy baja. Según dicho valor, nuestro modelo apenas puede explicar un 57% de la varianza. Es decir, el porcentaje de relación entre las variables que puede explicarse mediante nuestro modelo lineal es del 57,09%.\n Como sabemos, R2 varía entre 0 y 1. Es la proporción de la varianza en la variable dependiente (nuestra variable objetivo) que es predecible a partir de las variables independientes (nuestros predictores). Si la predicción fuera exactamente igual a lo real, R2 = 1 (es decir, 100%).\n El RMSE = 0,529. Es decir, tenemos un error típico de predicción de 0,529.\nSegún la gráfica de distribución de MAE podemos observar que nuestro modelo no es nada bueno para valores extremos de puntuación. De hecho no es capaz de predecir ninguna puntuación de 3 y apenas alguna de 8. Según vimos en la distribución de la variable objetivo, ésta se encuentra muy desbalanceada, apenas existen observaciones para los valores extremos, por lo que el modelo no tiene suficientes datos de entrenamiento para todas las puntuaciones de calidad.\nComo ejercicio final, podríamos probar a enfocar el modelado como un problema de clasificación, para evaluar si ofrece mejores resultados que un problema de regresión. Lo veremos en futuros posts.\n","description":"","tags":["regresión","ExtraTrees"],"title":"Calidad del vino - Un problema de regresión","uri":"/posts/wine-quality-un-problema-de-regresion/"},{"categories":["taxonomía"],"content":"Lee Sedol tenía solo 12 años cuando se convirtió en uno de los jugadores profesionales de Go más jóvenes de la historia. Cuando el 9 de marzo de 2016 cruzó las puertas del Hotel Four Seasons de Seúl tenía 33 años y era 18 veces campeón del mundo. Le esperaban cinco intensas partidas contra un duro contrincante. Ante el asombro general perdió 4-1. Ese día pasaría a la historia como el día en que el campeón del mundo de Go perdió contra AlphaGo, un programa informático perteneciente a la división DeepMind de Google. Lee Sedol también pasaría a la historia como el único humano que ha ganado una partida a AlphaGo (aunque posteriormente reconocería que fue debido a un error en su programa).\nGran parte de la magia negra de AlphaGo proviene del uso de técnicas y sistemas de Machine Learning e Inteligencia Artificial. Los sistemas de Machine learning (ML) o aprendizaje automático, están detrás de muchos de los productos de alta tecnología que nos rodean, de los motores de búsqueda de webs, del reconocimiento de habla de nuestros dispositivos, nos recomienda películas y series en nuestras plataformas de streaming favoritas, detecta el spam de nuestros correos, etc.\nPero ¿qué es machine Learning y qué significa que una máquina pueda aprender algo? Según la definición académica de Arthur Samuel, que popularizó dicho término en 1959, machine Learning es el campo de estudio que proporciona a los ordenadores la habilidad de aprender sin ser explícitamente programados. Como informáticos que somos, aquí va otra definición más “ingenieril”: Un programa de ordenador se dice que aprende de una experiencia E con respecto a alguna tarea T y alguna medida de la ejecución P, si su ejecución en T, medida por P, mejora con la experiencia E. (Tom Mitchell, 1997)\nEn vista de esto ¿si nos descargamos una copia de Wikipedia o la Hemeroteca Digital, nuestros ordenadores están aprendiendo algo? Evidentemente no. Dispondremos de una cantidad enorme de datos, pero de repente nuestras máquinas no serán mejores en ninguna tarea.\n¿Qué ventajas ofrece el uso de machine learning sobre otras técnicas de programación tradicionales? Utilizando el caso de uso del spam de correo que mencionamos anteriormente, con un enfoque tradicional haríamos lo siguiente:\n  Observaríamos que en los correos de spam aparecen palabras del tipo “para ti”, “gratis”, “increíble”, etc.\n  Codificaríamos un procedimiento que detectara estas palabras y etiquetaríamos como spam aquellos correos que contuvieran estos patrones.\n  Iteraríamos tantas veces por los dos pasos anteriores para codificar tantas reglas como patrones detectemos.\n  Un enfoque basado en técnicas de machine learning se centraría en aprender qué palabras o frases aparecen con mayor frecuencia en correos etiquetados como spam en comparación con correos “buenos”. Es lo que se denomina “entrenar” nuestro modelo, con el objetivo de que pueda clasificar los nuevos correos que nos lleguen.\nAdemás, supongamos que nuestro inteligente spammer compulsivo detecta que le bloqueamos aquellos correos donde aparece la palabra “gratis” y empieza a sustituirla por la palabra “gratuito”, y así sucesivamente cambiando las reglas. Un enfoque tradicional nos obligaría a estar constantemente cambiando nuestros patrones de detección y haciendo re-entregas. Un enfoque basado en ML detectaría automáticamente estos patrones inusualmente frecuentes en los correos marcados como spam y los marcaría en el futuro sin intervención humana.\nOtro campo donde realmente brilla machine learning es en el reconocimiento de escritura manual (o del habla). Podríamos escribir un programa que detectara determinados trazos o incluso el alfabeto completo, pero esto no escalaría a los miles de combinaciones escritas por millones de personas en el mundo. La mejor forma sería entrenar un modelo de ML proporcionándole muchos ejemplos de diferentes tipos de letras y patrones escritos a mano.\nComo vemos, machine learning es ideal para procesos donde tengamos mucho ajuste manual o un gran número de reglas, soluciones donde haya que adaptarse a nuevos datos, tratamiento de información no estructurada (sonidos, imágenes) y un largo etcétera de casos de uso.\nClasificación de los sistemas de machine learning Existen formas muy diversas de clasificar los sistemas de machine learning. Las más comunes serían las siguientes:\n  Si son entrenados con supervisión humana se pueden clasificar en: supervisados, no supervisados, semisupervisados y aprendizaje por reforzamiento.\n  Si pueden aprender incrementalmente al vuelo: aprendizaje online y aprendizaje por lotes.\n  Aprendizaje basado en instancia (donde los sistemas aprenden ejemplos “de memoria” y después generalizan a nuevos ejemplos usando medidas de similitud) vs aprendizaje basado en modelo (el sistema crea un modelo a partir de ejemplos de entrenamiento que usará posteriormente para realizar predicciones).\n  Esta tipología no es excluyente. Nuestro sistema de spam podría ser un ejemplo de aprendizaje supervisado online basado en modelo si lo entrenamos con una red neuronal.\nVeamos un poco más cerca nuestra primera categorización. Una mañana cualquiera nos acercamos a nuestro “banco amigo” a pedir un préstamo para montar nuestro soñado puesto de castañas. Después de rellenar varios formularios con datos de todo tipo, el director de la sucursal nos convoca para la semana siguiente, donde nos comunicará si nos concede dicho préstamo. ¿Cómo sabe el banco si devolveremos el préstamo? El banco tiene información de otros cientos de miles de operaciones similares a la nuestra y conoce si el cliente devolvió el préstamo o no (es decir, tiene datos etiquetados, aprendizaje supervisado). Con los datos que les hemos proporcionado y con sus modelos de clasificación, el banco puede predecir con un nivel de probabilidad en qué medida seremos capaces de devolver el préstamo. Queda a criterio del director de la sucursal si confiar ciegamente en lo que pronostican dichos modelos.\nEn el aprendizaje no supervisado no disponemos de datos etiquetados, por lo que el sistema debe aprender sin contar con un profesor. Los algoritmos no supervisados son muy útiles para detectar relaciones o agrupaciones entre los datos, algo que a una persona le resultaría muy difícil detectar. Por ejemplo, los modelos detrás de las empresas de venta online pueden detectar que las personas que compran un determinado producto X también suelen comprar el producto Z, por lo que nos los suelen sugerir (“Tal vez le interese…”, “Otros clientes también compraron…”, etc.) durante el proceso de compra. Este tipo de algoritmos no supervisados también se usan para la detección de anomalías (muy útil en la prevención del fraude bancario o en la detección de defectos de fabricación). El sistema está entrenado con ejemplos normales, por lo que es capaz de determinar si una nueva instancia es o no una anomalía.\nAlgunos sistemas de clasificación de imágenes serían un ejemplo de aprendizaje semisupervisado: son capaces de detectar personas y probablemente determinará que la persona X aparece en el siguiente grupo de imágenes. Tan solo hay que ayudarle indicándole quién es esa persona para que a la siguiente ocasión sepa etiquetarla correctamente.\nPor último, el aprendizaje por reforzamiento es un tipo muy diferente a los anteriores. El sistema obtiene recompensas o penalizaciones en función de sus acciones. Debe aprender a partir de ellas, eligiendo cuál sería la mejor estrategia (denominada política) para obtener la mayor recompensa a lo largo del tiempo. AlphaGo sería un ejemplo de aprendizaje por refuerzo. Aprendió su política ganadora estudiando millones de partidas. Durante su combate con el campeón del mundo aplicó las políticas que había aprendido.\nEn posteriores artículos hablaremos también de algunos de los lenguajes más idóneos y la combinación de herramientas que tenemos a nuestra disposición para trabajar de forma inmediata en machine learning: Python, Jupyter Notebook, Scikit-Learn, Tensor-Flow, Keras, etc.\nRevisaremos cuáles son las fases principales de un proyecto típico de machine learning con ejemplos prácticos.\n¡Bienvenidos al mundo de machine learning!\nPor cierto, apenas 3 años después de su derrota por AlphaGo, Lee Sedol se retiró de las competiciones oficiales. Si tenéis oportunidad no dejéis de ver el documental AlphaGo – The movie, que narra la apasionante crónica del combate entre ambas “mentes”.\n","description":"","tags":["aprendizaje automático","machine learning"],"title":"Breve Introducción a Machine Learning","uri":"/posts/breve-introduccion-machine-learning/"}]
