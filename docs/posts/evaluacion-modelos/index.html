<!DOCTYPE html>
<html lang="es-ES">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.83.1" /><meta name="theme-color" content="#fff" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>Evaluación del rendimiento de modelos | Lords of the Machine Learning</title>

    <link rel="stylesheet" href="/css/meme.min.317d84c7f6754ad23a2ae219d212f73a900416875fb1144e27652ca955c5aac2.css"/>

    
    
        <script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lunr-languages@1.4.0/min/lunr.stemmer.support.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lunr-languages@1.4.0/min/lunr.es.min.js" defer></script><script src="/js/meme.min.49400d21a299355489a0a02cf7737ef1e4e5fe75fe5bd16cd756376c2bb6b749.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" /></noscript>

    <meta name="author" content="Antonio Méndez" /><meta name="description" content="En anteriores posts vimos el framework general de validación cruzada y su uso para evaluar el rendimiento de modelos. Sin embargo, es importante tener en cuenta que algunos elementos de la validación cruzada deben decidirse en función de la naturaleza del problema: (i) la estrategia de validación cruzada y (ii) las métricas de evaluación." />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="Lords of the Machine Learning" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="Lords of the Machine Learning" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://sgtsteiner.github.io/posts/evaluacion-modelos/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2022-05-03T17:30:50+02:00",
        "dateModified": "2022-05-10T10:29:00+02:00",
        "url": "https://sgtsteiner.github.io/posts/evaluacion-modelos/",
        "headline": "Evaluación del rendimiento de modelos",
        "description": "En anteriores posts vimos el framework general de validación cruzada y su uso para evaluar el rendimiento de modelos. Sin embargo, es importante tener en cuenta que algunos elementos de la validación cruzada deben decidirse en función de la naturaleza del problema: (i) la estrategia de validación cruzada y (ii) las métricas de evaluación.",
        "inLanguage" : "es-ES",
        "articleSection": "posts",
        "wordCount":  14284 ,
        "image": ["https://sgtsteiner.github.io/images/output_13_0.png","https://sgtsteiner.github.io/images/output_29_1.png","https://sgtsteiner.github.io/images/output_35_0.png","https://sgtsteiner.github.io/images/output_47_2.png","https://sgtsteiner.github.io/images/output_52_3.png","https://sgtsteiner.github.io/images/output_53_1.png","https://sgtsteiner.github.io/images/output_58_0.png","https://sgtsteiner.github.io/images/output_59_1.png","https://sgtsteiner.github.io/images/output_64_0.png","https://sgtsteiner.github.io/images/output_65_0.png","https://sgtsteiner.github.io/images/output_77_0.png","https://sgtsteiner.github.io/images/output_87_2.png","https://sgtsteiner.github.io/images/output_92_0.png","https://sgtsteiner.github.io/images/output_98_1.png","https://sgtsteiner.github.io/images/output_110_0.png","https://sgtsteiner.github.io/images/output_115_0.png","https://sgtsteiner.github.io/images/output_136_0.png","https://sgtsteiner.github.io/images/output_142_1.png","https://sgtsteiner.github.io/images/output_165_0.png","https://sgtsteiner.github.io/images/output_171_0.png","https://sgtsteiner.github.io/images/output_184_0.png","https://sgtsteiner.github.io/images/output_186_0.png","https://sgtsteiner.github.io/images/output_208_0.png","https://sgtsteiner.github.io/images/output_229_0.png","https://sgtsteiner.github.io/images/output_233_0.png","https://sgtsteiner.github.io/images/output_261_0.png","https://sgtsteiner.github.io/images/output_288_0.png","https://sgtsteiner.github.io/images/output_290_0.png","https://sgtsteiner.github.io/images/output_296_0.png"],
        "author": {
            "@type": "Person",
            "description": "Sgt. Steiner",
            "email": "futitotal@gmail.com",
            "image": "https://sgtsteiner.github.io/icons/apple-touch-icon.png",
            "url": "https://sgtsteiner.github.io/",
            "name": "Antonio Méndez"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es)",
        "publisher": {
            "@type": "Organization",
            "name": "Lords of the Machine Learning",
            "logo": {
                "@type": "ImageObject",
                "url": "https://sgtsteiner.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://sgtsteiner.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://sgtsteiner.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />


<meta name="twitter:site" content="@Steiner_69" />
<meta name="twitter:creator" content="@Steiner_69" />

    



<meta property="og:title" content="Evaluación del rendimiento de modelos" />
<meta property="og:description" content="En anteriores posts vimos el framework general de validación cruzada y su uso para evaluar el rendimiento de modelos. Sin embargo, es importante tener en cuenta que algunos elementos de la validación cruzada deben decidirse en función de la naturaleza del problema: (i) la estrategia de validación cruzada y (ii) las métricas de evaluación." />
<meta property="og:url" content="https://sgtsteiner.github.io/posts/evaluacion-modelos/" />
<meta property="og:site_name" content="Lords of the Machine Learning" />
<meta property="og:locale" content="es" /><meta property="og:image" content="https://sgtsteiner.github.io/images/output_13_0.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2022-05-03T17:30:50&#43;02:00" />
    <meta property="article:modified_time" content="2022-05-10T10:29:00&#43;02:00" />
    
    <meta property="article:section" content="posts" />



    
    

    
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">Lords of the Machine Learning</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7 0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6 0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6 0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3 0 64v48c0 8.8 7.2 16 16 16h480c8.8 0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class="menu-item-name">Posts</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255 0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255 0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255 0-24 10.745-24 24zm386.667-56H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24z"/></svg><span class="menu-item-name">Categories</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">Tags</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">About</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
            
                <li class="menu-item search-item">
                        <form id="search" class="search" role="search">
    <label for="search-input"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
    <input type="search" id="search-input" class="search-input">
</form>

<template id="search-result" hidden>
    <article class="content post">
        <h2 class="post-title"><a class="summary-title-link"></a></h2>
        <summary class="summary"></summary>
        <div class="read-more-container">
            <a class="read-more-link"> »</a>
        </div>
    </article>
</template>

                    </li>
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-small-caps="true" data-align="default" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">Evaluación del rendimiento de modelos</h1>

            

            
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2022-05-03T17:30:50&#43;02:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;3.5.2022</time>
    
    
        
        <time datetime="2022-05-10T10:29:00&#43;02:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;10.5.2022</time>
    
    
    
        
        
        
            
                <span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href="/categories/tutoriales/" class="category-link p-category">tutoriales</a></span>
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;14284</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;68&nbsp;</span>
    
    
    
</div>

            

            <div class="post-body e-content">
              <p style="text-indent:0"><span class="drop-cap">E</span>n anteriores posts vimos el framework general de validación cruzada y su uso para evaluar el rendimiento de modelos. Sin embargo, es importante tener en cuenta que algunos elementos de la validación cruzada deben decidirse en función de la naturaleza del problema: (i) la estrategia de validación cruzada y (ii) las métricas de evaluación. Además, siempre es bueno comparar el rendimiento de los modelos respecto de alguna línea base.</p>
<p>En este post presentaremos ambos aspectos y ofreceremos intuiciones e ideas de cuándo usar una estrategia de validación cruzada y métrica específicas. Además, también daremos algunas ideas sobre cómo comparar un modelo con alguna línea base.</p>
<p>Como objetivos generales intentaremos:</p>
<ul>
<li>comprender la necesidad de usar una estrategia adecuada de validación cruzada dependiendo de los datos;</li>
<li>obtener las intuiciones y principios que hay detrás del uso de la validación cruzada anidada cuando el modelo necesita ser evaluado y optimizado;</li>
<li>comprender las diferencias entre las métricas de regresión y clasificación;</li>
<li>comprender las diferencias entre métricas.</li>
</ul>
<h1 id="comparación-del-rendimiento-del-modelo-con-una-línea-base-simple"><a href="#comparación-del-rendimiento-del-modelo-con-una-línea-base-simple" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Comparación del rendimiento del modelo con una línea base simple</h1>
<p>Vamos a ver cómo comparar el rendimiento de generalización de un modelo con una mínima linea base. En regresión, podemos usar la clase <code>DummyRegressor</code> para predecir el valor medio del objetivo observado en el conjunto de entrenamiento sin usar las variables de entrada.</p>
<p>Demostraremos cómo calcular la puntuación de un modelo de regresión y compararlo con una línea base en el dataset de viviendas de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala el objetivo en k$</span></code></pre></div>
<p>En todas las evaluaciones usaremos un divisor de validación cruzada <code>ShuffleSplit</code> con el 20% de los datos reservados para validación.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<p>Empezaremos ejecutando la validación cruzada para un simple árbol de decisión regresor, que es nuestro modelo de interés. Además, almacenaremos el error de prueba en un objeto Series de pandas para hacer más sencillo dibujar los resultados.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>
<span class="n">cv_results_tree_regressor</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">errors_tree_regressor</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="o">-</span><span class="n">cv_results_tree_regressor</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Arbol decisión regresor&#34;</span>
<span class="p">)</span>
<span class="n">errors_tree_regressor</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span></code></pre></div>
<pre><code>count    30.000000
mean     45.641306
std       1.249005
min      43.111065
25%      44.691150
50%      45.586332
75%      46.640596
max      47.711138
Name: Arbol decisión regresor, dtype: float64
</code></pre>
<p>Luego, evaluamos nuestra línea base.  Esta línea base se denomina regresor dummy. Este regresor dummy siempre predecirá la media del objetivo calculada en la variable objetivo de entrenamiento. Por lo tanto, el regresor dummy no usa ninguna información de las variables de entrada almacenadas en el dataframe llamado <code>X</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyRegressor</span>

<span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">)</span>
<span class="n">result_dummy</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">dummy</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">errors_dummy_regressor</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="o">-</span><span class="n">result_dummy</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Regresor dummy&#34;</span>
<span class="p">)</span>
<span class="n">errors_dummy_regressor</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span></code></pre></div>
<pre><code>count    30.000000
mean     91.140009
std       0.821140
min      89.757566
25%      90.543652
50%      91.034555
75%      91.979007
max      92.477244
Name: Regresor dummy, dtype: float64
</code></pre>
<p>Ahora dibujaremos los errores de prueba de la validación cruzada para la línea base usando la media del objetivo y el actual árbol de decisión regresor.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_errors</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">errors_tree_regressor</span><span class="p">,</span> <span class="n">errors_dummy_regressor</span><span class="p">],</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">all_errors</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Arbol decisión regresor</th>
      <th>Regresor dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>47.156293</td>
      <td>90.713153</td>
    </tr>
    <tr>
      <th>1</th>
      <td>46.491834</td>
      <td>90.539353</td>
    </tr>
    <tr>
      <th>2</th>
      <td>43.960410</td>
      <td>91.941912</td>
    </tr>
    <tr>
      <th>3</th>
      <td>43.343595</td>
      <td>90.213912</td>
    </tr>
    <tr>
      <th>4</th>
      <td>47.711138</td>
      <td>92.015862</td>
    </tr>
    <tr>
      <th>5</th>
      <td>44.960189</td>
      <td>90.542490</td>
    </tr>
    <tr>
      <th>6</th>
      <td>44.180467</td>
      <td>89.757566</td>
    </tr>
    <tr>
      <th>7</th>
      <td>44.498127</td>
      <td>92.477244</td>
    </tr>
    <tr>
      <th>8</th>
      <td>45.463726</td>
      <td>90.947952</td>
    </tr>
    <tr>
      <th>9</th>
      <td>45.048634</td>
      <td>91.991373</td>
    </tr>
    <tr>
      <th>10</th>
      <td>46.661503</td>
      <td>92.023571</td>
    </tr>
    <tr>
      <th>11</th>
      <td>46.020580</td>
      <td>90.556965</td>
    </tr>
    <tr>
      <th>12</th>
      <td>45.755225</td>
      <td>91.539567</td>
    </tr>
    <tr>
      <th>13</th>
      <td>45.130518</td>
      <td>91.185225</td>
    </tr>
    <tr>
      <th>14</th>
      <td>47.388774</td>
      <td>92.298971</td>
    </tr>
    <tr>
      <th>15</th>
      <td>44.601470</td>
      <td>91.084639</td>
    </tr>
    <tr>
      <th>16</th>
      <td>45.564095</td>
      <td>90.984471</td>
    </tr>
    <tr>
      <th>17</th>
      <td>47.202268</td>
      <td>89.981744</td>
    </tr>
    <tr>
      <th>18</th>
      <td>44.568353</td>
      <td>90.547140</td>
    </tr>
    <tr>
      <th>19</th>
      <td>46.764385</td>
      <td>89.820219</td>
    </tr>
    <tr>
      <th>20</th>
      <td>43.111065</td>
      <td>91.768721</td>
    </tr>
    <tr>
      <th>21</th>
      <td>45.608568</td>
      <td>92.305556</td>
    </tr>
    <tr>
      <th>22</th>
      <td>45.263799</td>
      <td>90.503017</td>
    </tr>
    <tr>
      <th>23</th>
      <td>46.884297</td>
      <td>92.147974</td>
    </tr>
    <tr>
      <th>24</th>
      <td>46.215357</td>
      <td>91.386320</td>
    </tr>
    <tr>
      <th>25</th>
      <td>45.934370</td>
      <td>90.815660</td>
    </tr>
    <tr>
      <th>26</th>
      <td>44.374564</td>
      <td>92.216574</td>
    </tr>
    <tr>
      <th>27</th>
      <td>46.577874</td>
      <td>90.107460</td>
    </tr>
    <tr>
      <th>28</th>
      <td>45.182111</td>
      <td>90.620318</td>
    </tr>
    <tr>
      <th>29</th>
      <td>47.615573</td>
      <td>91.165331</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">all_errors</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Error absoluto medio (k$)&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Errores de prueba de validación cruzada&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_13_0.png" alt="png"></p>
<p>Vemos que el rendimiento de generalización de nuestro ábol de decisión está lejos de ser perfecto: las predicciones del precio tiene aproximadamente un márgen de error de 45000 dólares de media. Sin embargo, es mucho mejor que el margen de error de la línea base. Por tanto, esto confirma que es posible predecir mucho mejor el precio de la vivienda usando un modelo que tenga en cuenta los valores de las variables de entrada (localización de la propiedad, tamaño, ingresos del vecindario, etc.). Dicho modelo hace predicciones más formadas y, aproximadamente, divide la tasa de error por la mitad comparado con la línea base que ignora las variables de entrada.</p>
<p>Observemos que hemos usado la media del precio como predicción de línea base. Podríamos haber usado en su lugar la mediana. Véase la documentación online de la clase <a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html" target="_blank" rel="noopener"><code>sklearn.dummy.DummyRegressor</code></a> para otras opciones. Para este ejemplo en particular, no existe mucha diferencia entre usar la media en lugar de la mediana, pero este podría ser el caso de un dataset con valores atípicos extremos.</p>
<h2 id="ejercicio"><a href="#ejercicio" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio</h2>
<p>Vamos a poner en práctica lo aprendido hasta ahora. Definiremos una línea base con un clasificador dummy y lo usaremos como referencia para evaluar el rendimiento predictivo relativo de un modelo de interés dado.</p>
<p>Ilustraremos dicha línea base con la ayuda del dataset del censo de adultos, usando únicamente las variables numéricas, por simplicidad.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">adult_census</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/adult-census-numeric-all.csv&#34;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">adult_census</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&#34;class&#34;</span><span class="p">),</span> <span class="n">adult_census</span><span class="p">[</span><span class="s2">&#34;class&#34;</span><span class="p">]</span></code></pre></div>
<p>Primero definiremos una estrategia de validación cruzada con <code>ShuffleSplit</code> tomando la mitad de las muestras como prueba en cada ciclo. Usaremos 10 ciclos de validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<p>Lo siguiente es crear un pipeline de machine learning compuesto por un transformador para estandarizar los datos seguido por un clasificador de regresión logística.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span></code></pre></div>
<p>Calculemos ahora las puntuaciones de prueba de validación cruzada para el clasificador en este dataset y almacenaremos los resultados en una Series de panda.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">cv_results_logistic_regression</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">classifier</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">test_score_logistic_regression</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="n">cv_results_logistic_regression</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Regresión logística&#34;</span>
<span class="p">)</span>
<span class="n">test_score_logistic_regression</span></code></pre></div>
<pre><code>0    0.815937
1    0.813849
2    0.815036
3    0.815569
4    0.810982
5    0.814709
6    0.813112
7    0.810327
8    0.812416
9    0.816388
Name: Regresión logística, dtype: float64
</code></pre>
<p>Ahora calcularemos las puntuaciones de validación cruzada de un clasificador dummy que prediga constantemente la clase más frecuente observada en el conjunto de entrenamiento. Almacenaremos los resultados en un Series de panda.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>

<span class="n">dummy_most_frequent</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;most_frequent&#34;</span><span class="p">)</span>
<span class="n">result_dummy</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">dummy_most_frequent</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">test_dummy_most_frequent</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="n">result_dummy</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Most-frequent dummy&#34;</span>
<span class="p">)</span>
<span class="n">test_dummy_most_frequent</span></code></pre></div>
<pre><code>0    0.760329
1    0.756808
2    0.759142
3    0.760739
4    0.761681
5    0.761885
6    0.757463
7    0.757176
8    0.761885
9    0.763114
Name: Most-frequent dummy, dtype: float64
</code></pre>
<p>Ahora que hemos recopilado los resultados tanto de la línea base como del modelo, vamos a concatenar las puntuaciones de prueba como columnas en un dataframe de pandas.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">test_score_logistic_regression</span><span class="p">,</span> <span class="n">test_dummy_most_frequent</span><span class="p">],</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">all_scores</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Regresión logística</th>
      <th>Most-frequent dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.815937</td>
      <td>0.760329</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.813849</td>
      <td>0.756808</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.815036</td>
      <td>0.759142</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.815569</td>
      <td>0.760739</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.810982</td>
      <td>0.761681</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.814709</td>
      <td>0.761885</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.813112</td>
      <td>0.757463</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.810327</td>
      <td>0.757176</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.812416</td>
      <td>0.761885</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.816388</td>
      <td>0.763114</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>Lo siguiente que haremos será dibujar el histograma de las puntuaciones de prueba de validación cruzada para ambos modelos con la ayuda de la <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#histograms" target="_blank" rel="noopener">función de dibujado incorporada en pandas</a>. ¿Qué conclusiones obtenemos de los resultados?</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Accuracy (%)&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Puntuaciones de prueba de validación cruzada&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_29_1.png" alt="png"></p>
<p>Observamos que los dos histogramas están bien separados. Por lo tanto el clasificador dummy con la estrategia <code>most_frequent</code> tiene mucha menos precisión que el clasificador de regresión logística. Podemos concluir que el modelo de regresión logística puede encontrar satisfactoriamente información predictiva en las variables de entrada para mejorar la línea base.</p>
<p>Por último, vamos a cambiar la estrategia del clasificador dummy a <code>&quot;stratified&quot;</code> y calcularemos los resultados. De igual forma, calcularemos las puntuaciones para la <code>strategy=&quot;uniform&quot;</code> y después dibujaremos la distribución conjuntamente con los otros resultados.</p>
<p>¿Estas nuevas líneas base son mejores que la previa? ¿Por qué es este el caso?</p>
<p>Podemos consultar la documentación de <a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html" target="_blank" rel="noopener"><code>sklearn.dummy.DummyClassifier</code></a> para conocer el significado de estas estrategias.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dummy_stratified</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;stratified&#34;</span><span class="p">)</span>
<span class="n">result_dummy_stratified</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">dummy_stratified</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">test_dummy_stratified</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="n">result_dummy_stratified</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Stratified dummy&#34;</span>
<span class="p">)</span>
<span class="n">test_dummy_stratified</span></code></pre></div>
<pre><code>0    0.638590
1    0.640596
2    0.636133
3    0.634085
4    0.634085
5    0.634126
6    0.632652
7    0.638016
8    0.639327
9    0.635027
Name: Stratified dummy, dtype: float64
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">dummy_uniform</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;uniform&#34;</span><span class="p">)</span>
<span class="n">result_dummy_uniform</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">dummy_uniform</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">test_dummy_uniform</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
    <span class="n">result_dummy_uniform</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Uniform dummy&#34;</span>
<span class="p">)</span>
<span class="n">test_dummy_uniform</span></code></pre></div>
<pre><code>0    0.502477
1    0.505426
2    0.505549
3    0.501003
4    0.503911
5    0.497318
6    0.501413
7    0.499775
8    0.500143
9    0.496171
Name: Uniform dummy, dtype: float64
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
    <span class="p">[</span><span class="n">test_score_logistic_regression</span><span class="p">,</span>
     <span class="n">test_dummy_most_frequent</span><span class="p">,</span>
     <span class="n">test_dummy_stratified</span><span class="p">,</span>
     <span class="n">test_dummy_uniform</span><span class="p">,],</span>
    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">all_scores</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Regresión logística</th>
      <th>Most-frequent dummy</th>
      <th>Stratified dummy</th>
      <th>Uniform dummy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.815937</td>
      <td>0.760329</td>
      <td>0.638590</td>
      <td>0.502477</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.813849</td>
      <td>0.756808</td>
      <td>0.640596</td>
      <td>0.505426</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.815036</td>
      <td>0.759142</td>
      <td>0.636133</td>
      <td>0.505549</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.815569</td>
      <td>0.760739</td>
      <td>0.634085</td>
      <td>0.501003</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.810982</td>
      <td>0.761681</td>
      <td>0.634085</td>
      <td>0.503911</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.814709</td>
      <td>0.761885</td>
      <td>0.634126</td>
      <td>0.497318</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.813112</td>
      <td>0.757463</td>
      <td>0.632652</td>
      <td>0.501413</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.810327</td>
      <td>0.757176</td>
      <td>0.638016</td>
      <td>0.499775</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.812416</td>
      <td>0.761885</td>
      <td>0.639327</td>
      <td>0.500143</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.816388</td>
      <td>0.763114</td>
      <td>0.635027</td>
      <td>0.496171</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Accuracy (%)&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Puntuaciones de prueba de validación cruzada&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_35_0.png" alt="png"></p>
<p>Vemos que al usar <code>strategy=&quot;stratified&quot;</code> los resultados son mucho peores que con la estrategia <code>most_frequent</code>. Dado que las clases están desbalanceadas, predecir la más frecuente implica que acertaremos en la proporción de esa clase (aproximadamente el 75% de las muestras), es decir, el 75% de las veces. Sin embargo, la estrategia <code>&quot;stratified&quot;</code> generará predicciones aleatoriamente respetando la distribución de las clases del conjunto de entrenamiento, lo que dará como resultado algunas predicciones incorrectas, incluso para la clase más frecuente, por lo tanto obtenemos una precisión menor.</p>
<p>Esto es aún más cierto para <code>strategy=&quot;uniform&quot;</code>: esta estrategia asigna etiquetas de clase uniformemente al azar. Por lo tanto, en un problema de clasificación binaria, la precisión de validación cruzada es del 50% de media, por lo cual es la más débil de las tres líneas base dummy.</p>
<p>Nota: se podría argumentar que las estrategias <code>&quot;uniform&quot;</code> y <code>&quot;stratified&quot;</code> son maneras válidas de definir un &ldquo;nivel de aleatoriedad&rdquo; de la precisión de una línea base para el problema de clasificación, debido a que hacen predicciones &ldquo;al azar&rdquo;.</p>
<p>Otra forma de definir un nivel de azar podría ser usando la utilidad de scikit-learn <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html" target="_blank" rel="noopener">sklearn.model_selection.permutation_test_score</a>. En lugar de usar un clasificador dummy, esta función compara la precisión de validación cruzada de un modelo de interés con la precisión de validación cruzada de este mismo modelo pero entrenado en etiquetas de clase permutadas aleatoriamente. Por lo tanto, <code>permutation_test_score</code> define un nivel de azar que depende de la elección de la clase y los hiperparámetros del estimador de interés. Cuando entrenamos en tales etiquetas permutadas aleatoriamente, muchos estimadores de machine learning terminan comportándose aproximadamente como <code>DummyClassifier(strategy=&quot;most_frequent&quot;)</code>, prediciendo siempre la clase mayoritaria, independientemente de las variables de entrada. Como resultado, esta línea base <code>most_frequent</code> se llama algunas veces &ldquo;nivel de azar&rdquo; para problemas de clasificación desbalanceados, aunque sus predicciones son completamente deterministas y no involucran mucho azar.</p>
<p>Definir el nivel de azar usando <code>permutation_test_score</code> es bastante costoso computacionalmente, debido a que requiere entrenar muchos modelos no dummys en permutaciones aleatorias de los datos. Usar clasificadores dummys como líneas base suele ser suficiente para fines prácticos. Para problemas de clasificacion desbalanceados, la estrategia <code>most_frequent</code> es la más fuerte de las tres líneas bases y por tanto la que debemos usar.</p>
<h1 id="elección-de-validación-cruzada"><a href="#elección-de-validación-cruzada" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Elección de validación cruzada</h1>
<h2 id="estratificación"><a href="#estratificación" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Estratificación</h2>
<p>Generalmente hemos usado por defecto una estrategia de validación cruzada <code>KFold</code> o <code>ShuffleSplit</code> para dividir de forma iterativa nuestro dataset. Sin embargo, no debemos asumir que estos enfoques son siempre la mejor opción: otras estrategias de validación cruzada podrían adaptarse mejor a nuestro problema.</p>
<p>Comencemos con el concepto de estratificación, dando un ejemplo donde podemos tener problemas si no somos cuidadosos. Carguemos el dataset iris.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></div>
<p>Vamos a crear un modelo de machine learning básico: una regresión logística. Esperamos que este modelo funcione bastante bien en el dataset iris, ya que es un dataset bastante simple.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">())</span></code></pre></div>
<p>Una vez que hemos creado nuestro modelo, usaremos validación cruzada para evaluarlo. Usaremos la estrategia <code>KFold</code>. Definiremos un dataset con nueve muestras y repetiremos la validación cruzada tres veces (es decir, <code>n_splits</code>).</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="n">X_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_random</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;ENTRENAMIENTO: {train_index}  PRUEBA: {test_index}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>ENTRENAMIENTO: [3 4 5 6 7 8]  PRUEBA: [0 1 2]
ENTRENAMIENTO: [0 1 2 6 7 8]  PRUEBA: [3 4 5]
ENTRENAMIENTO: [0 1 2 3 4 5]  PRUEBA: [6 7 8]
</code></pre>
<p>Al definir tres divisiones, usaremos tres muestras para prueba y seis para entrenamiento cada vez. <code>KFold</code> por defecto no baraja. Lo que significa que seleccionará las tres primeras muestras para el conjunto de prueba en la primera división, luego las tres siguientes muestras para la segunda división y las siguientes tres para la última división. Al final, todas las muestras se habrán usado en la prueba al menos una vez entre las diferentes divisiones.</p>
<p>Ahora  vamos a aplicar esta estrategia para verificar el rendimiento de generalización de nuestro modelo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.3f} +/- {test_score.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es: 0.000 +/- 0.000
</code></pre>
<p>Es una sorpresa real que nuestro modelo no pueda clasificar correctamente ninguna muestra en ninguna división de validación cruzada. Comprobemos nuestros valores de objetivo para comprender el problema.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">y</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Indice muestra&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Clase&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Valor de la clase en el objetivo y&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_47_2.png" alt="png"></p>
<p>Vemos que el vector objetivo <code>y</code> está ordenado. Tendrá algunas consecuencias inesperadas cuando usemos la validación cruzada <code>KFold</code>. Para ilustrar las consecuencias, mostraremos el recuento de clases en cada partición de la validación cruzada en los conjuntos de entrenamiento y prueba y dibujaremos esta información en un barplot.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">)</span>

<span class="n">train_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

    <span class="n">train_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
    <span class="n">test_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">train_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">train_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta clase&#34;</span>
<span class="n">train_cv_counts</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Partición #0</th>
      <th>Partición #1</th>
      <th>Partición #2</th>
    </tr>
    <tr>
      <th>Etiqueta clase</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>50.0</td>
      <td>NaN</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>50.0</td>
      <td>50.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>NaN</td>
      <td>50.0</td>
      <td>50.0</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">test_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">test_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta  clase&#34;</span>
<span class="n">test_cv_counts</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Partición #0</th>
      <th>Partición #1</th>
      <th>Partición #2</th>
    </tr>
    <tr>
      <th>Etiqueta  clase</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>50.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NaN</td>
      <td>50.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>50.0</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto entrenamiento&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_52_3.png" alt="png"></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto prueba&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_53_1.png" alt="png"></p>
<p>Podemos confirmar que en cada partición del conjunto de entrenamiento solo están presentes dos de las tres clases y todas las muestras de la clase restante se usan como conjunto de prueba. Por tanto, nuestro modelo es incapaz de predecir esta clase que no ha sido visto nunca durante la fase de entrenamiento.</p>
<p>Una posibilidad para resolver este problema es barajar los datos antes de dividirlos en los tres grupos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.3f} +/- {test_score.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es: 0.953 +/- 0.009
</code></pre>
<p>Obtenemos un resultado que está más cercano a lo se podría esperar con una precisión por encima del 90%. Ahora que hemos resulto nuestro primer problema, podría ser interesante comprobar si la frecuencia de clases en el conjunto de entrenamiento y prueba son iguales que las de nuestro dataset original. Aseguraría que estamos entrenando y probando nuestro modelo con una distribución de clases que encontaremos en producción.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

    <span class="n">train_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
    <span class="n">test_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
<span class="n">train_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">train_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">test_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">test_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">train_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta clase&#34;</span>
<span class="n">test_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta clase&#34;</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto entrenamiento&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_58_0.png" alt="png"></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto prueba&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_59_1.png" alt="png"></p>
<p>Vemos que ni el conjunto de entrenamiento ni el de prueba tienen las mismas frecuencias de clase que nuestro dataset original debido a que el recuento de cada clase varía un poco.</p>
<p>Sin embargo, podríamos querer dividir nuestros datos preservando las frecuencias de clase originales: queremos <strong>estratificar</strong> nuestros datos por clase. En scikit-learn, algunas estrategias de validación cruzada implementan la estratificación; contienen <code>Stratified</code> en sus nombres.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.3f} +/- {test_score.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es: 0.960 +/- 0.016
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_cv_counts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">fold_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>

    <span class="n">train_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
    <span class="n">test_cv_counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
<span class="n">train_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">train_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">test_cv_counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">test_cv_counts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">keys</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s2">&#34;Partición #{idx}&#34;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)])</span>
<span class="n">train_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta clase&#34;</span>
<span class="n">test_cv_counts</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;Etiqueta clase&#34;</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">train_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto entrenamiento&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_64_0.png" alt="png"></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_cv_counts</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Recuento&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Conjunto prueba&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_65_0.png" alt="png"></p>
<p>En este caso, observamos que el recuento de clases es muy parecido entre el conjunto de entrenamiento y el conjunto de prueba. La diferencia es debida al pequeño número de muestras del dataset iris.</p>
<p>En conclusión, es una buena práctica usar estratificación dentro de la validación cruzada cuando se trata de un problema de clasificación.</p>
<h2 id="agrupación-de-muestras"><a href="#agrupación-de-muestras" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Agrupación de muestras</h2>
<p>Vamos a detenernos en el concepto de grupos de muestras. Usaremos el dataset de dígitos escritos a mano.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span></code></pre></div>
<p>Vamos a recrear el mismo modelo que vimos más arriba: un clasificador de regresión logística con preprocesamiento para escalar los datos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">MinMaxScaler</span><span class="p">(),</span>
                      <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1_000</span><span class="p">))</span></code></pre></div>
<p>Usaremos el mismo modelo de línea base. Usaremos una validación cruzada <code>KFold</code> sin mezclar los datos al principio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">test_score_no_shuffling</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score_no_shuffling.mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score_no_shuffling.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es: 0.931 +/- 0.026
</code></pre>
<p>Ahora repitamos lo mismo mezclando los datos dentro de la validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_score_with_shuffling</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score_with_shuffling.mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score_with_shuffling.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es: 0.967 +/- 0.008
</code></pre>
<p>Observamos que mezclar los datos mejora la precisión media. Podríamos ir un poco más allá y dibujar la distribución de la puntuación de prueba. Primero concatenemos las puntuaciones de prueba.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">test_score_no_shuffling</span><span class="p">,</span> <span class="n">test_score_with_shuffling</span><span class="p">],</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;KFold sin mezclado&#34;</span><span class="p">,</span> <span class="s2">&#34;KFold con mezclado&#34;</span><span class="p">],</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Puntuación de precisión&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Distribución de las puntuaciones de prueba&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_77_0.png" alt="png"></p>
<p>El error de prueba de validación cruzada que usa el mezclado tiene menos varianza que la que no impone ningún mezclado. Lo que significa que, en este caso, alguna partición específica conduce a una puntuación baja.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">test_score_no_shuffling</span><span class="p">)</span></code></pre></div>
<pre><code>[0.94166667 0.89722222 0.94986072 0.9637883  0.90250696]
</code></pre>
<p>Por lo tanto, existe una estructura subyacente en los datos que al mezclarlos se rompe y se obtienen mejores resultados. Para tener una mejor comprensión, podríamos leer la documentación que acompaña al dataset.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span></code></pre></div>
<pre><code>.. _digits_dataset:

Optical recognition of handwritten digits dataset
--------------------------------------------------

**Data Set Characteristics:**

    :Number of Instances: 1797
    :Number of Attributes: 64
    :Attribute Information: 8x8 image of integer pixels in the range 0..16.
    :Missing Attribute Values: None
    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)
    :Date: July; 1998

This is a copy of the test set of the UCI ML hand-written digits datasets
https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits

The data set contains images of hand-written digits: 10 classes where
each class refers to a digit.

Preprocessing programs made available by NIST were used to extract
normalized bitmaps of handwritten digits from a preprinted form. From a
total of 43 people, 30 contributed to the training set and different 13
to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of
4x4 and the number of on pixels are counted in each block. This generates
an input matrix of 8x8 where each element is an integer in the range
0..16. This reduces dimensionality and gives invariance to small
distortions.

For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.
T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
1994.

.. topic:: References

  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
    Graduate Studies in Science and Engineering, Bogazici University.
  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.
  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.
    Linear dimensionalityreduction using relevance weighted LDA. School of
    Electrical and Electronic Engineering Nanyang Technological University.
    2005.
  - Claudio Gentile. A New Approximate Maximal Margin Classification
    Algorithm. NIPS. 2000.
</code></pre>
<p>Si leemos cuidadosamente, 13 personas escribieron los dígitos para nuestro dataset, lo que supone un total de 1797 muestras. Por tanto, una persona escribió varias veces el mismo número. Supongamos que las muestras de la persona están agrupadas. En consecuencia, no mezclar los datos mantendrá todas las muestras de la persona juntas, ya sea en el conjunto de entrenamiento como en el de prueba. Mezclar los datos romperá esta estructura y, por tanto, dígitos escritos por la misma persona estarán disponibles tanto en conjunto de entrenamiento como en el de prueba.</p>
<p>Además, por norma general, una persona tenderá a escribir los dígitos de la misma manera. Así, nuestro modelo aprenderá a identificar los patrones de una persona para cada dígito en lugar de reconocer el dígito en sí mismo.</p>
<p>Podemos resolver este problema asegurando que los datos asociados a una persona pertenezcan al conjunto de entrenamiento o al de prueba. Por tanto, queremos agrupar muestras para cada persona. De hecho, podemos recuperar los grupos mirando la variable objetivo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">]</span></code></pre></div>
<pre><code>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
       2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7,
       7, 3, 5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6,
       6, 6, 4, 9, 1, 5, 0, 9, 5, 2, 8, 2, 0, 0, 1, 7, 6, 3, 2, 1, 7, 4,
       6, 3, 1, 3, 9, 1, 7, 6, 8, 4, 3, 1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5,
       4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5, 4, 8, 8, 4, 9, 0, 8, 9, 8, 0, 1,
       2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,
       4, 5, 6, 7, 8, 9, 0, 9, 5, 5, 6, 5, 0, 9, 8, 9, 8, 4, 1, 7, 7, 3,
       5, 1, 0, 0, 2, 2, 7, 8, 2, 0, 1, 2, 6, 3, 3, 7, 3, 3, 4, 6, 6, 6,
       4, 9])
</code></pre>
<p>Puede que no sea obvio al principio, pero existe una estructura en el objetivo: existe un patrón repetitivo que siempre empieza por una serie ordenada de dígitos del 0 al 9 seguidos de dígitos aleatorios en un cierto punto. Si miramos en detalle, vemos que existen 14 de tales patrones, siempre con alrededor de 130 muestras cada uno.</p>
<p>Incluso si no corresponden exactamente a las 13 personas que menciona la documentación (quizás una persona escribió dos series de dígitos), podemos hipotetizar que cada uno de esos patrones corresponde a una persona diferente y, por tanto, a un grupo diferente.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">count</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># define los límites inferior y superior de cada índice de muestras</span>
<span class="c1"># para cada escritor</span>
<span class="n">writer_boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">130</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">386</span><span class="p">,</span> <span class="mi">516</span><span class="p">,</span> <span class="mi">646</span><span class="p">,</span> <span class="mi">776</span><span class="p">,</span> <span class="mi">915</span><span class="p">,</span> <span class="mi">1029</span><span class="p">,</span>
                     <span class="mi">1157</span><span class="p">,</span> <span class="mi">1287</span><span class="p">,</span> <span class="mi">1415</span><span class="p">,</span> <span class="mi">1545</span><span class="p">,</span> <span class="mi">1667</span><span class="p">,</span> <span class="mi">1797</span><span class="p">]</span>
<span class="n">groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">lower_bounds</span> <span class="o">=</span> <span class="n">writer_boundaries</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">upper_bounds</span> <span class="o">=</span> <span class="n">writer_boundaries</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

<span class="k">for</span> <span class="n">group_id</span><span class="p">,</span> <span class="n">lb</span><span class="p">,</span> <span class="n">up</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">count</span><span class="p">(),</span> <span class="n">lower_bounds</span><span class="p">,</span> <span class="n">upper_bounds</span><span class="p">):</span>
    <span class="n">groups</span><span class="p">[</span><span class="n">lb</span><span class="p">:</span><span class="n">up</span><span class="p">]</span> <span class="o">=</span> <span class="n">group_id</span></code></pre></div>
<p>Podemos verificar los grupos dibujando los índices asociados a los id&rsquo;s de escritor.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">groups</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">groups</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">writer_boundaries</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Índice del objetivo&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Índice del escritor&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Grupos de escritores subyacentes existentes en el objetivo&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_87_2.png" alt="png"></p>
<p>Una vez que agrupamos los dígitos por escritor, podemos usar validación cruzada para tener esta información en cuenta: la clase conteniendo <code>Group</code> debe ser usada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GroupKFold</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">GroupKFold</span><span class="p">()</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                             <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La precisión media es &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La precisión media es 0.920 +/- 0.021
</code></pre>
<p>Vemos que esta estrategia es menos optimista en lo que respecta al rendimiento de generalización del modelo. Sin embargo, este es más confiable si nuestro propósito es hacer reconocimiento de dígitos manuscritos por escritores independientes. Además, podemos ver que la desviación estándar se ha reducido.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">test_score_no_shuffling</span><span class="p">,</span> <span class="n">test_score_with_shuffling</span><span class="p">,</span> <span class="n">test_score</span><span class="p">],</span>
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;KFold sin mezclado&#34;</span><span class="p">,</span> <span class="s2">&#34;KFold con mezclado&#34;</span><span class="p">,</span>
           <span class="s2">&#34;KFold con grupos&#34;</span><span class="p">],</span>
<span class="p">)</span><span class="o">.</span><span class="n">T</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Puntuación de precisión&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Distribución de las puntuaciones de prueba&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_92_0.png" alt="png"></p>
<p>Como conclusión, cuando evaluamos un modelo es realmente importante tener en cuenta cualquier patrón de agrupamiento de las muestras. De lo contario, los resultados obtenidos podrían ser demasiado optimistas respecto a la realidad.</p>
<h2 id="sin-datos-iid"><a href="#sin-datos-iid" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Sin datos i.i.d</h2>
<p>En machine learning es bastante común asumir que los datos son i.i.d. (acrónimo inglés correspondiente a &ldquo;<em>independent and identically distributed</em>&rdquo;, independientes e idénticamente distribuidos), lo que significa que el proceso generativo no tiene ninguna memoria de muestras pasadas para generar nuevas muestras.</p>
<p>Esta asunción es violada normalmente cuando tratamos con series temporales. Una muestra depende de información pasada.</p>
<p>Veremos un ejemplo para destacar estos problemas con datos no i.i.d en las estrategias de validación cruzada presentadas anteriormente. Vamos a cargar cotizaciones financieras de algunas empresas de energía.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">symbols</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;TOT&#34;</span><span class="p">:</span> <span class="s2">&#34;Total&#34;</span><span class="p">,</span> <span class="s2">&#34;XOM&#34;</span><span class="p">:</span> <span class="s2">&#34;Exxon&#34;</span><span class="p">,</span> <span class="s2">&#34;CVX&#34;</span><span class="p">:</span> <span class="s2">&#34;Chevron&#34;</span><span class="p">,</span>
           <span class="s2">&#34;COP&#34;</span><span class="p">:</span> <span class="s2">&#34;ConocoPhillips&#34;</span><span class="p">,</span> <span class="s2">&#34;VLO&#34;</span><span class="p">:</span> <span class="s2">&#34;Valero Energy&#34;</span><span class="p">}</span>
<span class="n">template_name</span> <span class="o">=</span> <span class="s2">&#34;../data/financial-data/{}.csv&#34;</span>

<span class="n">quotes</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
        <span class="n">template_name</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">symbol</span><span class="p">),</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
    <span class="n">quotes</span><span class="p">[</span><span class="n">symbols</span><span class="p">[</span><span class="n">symbol</span><span class="p">]]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&#34;open&#34;</span><span class="p">]</span>
<span class="n">quotes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">quotes</span><span class="p">)</span></code></pre></div>
<p>Vamos a empezar dibujando las diferentes cotizaciones financieras.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">quotes</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Valor de cotización&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Evolución del valor de las acciones&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_98_1.png" alt="png"></p>
<p>Vamos a repetir el experimento anterior. En lugar de usar datos aleatorios, esta vez usaremos cotizaciones reales. Aunque es obvio que un modelo predictivo no funciona en la práctica en datos aleatorios, esto es lo mismo con estos datos reales. Aquí queremos predecir la cotización de Chevron usando las cotizaciones de las otras compañías de energía.</p>
<p>Para hacer gráficos explicativos, usaremos una única división además de la validación cruzada que ya usamos anteriormente.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">quotes</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Chevron&#34;</span><span class="p">]),</span> <span class="n">quotes</span><span class="p">[</span><span class="s2">&#34;Chevron&#34;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<p>Usaremos un árbol de decisión regresor que esperamos que tenga overfitting y, por tanto, no generalizará a datos no vistos. Usaremos una validación cruzada <code>ShuffleSplit</code> para comprobar el rendimiento de generalización de nuestro modelo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                             <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La medida de R2 es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.2f} +/- {test_score.std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La medida de R2 es: 0.95 +/- 0.07
</code></pre>
<p>Sorprendentemente, hemos obtenido un rendimiento de generalización excepcional. Investigaremos y buscaremos la razón de tan buenos resultados con un modelo que se espera que falle. Anteriormente, hemos mencionado que <code>ShuffleSplit</code> es un esquema de validación cruzada iterativo que mezcla y divide datos. Simplificaremos este procedimiento con una única división y dibujaremos la predicción. Para este propósito podemos usar <code>train_test_split</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Afecta el índice de `y_pred` para facilitar el dibujado</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span></code></pre></div>
<p>Vamos a comprobar el rendimiento de generalización de nuestro modelo en esta división.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="n">test_score</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2 en esta única división es: {test_score:.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2 en esta única división es: 0.83
</code></pre>
<p>De forma similar, obtenemos buenos resultados en términos de $R^2$. Dibujaremos las muestras de entrenamiento, prueba y predicción.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Entrenamiento&#34;</span><span class="p">)</span>
<span class="n">y_test</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Prueba&#34;</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicción&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Valor cotización&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicción del modelo usando estrategia ShuffleSplit&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_110_0.png" alt="png"></p>
<p>Entonces, en este contexto, parece que las predicciones del modelo siguen las pruebas. Pero también podemos ver que las muestras de prueba están al lado de alguna muestra de entrenamiento. Y con estas series temporales vemos una relación entre una muestra en el momento <code>t</code> y una muestra en <code>t+1</code>. En este caso estamos violando la asuncion i.i.d. La idea a obtener es la siguiente: un modelo puede generar su conjunto de entrenamiento en el momento <code>t</code> para una muestra de prueba en el momento <code>t+1</code>. Esta predicción estaría cerca del valor real, incluso aunque nuestro modelo no aprendiera nada, solo memorizando el dataset de entrenamiento.</p>
<p>Una manera sencilla de verificar esta hipótesis es no mezclar los datos cuando hacemos la división. En este caso, usaremos el primer 75% de los datos para entrenar y los datos restantes para prueba.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">y_test</span><span class="o">.</span><span class="n">index</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_score</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2 en esta única división es: {test_score:.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2 en esta única división es: -2.16
</code></pre>
<p>En este caso vemos que nuestro modelo ya no es mágico. De hecho, su rendimiento es peor que sólo predecir la media del objetivo. Podemos comprobar visualmente qué estamos prediciendo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Entrenamiento&#34;</span><span class="p">)</span>
<span class="n">y_test</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Prueba&#34;</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicción&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Valor cotización&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicción del modelo usando una división sin shuffling&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_115_0.png" alt="png"></p>
<p>Vemos que nuestro modelo no puede predecir nada porque no tiene muestras alrededor de la muestra de prueba. Comprobemos cómo podríamos haber hecho un esquema adecuado de validación cruzada para obtener una estimación razonable del rendimiento de generalización.</p>
<p>Una solución podría ser agrupar los ejemplos en bloques de tiempo, por ejemplo por cuatrimestres, y predecir la información de cada grupo usando información de los otros grupos. Para este propósito, podemos usar la validación cruzada <code>LeaveOneGroupOut</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneGroupOut</span>

<span class="n">groups</span> <span class="o">=</span> <span class="n">quotes</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">to_period</span><span class="p">(</span><span class="s2">&#34;Q&#34;</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">LeaveOneGroupOut</span><span class="p">()</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                             <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La R2 media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.2f} +/- {test_score.std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La R2 media es: -0.74 +/- 1.72
</code></pre>
<p>En este caso, vemos que no podemos hacer buenas predicciones, lo que es menos sorprendente que nuestra resultados originales.</p>
<p>Otra cosa a considerar es la aplicación real de nuestra solución. Si nuestro modelo tiene como objetivo la previsión (es decir, predicción de datos futuros a partir de datos pasados), no debemos usar datos de entrenamiento que sean posteriores a los datos de prueba. En este caso, podemos usar la validación <code>TimeSeriesSplit</code> para forzar este comportamiento.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">TimeSeriesSplit</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">TimeSeriesSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">groups</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                             <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La R2 media es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.2f} +/- {test_score.std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La R2 media es: -2.27 +/- 3.42
</code></pre>
<p>En conclusión, es importante no usar una estrategia de validación cruzada que no respete algunas asunciones, como tener datos i.i.d. Podría conducir a obtener resultados absurdos que podrían hacer pensar que un modelo predictivo podría funcionar.</p>
<h1 id="validación-cruzada-anidada"><a href="#validación-cruzada-anidada" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Validación cruzada anidada</h1>
<p>La validación cruzada se puede usar tanto para el ajuste de hiperparámetros como para la estimación del rendimiento de generalización de un modelo. Sin embargo, usarla para ambos propósitos al mismo tiempo puede ser problemático, ya que la evaluación resultante puede subestimar algún overfitting que resulta del procedimiento de ajuste de hiperparámetros en sí mismo.</p>
<p>Desde un punto de vista filosófico, el ajuste de hiperparámetros es una forma de machine learning en sí misma y, por tanto, necesita otro bucle externo de validación cruzada para evaluar apropiadamente el rendimiento de generalización del procedimiento de modelado global.</p>
<p>Vamos a destacar la validación cruzada anidada y su impacto en el rendimiento de generalización estimado comparado con usar simplemente un único nivel de validación cruzada, tanto para el ajuste de hiperparámetros como para la evaluación del rendimiento de generalización.</p>
<p>Ilustraremos esta diferencia usando el dataset de cáncer de mama.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></div>
<p>En primer lugar, usaremos <code>GridSearchCV</code> para encontrar los mejores hiperparámetros a través de validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;C&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="s2">&#34;gamma&#34;</span><span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="mo">01</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">]}</span>
<span class="n">model_to_tune</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">model_to_tune</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>GridSearchCV(estimator=SVC(), n_jobs=-1,
             param_grid={'C': [0.1, 1, 10], 'gamma': [0.01, 0.1]})
</code></pre>
<p>Recordemos que, internamente, <code>GridSearchCV</code> entrena varios modelos para cada conjunto de entrenamiento submuestreado y los evalúa cada uno de ellos en los conjuntos de prueba correspondientes usando validación cruzada. Este procedimiento de evaluación se controla a través del parámetro <code>cv</code>. El proceso se repite para todas las posibles combinaciones de parámetros dados en <code>param_grid</code>.</p>
<p>El atributo <code>best_params_</code> proporciona el mejor conjunto de parámetros que maximizan la puntuación media en los conjuntos de prueba internos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Los mejores parámetros encontrados son: {search.best_params_}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Los mejores parámetros encontrados son: {'C': 0.1, 'gamma': 0.01}
</code></pre>
<p>También podemos mostrar la puntuación media obtenida usando los parámetros <code>best_params_</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La media de puntuación CV del mejor modelo es: {search.best_score_:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La media de puntuación CV del mejor modelo es: 0.627
</code></pre>
<p>En esta fase, debemos ser extremadamente cuidadosos al usar esta puntuación. La malinterpretación podría ser la siguiente: dado que esta puntuación media se ha calculado usando conjuntos de prueba de validación cruzada, podemos usarla para evaluar el rendimiento de generalización del modelo entrenado con los mejores hiperparámetros.</p>
<p>Sin embargo, no debemos olvidar que usamos esta puntuación para seleccionar el mejor modelo. Lo que significa que usamos el conocimiento de los conjuntos de prueba (es decir, las puntuaciones de prueba) para seleccionar los hiperparámetros del modelo en sí mismos.</p>
<p>Por lo tanto, esta puntuación media no es una estimación justa de nuestro error de prueba. De hecho, puede ser demasiado optimista, en particular cuando ejecutamos una búsqueda de parámetros de una lista grande con muchos hiperparámetros y muchos posibles valores de los mismos. Una forma de evitar este escollo es usar una validación cruzada anidada.</p>
<p>A continuación, usaremos una validación cruzada interna correspondiente al procedimiento anterior para optimizar únicamente los hiperparámetros. También incluiremos este procedimiento de tunning dentro de una validación cruzada externa, la cual se dedicará a estimar el error de prueba de nuestro modelo tuneado.</p>
<p>En este caso, nuestra validación cruzada interna siempre obtiene el conjunto de entrenamiento de la validación cruzada externa, lo que hace posible calcular siempre las puntuaciones de prueba finales en conjuntos de muestras completamente independientes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>

<span class="c1"># Declara las estrategias de validación cruzada interna y externa</span>
<span class="n">inner_cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">outer_cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Validación cruzada interna pra la búsqueda de parámetros</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">model_to_tune</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">inner_cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># Validación cruzada externa para calcular la puntuación de prueba</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">outer_cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;La puntuación media usando CV anidada es: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{test_score.mean():.3f} +/- {test_score.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>La puntuación media usando CV anidada es: 0.627 +/- 0.014
</code></pre>
<p>La puntuación reportada es más confiable y deber estar cerca del rendimiento de generalización esperado en producción. Hay que tener en cuenta que, en este caso, las dos puntuaciones son muy parecidas para este primer intento.</p>
<p>Nos gustaría mejorar la evaluación de la diferencia entre las puntuaciones de validación cruzada anidada y no anidada para mostrar que esta última puede ser demasiado optimista en la práctica. Para hacer esto, repetimos el experimento varias veces y mezclamos los datos de forma diferente para asegurar que nuestras conclusiones no dependen de un muestreo particular de los datos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">test_score_not_nested</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score_nested</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">N_TRIALS</span> <span class="o">=</span> <span class="mi">20</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_TRIALS</span><span class="p">):</span>
    <span class="c1"># Para cada intento, usamos divisiones de validación cruzada en</span>
    <span class="c1"># datos mezclados aleatoriamente de forma independiente pasando</span>
    <span class="c1"># distintos valores al parámetro random_state</span>
    <span class="n">inner_cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">outer_cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>

    <span class="c1"># Búsqueda de parámetros no anidada y puntuación</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model_to_tune</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
                         <span class="n">cv</span><span class="o">=</span><span class="n">inner_cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">test_score_not_nested</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>

    <span class="c1"># Validación cruzada anidada con optimización de parámetros</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">outer_cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_score_nested</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_score</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span></code></pre></div>
<p>Podemos unir los datos juntos y hacer un box plot de las dos estrategias.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">all_scores</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Not nested CV&#34;</span><span class="p">:</span> <span class="n">test_score_not_nested</span><span class="p">,</span>
    <span class="s2">&#34;Nested CV&#34;</span><span class="p">:</span> <span class="n">test_score_nested</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">all_scores</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">color</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;whiskers&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;medians&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;caps&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">}</span>
<span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Precisión&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Comparación de precisión media obtenida en el conjunto de prueba con</span><span class="se">\n</span><span class="s2">&#34;</span>
              <span class="s2">&#34;y sin validación cruzada anidada&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_136_0.png" alt="png"></p>
<p>Observamos que el rendimiento de generalización estimado sin usar CV anidada es mayor que el que obtenemos con CV anidada. La razón es que el procedimiento de tuneado en sí mismo selecciona el modelo con la mayor puntuación de CV interna. Si hay muchas combinaciones de hiperparámetros y si las puntuaciones de CV interna tienen comparativamente grandes desviaciones estándar, tomar el valor máximo puede atraer al científico de datos novato a sobreestimar el verdadero rendimiento de generalización del resultado del procedimiento de aprendizaje completo. Usar un procedimiento de validación cruzada externo proporciona una estimación más confiable del rendimiento de generalización en el procedimiento de aprendizaje completo, incluido el efecto de ajuste de hiperparámetros.</p>
<p>Como conclusión, cuando se optimizan partes de un pipeline de machine learning (por ejemplo, hiperparámetros, transformadores, etc.), se necesita usar validación cruzada anidada para evaluar el rendimiento de generalización del modelo predictivo. De lo contrario, los resultados obtenidos sin validación cruzada anidada suelen ser demasiado optimistas.</p>
<h1 id="métricas-de-clasificación"><a href="#métricas-de-clasificación" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Métricas de clasificación</h1>
<p>Los modelos de machine learning se basan en optimizar una función objetivo, buscando su mínimo o máximo. Es importante comprender que esta función objetivo suele estar desacoplada de la métrica de evaluación que queremos optimizar en la práctica. La función objetivo sirve como un proxy de la métrica de evaluación. Por tanto, vamos a presentar las diferentes métricas de evaluación usadas en machine learning.</p>
<p>Ahora vamos a proporcionar una panorámica de las métricas de clasificación que se pueden usar para evaluar el rendimiento de generalización de un modelo predictivo. Recordemos que en un problema de clasificación, el vector <code>objetivo</code> es categórico, en lugar de continuo.</p>
<p>Cargaremos el dataset de trasfusiones de sangre.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">blood_transfusion</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/blood_transfusion.csv&#34;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">blood_transfusion</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&#34;Class&#34;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">blood_transfusion</span><span class="p">[</span><span class="s2">&#34;Class&#34;</span><span class="p">]</span></code></pre></div>
<p>Comencemos comprobando las clases presentes en el vector objetivo <code>y</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">y</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Número de muestras&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Número de muestras por clases presentes</span><span class="se">\n</span><span class="s2"> en el objetivo&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_142_1.png" alt="png"></p>
<p>Podemos ver que el vector <code>y</code> contiene dos clases correspondientes a si una persona donó sangre. Usaremos un clasificador de regresión logística para predecir este resultado.</p>
<p>Para centrarnos en la presentación de métricas, solo usaremos una única división en lugar de validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span></code></pre></div>
<p>Usaremos una regresión logística como modelo de base. Entrenaremos el modelo en el conjunto de entrenamiento y, después, usaremos el conjunto de prueba para calcular las diferentes métricas de clasificación.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></div>
<pre><code>LogisticRegression()
</code></pre>
<h2 id="predicciones-del-clasificador"><a href="#predicciones-del-clasificador" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Predicciones del clasificador</h2>
<p>Antes de entrar en detalles respecto a las métricas, recordemos qué tipo de predicciones puede proporcionar un clasificador.</p>
<p>Por esta razón, crearemos una muestra sintética para un nuevo donante potencial: él/ella donó sangre dos veces en el pasado (1000 cc cada vez). La última vez fue hace 6 meses y la primera hace 20 meses.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">new_donor</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&#34;Recency&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span>
        <span class="s2">&#34;Frequency&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="s2">&#34;Monetary&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1000</span><span class="p">],</span>
        <span class="s2">&#34;Time&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span></code></pre></div>
<p>Podemos obtener la clase predicha por el clasificador llamando al método <code>predict</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_donor</span><span class="p">)</span></code></pre></div>
<pre><code>array(['not donated'], dtype=object)
</code></pre>
<p>Con esta información, nuestro clasificador predice que este sujeto sintético es más probable que no done sangre otra vez.</p>
<p>Sin embargo, no podemos comprobar que la predicción es correcta (no sabemos el valor objetivo verdadero). Este es el propósito del conjunto de prueba. Primero, predecimos si un sujeto donará sangre con la ayuda del clasificador entrenado.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span></code></pre></div>
<pre><code>array(['not donated', 'not donated', 'not donated', 'not donated',
       'donated'], dtype=object)
</code></pre>
<h2 id="precisión-como-línea-base"><a href="#precisión-como-línea-base" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Precisión como línea base</h2>
<p>Ahora que tenemos estas predicciones, podemos compararlas con las predicciones reales (también denominadas verdaderas) que no usamos hasta ahora.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span></code></pre></div>
<pre><code>258     True
521    False
14     False
31     False
505     True
       ...  
665     True
100    False
422     True
615     True
743     True
Name: Class, Length: 374, dtype: bool
</code></pre>
<p>En la comparación anterior, <code>True</code> significa que el valor predicho por nuestra clasificador es idéntico al valor real, mientras que <code>False</code> significa que nuestro clasificador ha cometido un error. Una forma de obtener una tasa general que represente el rendimiento de generalización de nuestro clasificador podría ser calcular cuántas veces es correcto nuestro clasificador dividido por el número de muestras de nuestro conjunto.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;{np.mean(y_test == y_pred):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>0.778
</code></pre>
<p>Esta medida se denomina precisión (<em>accuracy</em>). En este caso, nuestro clasificador tiene un 78% de precisión al clasificar si un sujeto donará sangre. <code>scikit-learn</code> provee una función que calcula esta métrica en el módulo <code>sklearn.metrics</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Accuracy: {accuracy:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Accuracy: 0.778
</code></pre>
<p><code>LogisticRegression</code> también tiene un método denominado <code>score</code> (que es parte de la API estándar de scikit-learn) que calcula la puntuación de accuracy.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Accuracy: {accuracy:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Accuracy: 0.778
</code></pre>
<h2 id="matriz-de-confusión-y-métricas-asociadas"><a href="#matriz-de-confusión-y-métricas-asociadas" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Matriz de confusión y métricas asociadas</h2>
<p>La comparación que hicimos anteriormente y la precisión que calculamos no tienen en cuenta el tipo de error que nuestro clasificador está cometiendo. La accuracy es una agregación de los errores cometidos por el clasificador. Es posible que nos interese una granularidad, saber de forma independiente si los errores son por los siguientes casos:</p>
<ul>
<li>predecimos que una persona donará sangre pero no lo hace;</li>
<li>predecimos que una persona no donará sangre pero lo hace.</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_165_0.png" alt="png"></p>
<p>Los números en diagonal se refieren a las predicciones que fueron correctas, mientras que los números fuera de la diagonal se refieren a las predicciones incorrectas (mal clasisificadas). Ahora conocemos los cuatro tipos de predicciones correctas y erróneas:</p>
<ul>
<li>la esquina superior izquierda son los verdaderos positivos (TP - <em>true positives</em>) y corresponden a las personas que donaron sangre y se predijeron como tal por el clasificador;</li>
<li>la esquina inferior derecha son los verdaderos negativos (TN - <em>true negatives</em>) y corresponden a las personas que no donaron sangre y se predijeron como tal por el clasificador;</li>
<li>la esquina superior derecha se corresponde con los falsos negativos (FN - <em>false negatives</em>) y corresponden a las personas que donaron sangre pero se predijo que no habían donado sangre;</li>
<li>la esquina inferior izquierda son los falsos positivos (FP - <em>false positives</em>) y corresponden a las personas que no donaron sangre pero se predijo que sí lo hicieron.</li>
</ul>
<p>Una vez tenemos dividida esta información, podemos calcular métricas para destacar el rendimiento de generalización de nuestro clasificador en una configuración particular. Por ejemplo, podríamos estar interesados en la fracción de personas que realmente donaron sangre cuando el clasificador lo predijo así o en la fracción de personas que se predijo que habrían donado sangre de la población total que realmente lo hizo.</p>
<p>La métrica anterior, conocida como precisión (<em>precision</em>), se define como TP / (TP + FP) y representa la probabilidad de que la persona realmente haya donado sangre cuando el clasificador predijo que lo haría. El último, conocido como sensibilidad (<em>recall</em>), se define como TP / (TP + FN) y evalúa cómo de bien el clasificador es capaz de identificar correctamente a las personas que donaron sangre. Al igual que con la accuracy, podríamos calcular estos valores. Sin embargo, scikit-learn proporciona funciones para calcular estos estadísticos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span>

<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&#34;donated&#34;</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&#34;donated&#34;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación precisión: {precision:.3f}&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación sensibilidad: {recall:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación precisión: 0.688
Puntuación sensibilidad: 0.124
</code></pre>
<p>Estos resultados están en línea con que hemos visto en la matriz de confusión. Mirando en la columna de la izquierda, más de la mitad de las predicciones &ldquo;donated&rdquo; fueron correctas, lo que nos lleva a una precisión superior al 0.5. Sin embargo, nuestro clasificador etiquetó erroneamente a muchas personas que donaron sangre como &ldquo;not donated&rdquo;, lo que nos lleva a una baja sensibilidad de aproximadamente 0.1.</p>
<h2 id="el-problema-del-desequilibrio-de-clases"><a href="#el-problema-del-desequilibrio-de-clases" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>El problema del desequilibrio de clases</h2>
<p>En esta fase, podríamos plantearnos una pregunta razonable. Si bien la accuracy no parecía mala (es decir, 77%), la sensibilidad es relativamente baja (es decir, 12%). Como mencionábamos, la precisión y la sensibilidad solo se centran en muestras predichas como positivas. Además, no observamos la proporción de clases. Podríamos comprobar esta proporción en el conjunto de entrenamiento.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_train</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">barh</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Frecuencia de clase&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Frecuencia de clase en el conjunto de entrenamiento&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_171_0.png" alt="png"></p>
<p>Observamos que la clase positiva, <code>donated</code>, comprende únicamente el 24% de las muestras. La buena accuracy de nuestro clasificador esta ligada entonces a su habilidad de predecir correctamente la clase negativa <code>not donated</code>, que puede o no ser relevante, dependiendo de la aplicación. Podemos ilustrar este problema usando un clasificado dummy como línea base.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>

<span class="n">dummy_classifier</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;most_frequent&#34;</span><span class="p">)</span>
<span class="n">dummy_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Accuracy del clasificador dummy: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{dummy_classifier.score(X_test, y_test):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Accuracy del clasificador dummy: 0.762
</code></pre>
<p>Con el clasificador dummy, que siempre predice la clase más frecuente, en nuestro caso la clase <code>not donated</code>, obtenemos un accuracy del 76%. Por tanto, significa que este clasificador, sin aprender nada de los datos <code>X</code>, es capaz de predecir más precisamente que nuestro modelo de regresión logística.</p>
<p>Este problema es también conocido como el problema de desequilibrio de clases. Cuando las clases están desbalanceadas, no se debe usar accuracy. En este caso, debemos usar la precisión y la sensibilidad como presentamos anteriormente o la puntuación accuracy equilibrada en lugar de la accuracy.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">balanced_accuracy_score</span>

<span class="n">balanced_accuracy</span> <span class="o">=</span> <span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Accuracy balanceada: {balanced_accuracy:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Accuracy balanceada: 0.553
</code></pre>
<p>La accuracy balanceada es equivalente a la accuracy en un contexto de clases equilibradas. Se define como la sensibilidad media obtenida en cada clase.</p>
<h1 id="evaluación-y-diferentes-umbrales-de-probabilidad"><a href="#evaluación-y-diferentes-umbrales-de-probabilidad" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Evaluación y diferentes umbrales de probabilidad</h1>
<p>Todas las estadísticas que hemos presentado hasta ahora se basan en <code>classifier.predict</code> que devuelve la etiqueta más probable. No hemos hecho uso de la probabilidad asociada con esta predicción, la cual proporciona la confianza del clasificador en esta predicción. Por defecto, la predicción de un clasificador corresponde a un umbral de 0.5 de probabilidad en un problema de clasificación binaria. Podemos comprobar rápidamente esta relación con el clasificador que entrenamos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">target_proba_predicted</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span>
                                      <span class="n">columns</span><span class="o">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">target_proba_predicted</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>donated</th>
      <th>not donated</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.271820</td>
      <td>0.728180</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.451764</td>
      <td>0.548236</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.445211</td>
      <td>0.554789</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.441577</td>
      <td>0.558423</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.870583</td>
      <td>0.129417</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span></code></pre></div>
<pre><code>array(['not donated', 'not donated', 'not donated', 'not donated',
       'donated'], dtype=object)
</code></pre>
<p>Dado que las probabilidades suman 1, podemos obtener la clase con la mayor probabilidad sin usar el umbral de 0.5.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">equivalence_pred_proba</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">target_proba_predicted</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">equivalence_pred_proba</span><span class="p">)</span></code></pre></div>
<pre><code>True
</code></pre>
<p>El umbral de decisión predeterminado (0.5) puede no ser el mejor umbral que conduce al rendimiento de generalización óptimo de nuestro clasificador. En este caso, podemos variar el umbral de decisión y, por lo tanto, la predicción subsiguiente y calcular las mismas estadísticas presentadas anteriormente. Normalmente, las dos métricas, sensibilidad y precisión, son calculadas y dibujadas en un gráfico. Cada métrica dibujada en un eje del gráfico y cada punto del gráfico corresponde a un umbral de decisión específico. Empecemos calculando la curva <em>precision-recall</em>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">PrecisionRecallDisplay</span>

<span class="n">disp</span> <span class="o">=</span> <span class="n">PrecisionRecallDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">classifier</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;donated&#39;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&#34;+&#34;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Curva precision-recall&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_184_0.png" alt="png"></p>
<p>En esta curva, cada cruz azul corresponde a un nivel de probabilidad que usamos como umbral de decisión. Podemos ver que variando este umbral de decisión obtenemos diferentes valores de precisión vs sensibilidad.</p>
<p>Un clasificador perfecto tendría una precisión de 1 para todos los valores de sensibilidad. Una métrica que caracteriza la curva está referida al área bajo la curva (AUC - <em>area under the curve</em>) y se denomina precisión media (AP). Con un clasificador ideal, la precisión media sería 1.</p>
<p>Las métricas de precisión y sensibilidad se centran en la clase positiva, sin embargo, podríamos estar interesados en el compromiso entre discriminar con precisión la clase positiva y discriminar con precisión las clases negativas. Las estadísticas usadas para esto son la sensibilidad y la especificidad. La especificidad mide la proporción de muestras clasificadas correctamente en la clase negativa y se define como: TN / (TN + FP). De forma similar a la curva precisión-sensibilidad, la sensibilidad y la especificidad se dibujan generalmente con una curva denominada ROC (<em>receiver operating characteristic</em>). Esta sería la curva ROC:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">RocCurveDisplay</span>

<span class="n">disp</span> <span class="o">=</span> <span class="n">RocCurveDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">classifier</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;donated&#39;</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="s2">&#34;+&#34;</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">RocCurveDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">dummy_classifier</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;donated&#39;</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Curva ROC AUC&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_186_0.png" alt="png"></p>
<p>Esta curva se construyó usando el mismo principio que la curva precisión-sensibilidad: variamos el umbral de probabilidad para determinar la predicción &ldquo;dura&rdquo; y calculamos las métricas. Al igual que la curva precisión-sensibilidad, podemos calcular el área bajo la ROC (ROC-AUC) para caracterizar el rendimiento de generalización de nuestro clasificador. Sin embargo, es importante observar que el límite inferior de ROC-AUC es 0.5. De hecho, mostramos el rendimiento de generalización de un clasificador dummy (la linea discontinua naranja) para mostrar que incluso el peor rendimiento de generalización obtenido estará por encima de esta línea.</p>
<h2 id="ejercicio-métricas-de-clasificación"><a href="#ejercicio-métricas-de-clasificación" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio métricas de clasificación</h2>
<p>Anteriormente hemos presentado diferentes métricas de clasificación pero no las usamos con validación cruzada. En este ejercicio practicaremos e implementaremos validación cruzada.</p>
<p>Volveremos a usar el dataset de transfusiones de sangre.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">blood_transfusion</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/blood_transfusion.csv&#34;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">blood_transfusion</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&#34;Class&#34;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">blood_transfusion</span><span class="p">[</span><span class="s2">&#34;Class&#34;</span><span class="p">]</span></code></pre></div>
<p>En primer lugar, vamos a crear un árbol de decisión clasificador.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span></code></pre></div>
<p>Vamos a crear un objeto de validación cruzada <code>StratifiedKFold</code>. Después lo usaremos dentro de la función <code>cross_val_score</code> para evaluar el árbol de decisión. Primero usaremos la accuracy para evaluar el árbol de decisión. Usaremos explícitamente el parámetro <code>scoring</code> de <code>cross_val_score</code> para calcula la accuracy (aunque sea ésta la puntuación por defecto). Compruebe su documentación para aprender cómo hacerlo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;accuracy&#34;</span><span class="p">,</span>
                          <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación accuracy: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores.mean():.3f} +/- {scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación accuracy: 0.631 +/- 0.141
</code></pre>
<p>Repetiremos el experimento calculando la <code>balanced_accuracy</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_balanced_accuracy</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;balanced_accuracy&#34;</span><span class="p">,</span>
                          <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación accuracy balanceada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_balanced_accuracy.mean():.3f} +/- {scores_balanced_accuracy.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación accuracy balanceada: 0.503 +/- 0.114
</code></pre>
<p>Ahora añadiremos un poco de complejidad. Queremos calcular la precisión de nuestro modelo. Sin embargo, anteriormente vimos que necesitamos mencionar la etiqueta positiva, que en nuestro caso consideramos que es la clase <code>donated</code>.</p>
<p>Mostraremos que calcular la precisión sin suministrar la clase positiva no es soportado por scikit-learn porque, de hecho, es ambigüo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">try</span><span class="p">:</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;precision&#34;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">exc</span><span class="p">)</span></code></pre></div>
<pre><code>C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\model_selection\_validation.py&quot;, line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_scorer.py&quot;, line 264, in _score
    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1757, in precision_score
    p, _, _, _ = precision_recall_fscore_support(
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1544, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File &quot;C:\Program Files\Python310\lib\site-packages\sklearn\metrics\_classification.py&quot;, line 1356, in _check_set_wise_labels
    raise ValueError(
ValueError: pos_label=1 is not a valid label. It should be one of ['donated', 'not donated']

  warnings.warn(
</code></pre>
<p>Obtenemos una excepción, porque el scorer por defecto tiene su etiqueta positiva marcada como uno (<code>pos_label=1</code>), que no es nuestro caso (nuestra etiqueta positiva es <code>donated</code>). En este caso, necesitamos crear un scorer usando la función scoring y la función helper <code>make_scorer</code>.</p>
<p>Por tanto, importamos <code>sklearn.metrics.make_scorer</code> y <code>sklearn.metrics.precision_score</code>. Consulta la documentación para más información. Finalmente, creamos un scorer llamando a <code>make_scorer</code> usando la función de puntuación <code>precision_score</code> y pasándole el parámetro extra <code>pos_label=&quot;donated&quot;</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span>  <span class="n">make_scorer</span><span class="p">,</span> <span class="n">precision_score</span>

<span class="n">precision</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">precision_score</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&#34;donated&#34;</span><span class="p">)</span></code></pre></div>
<p>Ahora, en lugar de suministrar la cadena <code>&quot;precision&quot;</code> al parámetro <code>scoring</code> en la llamada <code>cross_val_score</code>, le pasamos el scorer que acabamos de crear.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_precision</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">precision</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación precisión: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_precision.mean():.3f} +/- {scores_precision.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación precisión: 0.246 +/- 0.170
</code></pre>
<p><code>cross_val_score</code> solo calculará un única puntuación proporcionada al parámetro <code>scoring</code>. La función <code>cross_validate</code> permite el cálculo de múltiples puntuaciones pasándole una lista de cadenas o scorer al parámetro <code>scoring</code>, lo que podría ser útil.</p>
<p>Importaremos <code>sklearn.model_selection.cross_validate</code> y calcularemos la accuracy y la accuracy balanceada a través de validación cruzada. Dibujaremos la puntuación de validación cruzada para ambas métricas usando un box plot.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;accuracy&#34;</span><span class="p">,</span> <span class="s2">&#34;balanced_accuracy&#34;</span><span class="p">],</span>
                         <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores</span></code></pre></div>
<pre><code>{'fit_time': array([0.00250149, 0.00250244, 0.00300288, 0.00250196, 0.00250196,
        0.00250149, 0.00350356, 0.00350428, 0.0035038 , 0.00400329]),
 'score_time': array([0.00200152, 0.00250149, 0.00450349, 0.00200224, 0.00250244,
        0.00250363, 0.00300264, 0.00200105, 0.00250173, 0.002002  ]),
 'test_accuracy': array([0.29333333, 0.53333333, 0.77333333, 0.56      , 0.58666667,
        0.68      , 0.68      , 0.78666667, 0.64864865, 0.74324324]),
 'test_balanced_accuracy': array([0.42105263, 0.48391813, 0.66081871, 0.40643275, 0.42397661,
        0.44736842, 0.54239766, 0.74561404, 0.4623323 , 0.50309598])}
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación accuracy: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores[&#39;test_accuracy&#39;].mean():.3f} +/- {scores[&#39;test_accuracy&#39;].std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación accuracy: 0.629 +/- 0.139
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación accuracy balanceada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores[&#39;test_balanced_accuracy&#39;].mean():.3f} +/- {scores[&#39;test_balanced_accuracy&#39;].std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación accuracy balanceada: 0.510 +/- 0.106
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">all_scores</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Accuracy&#34;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_accuracy&#39;</span><span class="p">],</span>
    <span class="s2">&#34;Accucary balanceada&#34;</span><span class="p">:</span> <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_balanced_accuracy&#39;</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">all_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">all_scores</span><span class="p">)</span>

<span class="n">color</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;whiskers&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;medians&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;caps&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">}</span>
<span class="n">all_scores</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Puntuación&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Comparación accuracy vs accuracy balanceada&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_208_0.png" alt="png"></p>
<h1 id="métricas-de-regresión"><a href="#métricas-de-regresión" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Métricas de regresión</h1>
<p>Vamos a presentar las métricas que pueden usarse en regresión. Existen un conjunto de métricas dedicadas a la regresión. De hecho, las métricas de clasificación no pueden usarse para evaluar el rendimiento de generalización de modelos de regresión porque hay una diferencia fundamental entre sus tipos de objetivos: es una variable continua en el caso de la regresión, mientras que en el caso de la clasificación es una variable discreta.</p>
<p>Usaremos el dataset de viviendas de Ames. El objetivo es predecir el precio de las propiedades en la ciudad de Ames, Iowa. Al igual que con la clasificación, solo usaremos una única división entrenamiento-prueba para enfocarnos únicamente en las métricas de regresión.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">ames_housing</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/house_prices.csv&#34;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ames_housing</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ames_housing</span><span class="p">[</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
<span class="n">y</span> <span class="o">/=</span> <span class="mi">1000</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span></code></pre></div>
<p>Algunos modelos de machine learning están diseñados para resolverse como un problema de optimización: minimizan un error (también conocida como función de pérdida) usando un conjunto de entrenamiento. Una función de pérdida básica que se usa en regresión es el error cuadrático medio (MSE). Por lo tanto, esta métrica se usa a veces para evaluar el modelo dado que ya está optimizada por este modelo.</p>
<p>Veamos un ejemplo usando un modelo de regresión lineal.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MSE en el conjunto de entrenamiento: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_squared_error(y_train, y_pred):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MSE en el conjunto de entrenamiento: 996.902
</code></pre>
<p>Nuestro modelo de regresión lineal está minimizando el error cuadrático medio en el conjunto de entrenamiento. Significa que no existe otro conjunto de coeficientes que reduzcan más el error.</p>
<p>Vamos a calcular el MSE en el conjunto de prueba.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MSE en el conjunto de prueba: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_squared_error(y_test, y_pred):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MSE en el conjunto de prueba: 2064.736
</code></pre>
<p>El MSE en bruto puede ser difícil de interpretar. Una forma es reescalar el MSE por la varianza del objetivo. Esta puntuación se conoce como $R^2$, también conocida como coeficiente de determinación. De hecho, esta es la puntuación por defecto que usa scikit-learn cuando se llama al método <code>score</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">regressor</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2 en el conjunto de prueba: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{regressor.score(X_test, y_test):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2 en el conjunto de prueba: 0.687
</code></pre>
<p>La puntuación $R^2$ representa la proporción de varianza del objetivo que es explicada por las variables independientes del modelo. La mejor puntuación posible es 1 pero no existe límite inferior. Sin embargo, un modelo que predice el valor esperado del objetivo obtendría una puntación de 0.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyRegressor</span>

<span class="n">dummy_regressor</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;mean&#34;</span><span class="p">)</span>
<span class="n">dummy_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2 para un regresor que predice la media:&#34;</span>
      <span class="n">f</span><span class="s2">&#34;{dummy_regressor.score(X_test, y_test):.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2 para un regresor que predice la media:-0.000
</code></pre>
<p>La puntuación $R^2$ nos da ideas de la calidad del ajuste del modelo. Sin embargo, esta puntuación no se puede comparar de un dataset a otro y el valor obtenido no tiene una interpretación significativa en relación a la unidad original del objetivo. Si buscamos obtener una puntuación interpretable, nos interesaría la mediana o el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test, y_pred):.3f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE: 22.608 k$
</code></pre>
<p>Al calcular el error absoluto medio, podemos interpretar que nuestro modelo está prediciendo en promedio con un márgen de 22.6 k$ respecto al valor verdadero de la propiedad. Una desventaja de esta métrica es que la media se puede ver impactada por errores grandes. Para algunas aplicaciones, es posible que no queramos que estos grandes errores tengan una gran influencia en nuestra métrica. En este caso, podemos usar el error absoluto mediano.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">median_absolute_error</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto mediano: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{median_absolute_error(y_test, y_pred):.3f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Error absoluto mediano: 14.137 k$
</code></pre>
<p>El error absoluto medio (o el error absoluto mediano) aún sigue teniendo una limitación conocida: cometer un error de 50 k dólares en una casa valorada en 50 k dólares tiene el mismo impacto que cometerlo en una casa de 500 k dólares. De hecho, el error absoluto medio no es relativo.</p>
<p>El error porcentual absoluto medio introduce este escalado relativo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_percentage_error</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error porcentual absoluto medio: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_percentage_error(y_test, y_pred) * 100:.3f} %&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Error porcentual absoluto medio: 13.574 %
</code></pre>
<p>Además de métricas, podemos representar visualmente los resultados dibujando los valores predichos vs los valores reales.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">predicted_actual</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Valores reales (k$)&#34;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="s2">&#34;Valores predichos (k$)&#34;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">}</span>
<span class="n">predicted_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_actual</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">predicted_actual</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="s2">&#34;Valores reales (k$)&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&#34;Valores predichos (k$)&#34;</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Perfect fit&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Regresión usando un modelo sin </span><span class="se">\n</span><span class="s2">transformación de objetivo&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_229_0.png" alt="png"></p>
<p>En este gráfico, las predicciones correctas estarían en la línea diagonal. Este gráfico permite detectar si el modelo comete errores de forma consistente, es decir, si tiene algún sesgo (bias).</p>
<p>En este gráfico vemos que para los valores reales altos, nuestro modelo tiende a subestimar el precio de la propiedad. Normalmente, este problema surge cuando el objetivo a predecir no sigue una distribución normal. En este caso, el modelo se podría beneficiar de la trasformación de objetivo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">QuantileTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">TransformedTargetRegressor</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="n">QuantileTransformer</span><span class="p">(</span>
    <span class="n">n_quantiles</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span> <span class="n">output_distribution</span><span class="o">=</span><span class="s2">&#34;normal&#34;</span><span class="p">)</span>
<span class="n">model_transformed_target</span> <span class="o">=</span> <span class="n">TransformedTargetRegressor</span><span class="p">(</span>
    <span class="n">regressor</span><span class="o">=</span><span class="n">regressor</span><span class="p">,</span> <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">)</span>
<span class="n">model_transformed_target</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_transformed_target</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">predicted_actual</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Valores reales (k$)&#34;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="s2">&#34;Valores predichos (k$)&#34;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">}</span>
<span class="n">predicted_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_actual</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">predicted_actual</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="s2">&#34;Valores reales (k$)&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&#34;Valores predichos (k$)&#34;</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Perfect fit&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Regresión usando un modelo que </span><span class="se">\n</span><span class="s2">transforma el objetivo antes de entrenar&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_233_0.png" alt="png"></p>
<p>Así, vemos que una vez trasformado el objetivo, vemos que corregimos algunos de los valores altos.</p>
<h2 id="ejercicio-de-métricas-de-regresión"><a href="#ejercicio-de-métricas-de-regresión" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio de métricas de regresión</h2>
<p>Vamos a evaluar las métricas de regresión dentro de una validación cruzada para familiarizarnos con la sintaxis. Usaremos el dataset de viviendas de Ames.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ames_housing</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/house_prices.csv&#34;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ames_housing</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ames_housing</span><span class="p">[</span><span class="s2">&#34;SalePrice&#34;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">)</span>
<span class="n">y</span> <span class="o">/=</span> <span class="mi">1000</span></code></pre></div>
<p>El primer paso es crear un modelo de regresión lineal.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span></code></pre></div>
<p>Después, usaremos <code>cross_val_score</code> para estimar el rendimiento de generalización del modelo. Usaremos una validación cruzada <code>KFold</code> con 10 particiones. Haremos uso de la puntuación $R^2$ asignando explícitamente el parámetro <code>scoring</code> (a pesar de que es la puntuación por defecto).</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;r2&#34;</span><span class="p">,</span>
                         <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2: {scores.mean():.3f} +/- {scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2: 0.794 +/- 0.103
</code></pre>
<p>Ahora, en lugar de usar la puntuación $R^2$, usaremos el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
                         <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: {-scores.mean():.3f} +/- {-scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE: 21.892 +/- -2.225
</code></pre>
<p>En scikit-learn, el parámetro <code>scoring</code> espera puntuaciones. Esto significa que cuanto mayor sean los valores y menores sean los errores, mejor será el modelo. Por lo tanto, el error debe multiplicarse por -1. Esta es la causa del prefijo <code>neg_</code> de la cadena del scoring cuando tratamos con métricas que son errores.</p>
<p>Por último, usaremos la función <code>cross_validate</code> y calcularemos múltiples puntuaciones/errores a la vez pasándole una lista de marcadores al parámetros <code>scoring</code>. Calcularemos la puntuación $R^2$ y el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;r2&#34;</span><span class="p">,</span> <span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">],</span>
                        <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2: {scores[&#39;test_r2&#39;].mean():.3f} +/- {scores[&#39;test_r2&#39;].std():.3f}&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: {-scores[&#39;test_neg_mean_absolute_error&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-scores[&#39;test_neg_mean_absolute_error&#39;].std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2: 0.794 +/- 0.103
MAE: 21.892 +/- -2.225
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores</span></code></pre></div>
<pre><code>{'fit_time': array([0.00350285, 0.00300288, 0.00350332, 0.00350285, 0.00350308,
        0.00350261, 0.00400424, 0.00350285, 0.00300217, 0.00300312]),
 'score_time': array([0.00200248, 0.0025022 , 0.00200105, 0.00250173, 0.00200129,
        0.00200224, 0.00200129, 0.00200176, 0.00150156, 0.00150156]),
 'test_r2': array([0.84390289, 0.85497435, 0.88752303, 0.74951104, 0.81698014,
        0.82013355, 0.81554085, 0.81452472, 0.50115778, 0.83330693]),
 'test_neg_mean_absolute_error': array([-20.48049905, -21.38003105, -21.26831487, -22.86887664,
        -24.79955736, -18.95827641, -20.11793792, -20.5040172 ,
        -26.76774564, -21.77871056])}
</code></pre>
<h1 id="ejercicio-1"><a href="#ejercicio-1" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio</h1>
<p>Vamos a poner en práctica lo aprendido en este post con un ejercicio. Para ello usaremos el dataset de bicis.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">cycling</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/bike_rides.csv&#34;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                      <span class="n">parse_dates</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">cycling</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
<span class="n">target_name</span> <span class="o">=</span> <span class="s2">&#34;power&#34;</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cycling</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">target_name</span><span class="p">),</span> <span class="n">cycling</span><span class="p">[</span><span class="n">target_name</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>heart-rate</th>
      <th>cadence</th>
      <th>speed</th>
      <th>acceleration</th>
      <th>slope</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-08-18 14:43:19</th>
      <td>102.0</td>
      <td>64.0</td>
      <td>4.325</td>
      <td>0.0880</td>
      <td>-0.033870</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:20</th>
      <td>103.0</td>
      <td>64.0</td>
      <td>4.336</td>
      <td>0.0842</td>
      <td>-0.033571</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:21</th>
      <td>105.0</td>
      <td>66.0</td>
      <td>4.409</td>
      <td>0.0234</td>
      <td>-0.033223</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:22</th>
      <td>106.0</td>
      <td>66.0</td>
      <td>4.445</td>
      <td>0.0016</td>
      <td>-0.032908</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:23</th>
      <td>106.0</td>
      <td>67.0</td>
      <td>4.441</td>
      <td>0.1144</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>En este dataset, el problema es intentar predecir la potencia de un ciclista usando sensores baratos (GPS, monitores de frecuencia cardíaca, etc.). De hecho, la potencia se puede registrar a través de medidores ciclistas de potencia, pero dichos dispositivos suelen ser bastante caros.</p>
<p>En lugar de usar machine learning a ciegas, primero presentaremos un poco de mecánica clásica: la segunda ley de Newton.</p>
<p>$P_{meca} = (\frac{1}{2} \rho . SC_x . V_{a}^{2} + C_r . mg . \cos \alpha + mg . \sin \alpha + ma) V_d$</p>
<p>donde $\rho$ es la densidad del aire en kg.m$^{-3}$, $S$ es la superficie frontal del ciclista en m$^{2}$, $C_x$ es el coeficiente de resistencia, $V_a$ es la velocidad del aire en m.s$^{-1}$, $C_r$ es la resistencia a la rodadura, $m$ es la masa del ciclista y la bicicleta en kg, $g$ es la aceleración estándar debida a la gravedad, que es igual a  9.81 m.s$^{-2}$, $\alpha$ es la pendiente en radianes, $V_d$ es la velocidad del ciclista m.s$^{-1}$, y $a$ es la aceleración del ciclista en m.s$^{-2}$.</p>
<p>Al principio, esta ecuación podría parecer un poco compleja pero podemos explicar con palabras lo que significan los diferéntes términos dentro del paréntesis:</p>
<ul>
<li>el primer término es la potencia que se requiere que produzca un ciclista para luchar contra el viento</li>
<li>el segundo término es la potencia que se requiere que produzca un ciclista para luchar contrar la resistencia a la rodadura creada por los neumáticos en la pista</li>
<li>el tercer término es la potencia que se requiere que produzca un ciclista para subir una colina si la pendiente es positiva. Si la pendiente es negativa, el ciclista no necesita producir ninguna potencia para avanzar</li>
<li>el cuarto y último término es la potencia que se requiere que produzca un ciclista para cambiar su velocidad (es decir, aceleración).</li>
</ul>
<p>Podemos simplificar el modelo anterior usando los datos que tenemos a mano. Se vería como lo siguiente:</p>
<p>$P_{meca} = \beta_{1} V_{d}^{3} + \beta_{2} V_{d} + \beta_{3} \sin(\alpha) V_{d} + \beta_{4} a V_{d}$</p>
<p>Este modelo está más cerca de lo que vimos anteriormente: es un modelo lineal entrenado en una transformación de features no lineal. Construiremos, entrenaremos y evaluaremos un modelo de este tipo como parte del ejercicio. Por tanto, necesitaremos:</p>
<ul>
<li>crear una nueva matriz de datos conteniendo el cubo de la velocidad, la velocidad, la velocidad multiplicada por el seno del ángulo de la pendiente y la velocidad multiplicada por la aceleración. Para calcular el ángulo de la pendiente, necesitamos tomar el arcotangente de la pendiente (<code>alpha = np.arctan(slope)</code>). Además, podemos limitarnos a la aceleración positiva solo recortando a 0 los valores de aceleración negativa (correspondería a alguna potencia creada por el frenado que no estamos modelando aquí).</li>
<li>usando la nueva matriz de datos, crear un modelo predictivo lineal basado en un <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" target="_blank" rel="noopener"><code>sklearn.preprocessing.StandardScaler</code></a> y <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html" target="_blank" rel="noopener"><code>sklearn.linear_model.RidgeCV</code></a>.</li>
<li>usar un estrategia de validación cruzada <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html" target="_blank" rel="noopener"><code>sklearn.model_selection.ShuffleSplit</code></a> con solo 4 particiones (<code>n_splits=4</code>) para evaluar el rendimiento de generalización del modelo. Usaremos el error absoluto medio (MAE) como métrica del rendimiento de generalización. Además, pasaremos el parámetro <code>return_estimator=True</code> y <code>return_train_score=True</code>. Tengamos en cuenta que la estrategia <code>ShuffleSplit</code> es un estrategia ingenua y simple e investigaremos las consecuencias de esta elección más adelante.</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_3&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">3</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_x_sin_slope&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&#34;slope&#34;</span><span class="p">]))</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_x_accel&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;acceleration&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>heart-rate</th>
      <th>cadence</th>
      <th>speed</th>
      <th>acceleration</th>
      <th>slope</th>
      <th>speed_3</th>
      <th>speed_x_sin_slope</th>
      <th>speed_x_accel</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-08-18 14:43:19</th>
      <td>102.0</td>
      <td>64.0</td>
      <td>4.325</td>
      <td>0.0880</td>
      <td>-0.033870</td>
      <td>80.901828</td>
      <td>-0.146402</td>
      <td>0.380600</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:20</th>
      <td>103.0</td>
      <td>64.0</td>
      <td>4.336</td>
      <td>0.0842</td>
      <td>-0.033571</td>
      <td>81.520685</td>
      <td>-0.145482</td>
      <td>0.365091</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:21</th>
      <td>105.0</td>
      <td>66.0</td>
      <td>4.409</td>
      <td>0.0234</td>
      <td>-0.033223</td>
      <td>85.707790</td>
      <td>-0.146398</td>
      <td>0.103171</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:22</th>
      <td>106.0</td>
      <td>66.0</td>
      <td>4.445</td>
      <td>0.0016</td>
      <td>-0.032908</td>
      <td>87.824421</td>
      <td>-0.146198</td>
      <td>0.007112</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:23</th>
      <td>106.0</td>
      <td>67.0</td>
      <td>4.441</td>
      <td>0.1144</td>
      <td>0.000000</td>
      <td>87.587538</td>
      <td>0.000000</td>
      <td>0.508050</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;{X[&#39;speed_x_sin_slope&#39;].mean():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>-0.003
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;speed_3&#34;</span><span class="p">,</span> <span class="s2">&#34;speed&#34;</span><span class="p">,</span> <span class="s2">&#34;speed_x_sin_slope&#34;</span><span class="p">,</span> <span class="s2">&#34;speed_x_accel&#34;</span><span class="p">]</span>
<span class="n">X_linear_model</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">features</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X_linear_model</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>speed_3</th>
      <th>speed</th>
      <th>speed_x_sin_slope</th>
      <th>speed_x_accel</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-08-18 14:43:19</th>
      <td>80.901828</td>
      <td>4.325</td>
      <td>-0.146402</td>
      <td>0.380600</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:20</th>
      <td>81.520685</td>
      <td>4.336</td>
      <td>-0.145482</td>
      <td>0.365091</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:21</th>
      <td>85.707790</td>
      <td>4.409</td>
      <td>-0.146398</td>
      <td>0.103171</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:22</th>
      <td>87.824421</td>
      <td>4.445</td>
      <td>-0.146198</td>
      <td>0.007112</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:23</th>
      <td>87.587538</td>
      <td>4.441</td>
      <td>0.000000</td>
      <td>0.508050</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span><span class="p">,</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">RidgeCV</span><span class="p">())</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_linear_model</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
                           <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                           <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cv_scores</span></code></pre></div>
<pre><code>{'fit_time': array([0.02552152, 0.02652311, 0.02452087, 0.02452159]),
 'score_time': array([0.00150347, 0.002002  , 0.00200343, 0.00150061]),
 'estimator': [Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('ridgecv', RidgeCV(alphas=array([ 0.1,  1. , 10. ])))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('ridgecv', RidgeCV(alphas=array([ 0.1,  1. , 10. ])))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('ridgecv', RidgeCV(alphas=array([ 0.1,  1. , 10. ])))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('ridgecv', RidgeCV(alphas=array([ 0.1,  1. , 10. ])))])],
 'test_score': array([-73.23006461, -72.1311734 , -72.89061823, -71.2370263 ]),
 'train_score': array([-72.35634493, -72.51703894, -72.42974777, -72.6121094 ])}
</code></pre>
<p><strong>¿De media, cuál es el error absoluto medio en el conjunto de prueba obtenido en la validación cruzada?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE es: {-cv_scores[&#39;test_score&#39;].mean():.2f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_scores[&#39;test_score&#39;].std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE es: 72.37 +/- 0.77
</code></pre>
<p><strong>Muestra los coeficientes del modelo lineal resultado de la validación cruzada</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">cv_scores</span><span class="p">[</span><span class="s2">&#34;estimator&#34;</span><span class="p">]:</span>
   <span class="k">print</span><span class="p">(</span><span class="n">estimator</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span></code></pre></div>
<pre><code>[ 5.66427806 32.84904152 80.08105928 10.85618779]
[ 5.68897463 32.83434375 80.99005594 11.34425   ]
[ 6.28736152 32.19112942 80.92397865 11.23297157]
[ 6.18278112 32.4035743  80.61344415 11.27427489]
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">estimator</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">cv_scores</span><span class="p">[</span><span class="s2">&#34;estimator&#34;</span><span class="p">]]</span>
<span class="n">coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">coefs</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Distribución de pesos del modelo lineal&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_261_0.png" alt="png"></p>
<p>Todos los pesos son mayores que 0. No es una sorpresa dado que los coeficientes están relacionados con productos de cantidades físicas positivas tales como la masa del ciclista y la bicicleta, gravedad, densidad del aire, etc. De hecho, juntando la primera ecuación y el valor de $\beta_{S}$ esperaríamos una relación que podría ser cercana a:</p>
<ul>
<li>$\beta_{1} \frac{1}{2} \rho . SC_x$</li>
<li>$\beta_{2} C_r . mg$</li>
<li>$\beta_{3}  mg$</li>
<li>$\beta_{4}  ma$</li>
</ul>
<p>Esta relación también explicaría por qué podríamos esperar que  $\beta_{1} &lt; \beta_{2} &lt; \beta_{3}$. De hecho, $C_r$ es una constante pequeña, por lo que podríamos esperar que $\beta_{2} &lt; \beta_{3} . \rho . SC_x$ son valores muy pequeños en comparación con $ C_r$ o $ mg $ y uno podría esperar $\beta_{1} &lt; \beta_{2}$.</p>
<p>Ahora crearemos un modelo predictivo que use todos los datos, incluyendo las medidas de los sensores disponibles, tales como la cadencia (velocidad a la que un ciclista gira los pedales medida en rotaciones por minuto) y frecuencia cardíaca (número de pulsaciones por minuto del ciclista mientras durante el ejercicio). También, usaremos un regresor no lineal, un <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html" target="_blank" rel="noopener"><code>sklearn.ensemble.HistGradientBoostingRegressor</code></a>. Estableceremos el número máximo de iteraciones en 1000 (<code>max_iter=1_000</code>) y activaremos la parada temprana (<code>early_stopping=True</code>). Repetiremos la evaluación anterior usando este regresor.</p>
<p><strong>De media, ¿cuál es el error absoluto medio en el conjunto de prueba obtenido a través de la validación cruzada?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">cycling</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">target_name</span><span class="p">)</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_3&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">3</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_x_sin_slope&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s2">&#34;slope&#34;</span><span class="p">]))</span>
<span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed_x_accel&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;speed&#34;</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="s2">&#34;acceleration&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>heart-rate</th>
      <th>cadence</th>
      <th>speed</th>
      <th>acceleration</th>
      <th>slope</th>
      <th>speed_3</th>
      <th>speed_x_sin_slope</th>
      <th>speed_x_accel</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2020-08-18 14:43:19</th>
      <td>102.0</td>
      <td>64.0</td>
      <td>4.325</td>
      <td>0.0880</td>
      <td>-0.033870</td>
      <td>80.901828</td>
      <td>-0.146402</td>
      <td>0.380600</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:20</th>
      <td>103.0</td>
      <td>64.0</td>
      <td>4.336</td>
      <td>0.0842</td>
      <td>-0.033571</td>
      <td>81.520685</td>
      <td>-0.145482</td>
      <td>0.365091</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:21</th>
      <td>105.0</td>
      <td>66.0</td>
      <td>4.409</td>
      <td>0.0234</td>
      <td>-0.033223</td>
      <td>85.707790</td>
      <td>-0.146398</td>
      <td>0.103171</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:22</th>
      <td>106.0</td>
      <td>66.0</td>
      <td>4.445</td>
      <td>0.0016</td>
      <td>-0.032908</td>
      <td>87.824421</td>
      <td>-0.146198</td>
      <td>0.007112</td>
    </tr>
    <tr>
      <th>2020-08-18 14:43:23</th>
      <td>106.0</td>
      <td>67.0</td>
      <td>4.441</td>
      <td>0.1144</td>
      <td>0.000000</td>
      <td>87.587538</td>
      <td>0.000000</td>
      <td>0.508050</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                      <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span>
                                                    <span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
                           <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                           <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cv_scores</span></code></pre></div>
<pre><code>{'fit_time': array([1.1509881 , 1.16600084, 1.37017584, 0.85473323]),
 'score_time': array([0.04653955, 0.04804134, 0.05254531, 0.03503013]),
 'estimator': [Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('histgradientboostingregressor',
                   HistGradientBoostingRegressor(early_stopping=True,
                                                 max_iter=1000))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('histgradientboostingregressor',
                   HistGradientBoostingRegressor(early_stopping=True,
                                                 max_iter=1000))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('histgradientboostingregressor',
                   HistGradientBoostingRegressor(early_stopping=True,
                                                 max_iter=1000))]),
  Pipeline(steps=[('standardscaler', StandardScaler()),
                  ('histgradientboostingregressor',
                   HistGradientBoostingRegressor(early_stopping=True,
                                                 max_iter=1000))])],
 'test_score': array([-44.6157647 , -43.98685225, -43.78464147, -43.55871723]),
 'train_score': array([-40.40936232, -40.42975609, -39.87099469, -41.43133008])}
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE es: {-cv_scores[&#39;test_score&#39;].mean():.2f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_scores[&#39;test_score&#39;].std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE es: 43.99 +/- 0.39
</code></pre>
<p>De media, el MAE de este modelo en el conjunto de prueba es de ~44 vatios. Por lo tanto, parece que las features adicionales y el cambio de regresor tienen un impacto positivo en el rendimiento de generalización.</p>
<p><strong>Comparando el modelo lineal y el modelo histogram gradient boosting y teniendo en cuenta el MAE de entrenamiento y prueba obtenidos a través de validación cruzada, ¿qué podemos concluir?</strong></p>
<p>Revisando las puntuaciones de entrenamiento y prueba de cada uno de los modelos, podemos concluir que:</p>
<ul>
<li>el rendimiento de generalización del histogram gradient boosting está limitado por su overfitting. El error de prueba es mayor que el error de entrenamiento. Esto es síntoma de un modelo con overfitting.</li>
<li>el rendimiento de generalización del modelo lineal está limitado por su underfitting. El error de entrenamiento y prueba son muy parecidos. Sin embargo, los errores son mucho mayores que los del histogram gradient boosting. El modelo linean tiene claramente underfitting.</li>
</ul>
<p>En la validación cruzada anterior, optamos por la opción de usar una estrategia <code>ShuffleSplit</code> de validación cruzada. Lo que significa que las muestras seleccionadas aleatoriamente se seleccionaron como conjunto de prueba, ignorando cualquier dependencia entre las líneas del dataframe.</p>
<p>Nos gustaría tener una estrategia de validación cruzada que evalúe la capacidad del nuestro modelo para predecir un recorrido completamente nuevo en bicicleta: las muestras del conjunto de validación sólo deben provenir de recorridos no presentes en el conjunto de entrenamiento.</p>
<p><strong>¿Cuántos recorridos en bicicleta están almacenados en el dataframe?</strong> Ayuda: Podemos comprobar los días únicos en <code>DatetimeIndex</code> (el índice del dataframe). De hecho, podemos asumir que en un día dado, el ciclista salió como máximo una vez al día. También podemos acceder a la fecha y hora de un <code>DatatimeIndex</code> usando <code>df.index.date</code> y <code>df.index.time</code>, respectivamente.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Nº de recorridos: {len(set(X.index.date))}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Nº de recorridos: 4
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># otra forma</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Nº de recorridos: {len(np.unique(X.index.date))}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Nº de recorridos: 4
</code></pre>
<p>En lugar de usar una estrategia simple <code>SuffleSplit</code>, usaremos una estrategia que tenga en cuenta los grupos definidos por cada fecha individual. Corresponde a un recorrido en bicicleta. Nos gustaría tener una estrategia de validación cruzada que evalúe la capacidad de nuestro modelo para predecir en un recorrido completamente nuevo: las muestras en el conjunto de validación solo deben provenir de recorrides no presentes en el conjunto de entrenamiento. Por lo tanto, podemos usar una estrategia <code>LeaveOneGroupOut</code>: en cada iteración de validación cruzada mantendremos un recorrido para la evaluación y usaremos los otros recorridos para entrenar nuestro modelo.</p>
<p>Por tanto, necesitamos:</p>
<ul>
<li>crear una variable llamda <code>group</code> que es un array unidimensional de numpy que contiene los índices de cada recorrido presente en el dataframe. Por tanto, la longitud de <code>group</code> será igual al número de muestras del dataframe. Si tuviéramos 2 recoridos, esperaríamos los índices 0 y 1 en <code>group</code> para diferenciar los recorridos. Podemos usar <a href="https://pandas.pydata.org/docs/reference/api/pandas.factorize.html" target="_blank" rel="noopener"><code>pd.factorize</code></a> para codificar cualquier tipo de Python en índices enteros.</li>
<li>crear un objeto de validación cruzada llamado <code>cv</code> usando la estrategia <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" target="_blank" rel="noopener"><code>sklearn.model_selection.LeaveOneGroupOut</code></a>.</li>
<li>evaluar tanto el modelo lineal como el histogram gradient boosting con esta estrategia.</li>
</ul>
<p><strong>Usando esta evaluación y observando los errores de entrenamiento y prueba de ambos modelos, ¿qué podemos concluir?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneGroupOut</span>

<span class="c1">#groups = pd.factorize(np.unique(X.index.date))</span>
<span class="n">groups</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">date</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">LeaveOneGroupOut</span><span class="p">()</span>

<span class="n">model_linear</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">()</span>
<span class="n">cv_scores_lineal_regression</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">model_linear</span><span class="p">,</span> <span class="n">X_linear_model</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">model_hgbr</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">cv_scores_hgbr</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">model_hgbr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">-</span><span class="n">cv_scores_lineal_regression</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">],</span> <span class="o">-</span><span class="n">cv_scores_lineal_regression</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>(array([72.43262484, 72.28760557, 68.96581013, 75.31143178]),
 array([72.44057575, 73.32215829, 81.30511116, 64.9905063 ]))
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">-</span><span class="n">cv_scores_hgbr</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">],</span> <span class="o">-</span><span class="n">cv_scores_hgbr</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>(array([39.46262019, 40.26826242, 38.58251339, 40.43820514]),
 array([47.74637053, 47.94907269, 53.96158726, 47.46269937]))
</code></pre>
<p>Revisando las puntuaciones de entrenamiento y prueba de cada uno de los modelos, podemos concluir que observamos el mismo comporamiento que con la estrategia <code>ShuffleSplit</code>, es decir:</p>
<ul>
<li>el rendimiento de generalización del histogram gradient boosting está limitado por su overfitting.</li>
<li>el rendimiento de generalización del modelo lineal está limitado por su underfitting.</li>
</ul>
<p>También observamos que histogram gradient boosting tiene claramente más overfitting con la estrategia de validación cruzada <code>LeaveOneGroupOut</code>, ya que la diferencia en las puntaciones de entrenamiento y prueba es aún mayor que la medida con la estrategia <code>ShuffleSplit</code>.</p>
<p>Por lo tanto, incluso si el modelo lineal está modelando el problema físico real, es probable que adolezca de deficiencias impuestas por las mediciones faltantes (por ejemplo, velocidad del viento) y por la incertidumbre de algunas medidas (por ejemplo, errores de GPS). Como resultado, el modelo lineal no es tan preciso como el regresor histogram gradient boosting que tiene acceso a mediciones externas. La información adicional, tales como la frecuencia cardíaca y la cadencia, atenúan las anteriores deficiencias.</p>
<p>En este caso, no podemos comparar las puntuaciones de validación cruzada partición a partición, ya que las particiones no están alineadas (no han sido generadas con la misma estrategia). En su lugar, comparemos la media de los errores de prueba de validación cruzadda en las evaluaciones del model lineal. <strong>¿Qué podemos concluir respecto al error de prueba medio cuando usamos <code>ShuffleSplit</code> en comparación con <code>LeaveOneGroupOut</code>?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE es: {-cv_scores_lineal_regression[&#39;test_score&#39;].mean():.2f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_scores_lineal_regression[&#39;test_score&#39;].std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE es: 73.01 +/- 5.78
</code></pre>
<p>El MAE del modelo lineal con estrategia <code>ShuffleSplit</code> era de 72.37, luego la diferencia es mínima, es decir, ambas estrategias de validación cruzada son equivalentes.</p>
<p>Comparemos la media de los errores de prueba de validación cruzada en las evaluaciones del modelo gradient boosting. <strong>¿Qué podemos concluir respecto al error de prueba medio cuando usamos <code>ShuffleSplit</code> en comparación con <code>LeaveOneGroupOut</code>?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE es: {-cv_scores_hgbr[&#39;test_score&#39;].mean():.2f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_scores_hgbr[&#39;test_score&#39;].std():.2f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE es: 49.28 +/- 2.71
</code></pre>
<p>El MAE del modelo gradient boosting con estrategia <code>ShuffleSplit</code> era de 44.22, luego la diferencia es menor por más de 3 vatios, es decir,  <code>ShuffleSplit</code> proporciona resultados demasiado optimistas. La estrategia <code>LeaveOneGroupOut</code> está más próxima a la configuración real encontrada cuando se pone el modelo en producción, por lo que podemos concluir que el rendimiento de generalización proporcionado por la estrategia de validación cruzada <code>ShuffleSplit</code> es demasiado optimista al estimar un error de prueba medio menor que el que observaríamos en recorridos futuros.</p>
<p>Es interesante notar que no vemos este comportamiento en el modelo lineal. La razón es que nuestro modelo lineal tiene underfitting.</p>
<p>Ahora entraremos en más detalle seleccionando un único recorrido para la prueba y analizando las predicciones del modelo para este recorrido de prueba. Para hacer esto, podemos reutilizar el objeto de validación cruzada <code>LeaveOneGroupOut</code> de la siguiente forma:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv</span> <span class="o">=</span> <span class="n">LeaveOneGroupOut</span><span class="p">()</span>
<span class="n">train_indices</span><span class="p">,</span> <span class="n">test_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">X_linear_model_train</span> <span class="o">=</span> <span class="n">X_linear_model</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
<span class="n">X_linear_model_test</span> <span class="o">=</span> <span class="n">X_linear_model</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span></code></pre></div>
<p>Ahora, ajustaremos tanto el modelo lineal como el modelo de histogram gradient boosting en los datos de entrenamiento y recopilaremos las predicciones en los datos de prueba. Haremos un scatter plot donde en el eje x dibujaremos las potencias medidas (objetivo real) y en el eje y dibujaremos las potencias predichas (objetivo predicho). Haremos dos gráficos separados para cada modelo. <strong>¿Qué conclusiones podemos obtener?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">model_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_linear</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test, y_pred):.3f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE: 61.469 k$
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">predicted_actual</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Potencias reales (Vatios)&#34;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="s2">&#34;Potencias predichas (Vatios)&#34;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">}</span>
<span class="n">predicted_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_actual</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">predicted_actual</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="s2">&#34;Potencias reales (Vatios)&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&#34;Potencias predichas (Vatios)&#34;</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1">#plt.axline((0, 0), slope=1, label=&#34;Perfect fit&#34;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Perfect fit&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Regresión modelo lineal&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_288_0.png" alt="png"></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">model_hgbr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model_hgbr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test, y_pred):.3f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE: 47.833 k$
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">predicted_actual</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;Potencias reales (Vatios)&#34;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="s2">&#34;Potencias predichas (Vatios)&#34;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">}</span>
<span class="n">predicted_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_actual</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">predicted_actual</span><span class="p">,</span>
                <span class="n">x</span><span class="o">=</span><span class="s2">&#34;Potencias reales (Vatios)&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&#34;Potencias predichas (Vatios)&#34;</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1">#plt.axline((0, 0), slope=1, label=&#34;Perfect fit&#34;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">800</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Perfect fit&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Regresión model histogram gradient boosting&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_290_0.png" alt="png"></p>
<p>Comencemos mirando las muestras con potencias altas. Vemos que tanto el modelo lineal como el regresor histogram gradient boosting predicen siempre potencias por debajo de las mediciones reales. Las muestras de altas potencias corresponden con esfuerzos en sprints. Al medir la velocidad y la aceleración, existe una especie de retardo para observar este cambio mientras que medir directamente la potencia aplicada en los pedales no se ve afectado por este problema.</p>
<p>Por otro lado, vemos que el modelo lineal predice una potencia negativa catastrófica para muestras con una potencia de 0 vatios. Este se debe a nuestro modelado. De hecho, la potencia basada en el cambio de energía cinética (potencia necesaria para acelerar o desacelerar) está produciendo tales artefactos. No estamos modelando la pérdida de potencia introducida por la disipación de calor cuando los frenos reducen la velocidad de la bicicleta (para tomar una curva, por ejemplo) y, por tanto, obtenemos valores sin sentido para muestras con baja potencia. El regresor histogram gradient boosting usa en su lugar la cadencia, ya que 0 rpm (revoluciones por minuto) se corresponderá con 0 vatios producidos.</p>
<p>Ahora seleccionaremos una porción de los datos de prueba usando el siguiente código:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">time_slice</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="s2">&#34;2020-08-18 17:00:00&#34;</span><span class="p">,</span> <span class="s2">&#34;2020-08-18 17:05:00&#34;</span><span class="p">)</span>

<span class="n">X_test_linear_model_subset</span> <span class="o">=</span> <span class="n">X_linear_model_test</span><span class="p">[</span><span class="n">time_slice</span><span class="p">]</span>
<span class="n">X_test_subset</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">time_slice</span><span class="p">]</span>
<span class="n">y_test_subset</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">time_slice</span><span class="p">]</span></code></pre></div>
<p>Permite seleccionar datos desde la 5.00 pm hasta las 5.05 pm. Usaremos los anteriores modelos ya entrenados (lineal y grandient boosting) para predecir en esta porción de los datos de prueba. Dibujaremos en el mismo gráfico los datos reales y las predicciones de cada modelo. <strong>¿Qué conclusiones podemos obtener?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred_linear_model</span> <span class="o">=</span> <span class="n">model_linear</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_subset</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE modelo lineal: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test_subset, y_pred_linear_model):.3f} k$&#34;</span><span class="p">)</span>

<span class="n">y_pred_hgbr</span> <span class="o">=</span> <span class="n">model_hgbr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_subset</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE hgbr: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test_subset, y_pred_hgbr):.3f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE modelo lineal: 61.359 k$
MAE hgbr: 52.386 k$
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ax</span> <span class="o">=</span> <span class="n">y_test_subset</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Objetivo real&#34;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test_subset</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y_pred_linear_model</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Modelo lineal&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test_subset</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y_pred_hgbr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Modelo HGBR&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;Potencia (Vatios)&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;comparación del objetivo real y predicciones de ambos modelos&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_296_0.png" alt="png"></p>
<p>Vemos que el modelo lineal tiene predicciones que están más alejadas del objetivo real que las predicciones del regresor histogram gradient boosting. Mientras que histogram gradient boosting es capaz de hacer cambios abruptos de potencia, el modelo lineal es incapaz de predecir dichos cambios y necesita tiempo para generar el nivel de potencia real.</p>
<p>Una vez mas, el rendimiento comparativamente malo del modelo de regresión lineal entrenado en las features físicamente significativas no se deriva necesariamente de errores en la ingeniería de características sino que podría de errores de medición que impiden una estimación precisa de cambios pequeños en la velocidad y aceleración.</p>
<p>El modelo gradient boosting sería capar de solucionar estas limitaciones de los errores de GPS a través de mediciones más precisas de frecuencia cardíaca y cadencia.</p>

            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text"></span><a href="https://sgtsteiner.github.io/" class="p-author h-card" target="_blank" rel="noopener">Antonio Méndez</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text"></span><a href="/posts/evaluacion-modelos/" target="_blank" rel="noopener">https://sgtsteiner.github.io/posts/evaluacion-modelos/</a></li>
            
            <li class="copyright-item license"><span class="copyright-item-text"></span><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></li>
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2022-05-10 10:29:00 CEST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2022-05-10</text><text x="915" y="140" textLength="650" transform="scale(.1)">2022-05-10</text></g></svg>
        </span></div>



        


        <div class="post-share">

        

        <div class="share-items">

            
                <div class="share-item twitter">
                    
                    <a href="https://twitter.com/share?url=https://sgtsteiner.github.io/posts/evaluacion-modelos/&amp;text=Evaluaci%c3%b3n%20del%20rendimiento%20de%20modelos&amp;hashtags=dummy,baseline,kfold,stratification,estratificaci%c3%b3n,ShuffleSplit,accuracy,precision,recall,AUC,ROCAUC,sensibilidad,especificidad,R2,&amp;via=Steiner_69" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon twitter-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </div>
            

            

            
                <div class="share-item linkedin">
                    
                    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://sgtsteiner.github.io/posts/evaluacion-modelos/&amp;title=Evaluaci%c3%b3n%20del%20rendimiento%20de%20modelos&amp;summary=En%20anteriores%20posts%20vimos%20el%20framework%20general%20de%20validaci%c3%b3n%20cruzada%20y%20su%20uso%20para%20evaluar%20el%20rendimiento%20de%20modelos.%20Sin%20embargo,%20es%20importante%20tener%20en%20cuenta%20que%20algunos%20elementos%20de%20la%20validaci%c3%b3n%20cruzada%20deben%20decidirse%20en%20funci%c3%b3n%20de%20la%20naturaleza%20del%20problema:%20%28i%29%20la%20estrategia%20de%20validaci%c3%b3n%20cruzada%20y%20%28ii%29%20las%20m%c3%a9tricas%20de%20evaluaci%c3%b3n.&amp;source=Lords%20of%20the%20Machine%20Learning" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon linkedin-icon"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a>
                </div>
            

            
                <div class="share-item telegram">
                    
                    <a href="https://t.me/share/url?url=https://sgtsteiner.github.io/posts/evaluacion-modelos/&amp;text=Evaluaci%c3%b3n%20del%20rendimiento%20de%20modelos" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon telegram-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </div>
            

            

            

            

            

            
                <div class="share-item qrcode">
                    <div class="qrcode-container" title=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id="qrcode-img"></div>
                    </div>
                    <script src="https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js"></script>

<script>
    var typeNumber = 0;
    var errorCorrectionLevel = 'L';
    var qr = qrcode(typeNumber, errorCorrectionLevel);
    qr.addData('https:\/\/sgtsteiner.github.io\/posts\/evaluacion-modelos\/');
    qr.make();
    document.getElementById('qrcode-img').innerHTML = qr.createImgTag();
</script>

                </div>
            

        </div>

    </div>




        
    
    
        <div class="related-posts">
            <h2 class="related-title"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/posts/predictive-modeling-pipeline/" class="related-link">Pipeline de modelado predictivo</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/modelos-conjunto/" class="related-link">Modelos de conjunto</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/modelos-arbol-decision/" class="related-link">Modelos de árbol de decisión</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/modelos-lineales/" class="related-link">Modelos Lineales</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/hyperparameters-tuning/" class="related-link">Ajuste de hiperparámetros</a>
                    </li>
                
            </ul>
        </div>
    



        
    
        <div class="post-tags">
            
                
                
                
                
                    
                    <a href="/tags/dummy/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>dummy</a>
                
            
                
                
                
                
                    
                    <a href="/tags/baseline/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>baseline</a>
                
            
                
                
                
                
                    
                    <a href="/tags/kfold/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>kfold</a>
                
            
                
                
                
                
                    
                    <a href="/tags/stratification/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>stratification</a>
                
            
                
                
                
                
                    
                    <a href="/tags/estratificaci%C3%B3n/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>estratificación</a>
                
            
                
                
                
                
                    
                    <a href="/tags/shufflesplit/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>ShuffleSplit</a>
                
            
                
                
                
                
                    
                    <a href="/tags/accuracy/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>accuracy</a>
                
            
                
                
                
                
                    
                    <a href="/tags/precision/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>precision</a>
                
            
                
                
                
                
                    
                    <a href="/tags/recall/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>recall</a>
                
            
                
                
                
                
                    
                    <a href="/tags/auc/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>AUC</a>
                
            
                
                
                
                
                    
                    <a href="/tags/roc-auc/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>ROC AUC</a>
                
            
                
                
                
                
                    
                    <a href="/tags/sensibilidad/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>sensibilidad</a>
                
            
                
                
                
                
                    
                    <a href="/tags/especificidad/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>especificidad</a>
                
            
                
                
                
                
                    
                    <a href="/tags/r2/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>R2</a>
                
            
        </div>
    



        


        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/posts/time-series-linear-regression/" rel="prev">&lt; Series temporales: (1) Regresión lineal</a>
                </li>
            
            
                <li class="post-nav-next">
                    <a href="/posts/modelos-conjunto/" rel="next">Modelos de conjunto &gt;</a>
                </li>
            
        </ul>
    



        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">©&nbsp;2021–2022&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;Antonio Méndez</div><div class="site-copyright"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="/rss.xml" target="_blank" rel="external noopener" title="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"/></svg></a>
                </li><li class="socials-item">
                    <a href="mailto:futitotal@gmail.com" target="_blank" rel="external noopener" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/SgtSteiner" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://twitter.com/Steiner_69" target="_blank" rel="external noopener" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://t.me/SgtSteiner" target="_blank" rel="external noopener" title="Telegram"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon social-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </li></ul>
    



            
        </div>
    </footer>


        </div>
        

        
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha256-gPJfuwTULrEAAcI3X4bALVU/2qBU+QY/TpoD3GO+Exw=" crossorigin="anonymous">
<script>
    if (typeof renderMathInElement === 'undefined') {
        var getScript = (options) => {
            var script = document.createElement('script');
            script.defer = true;
            script.crossOrigin = 'anonymous';
            Object.keys(options).forEach((key) => {
                script[key] = options[key];
            });
            document.body.appendChild(script);
        };
        getScript({
            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js',
            integrity: 'sha256-YTW9cMncW/ZQMhY69KaUxIa2cPTxV87Uh627Gf5ODUw=',
            onload: () => {
                getScript({
                    src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js',
                    integrity: 'sha256-yzSfYeVsWJ1x+2g8CYHsB/Mn7PcSp8122k5BM4T3Vxw=',
                    onload: () => {
                        getScript({
                            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js',
                            integrity: 'sha256-fxJzNV6hpc8tgW8tF0zVobKa71eTCRGTgxFXt1ZpJNM=',
                            onload: () => {
                                renderKaTex();
                            }
                        });
                    }
                });
            }
        });
    } else {
        renderKaTex();
    }
    function renderKaTex() {
        renderMathInElement(
            document.body,
            {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ]
            }
        );
    }
</script>










    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    mediumZoom(document.querySelectorAll('div.post-body img'), {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>







    </body>
</html>
