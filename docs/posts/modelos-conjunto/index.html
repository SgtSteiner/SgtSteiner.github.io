<!DOCTYPE html>
<html lang="es-ES">
    <head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
    <meta charset="UTF-8" />

    <meta name="generator" content="Hugo 0.83.1" /><meta name="theme-color" content="#fff" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <meta name="format-detection" content="telephone=no, date=no, address=no, email=no" />
    
    <meta http-equiv="Cache-Control" content="no-transform" />
    
    <meta http-equiv="Cache-Control" content="no-siteapp" />

    <title>Modelos de conjunto | Lords of the Machine Learning</title>

    <link rel="stylesheet" href="/css/meme.min.317d84c7f6754ad23a2ae219d212f73a900416875fb1144e27652ca955c5aac2.css"/>

    
    
        <script src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lunr-languages@1.4.0/min/lunr.stemmer.support.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lunr-languages@1.4.0/min/lunr.es.min.js" defer></script><script src="/js/meme.min.b18607e70c711c9fdee7b83c420df45357ac35f70c7adc262c3f62720c9747b8.js"></script>

    

    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />

        <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" media="print" onload="this.media='all'" />
        <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM&#43;Plex&#43;Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Source&#43;Code&#43;Pro:ital,wght@0,400;0,700;1,400;1,700&amp;family=Comfortaa:wght@700&amp;display=swap" /></noscript>

    <meta name="author" content="Antonio Méndez" /><meta name="description" content="En este post veremos en detalle los algoritmos que combinan varios modelos juntos, también llamados conjunto de modelos. Presentaremos dos familias de estas técnicas: (i) basados en bootstrapping y (ii) basados en boosting. Presentaremos bagging y árboles aleatorios como pertenecientes a la primera estrategia y AdaBoost y árboles de decisión gradient boosting que pertenecen a la última estrategia." />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#2a6df4" />
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-title" content="Lords of the Machine Learning" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black" />
    <meta name="mobile-web-app-capable" content="yes" />
    <meta name="application-name" content="Lords of the Machine Learning" />
    <meta name="msapplication-starturl" content="../../" />
    <meta name="msapplication-TileColor" content="#fff" />
    <meta name="msapplication-TileImage" content="../../icons/mstile-150x150.png" />
    <link rel="manifest" href="/manifest.json" />

    
    

    
    <link rel="canonical" href="https://sgtsteiner.github.io/posts/modelos-conjunto/" />
    

<script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "datePublished": "2022-04-20T10:37:29+02:00",
        "dateModified": "2022-05-03T17:20:37+02:00",
        "url": "https://sgtsteiner.github.io/posts/modelos-conjunto/",
        "headline": "Modelos de conjunto",
        "description": "En este post veremos en detalle los algoritmos que combinan varios modelos juntos, también llamados conjunto de modelos. Presentaremos dos familias de estas técnicas: (i) basados en bootstrapping y (ii) basados en boosting. Presentaremos bagging y árboles aleatorios como pertenecientes a la primera estrategia y AdaBoost y árboles de decisión gradient boosting que pertenecen a la última estrategia.",
        "inLanguage" : "es-ES",
        "articleSection": "posts",
        "wordCount":  11675 ,
        "image": ["https://sgtsteiner.github.io/images/output_15_1.png","https://sgtsteiner.github.io/images/output_19_2.png","https://sgtsteiner.github.io/images/output_23_3.png","https://sgtsteiner.github.io/images/output_23_4.png","https://sgtsteiner.github.io/images/output_23_5.png","https://sgtsteiner.github.io/images/output_29_0.png","https://sgtsteiner.github.io/images/output_31_1.png","https://sgtsteiner.github.io/images/output_37_1.png","https://sgtsteiner.github.io/images/output_39_0.png","https://sgtsteiner.github.io/images/output_46_4.png","https://sgtsteiner.github.io/images/output_86_0.png","https://sgtsteiner.github.io/images/output_95_0.png","https://sgtsteiner.github.io/images/output_98_0.png","https://sgtsteiner.github.io/images/output_105_1.png","https://sgtsteiner.github.io/images/output_105_3.png","https://sgtsteiner.github.io/images/output_105_5.png","https://sgtsteiner.github.io/images/output_112_0.png","https://sgtsteiner.github.io/images/output_116_0.png","https://sgtsteiner.github.io/images/output_119_0.png","https://sgtsteiner.github.io/images/output_123_0.png","https://sgtsteiner.github.io/images/output_125_0.png","https://sgtsteiner.github.io/images/output_148_0.png","https://sgtsteiner.github.io/images/output_199_0.png","https://sgtsteiner.github.io/images/output_221_0.png","https://sgtsteiner.github.io/images/output_224_0.png","https://sgtsteiner.github.io/images/output_236_0.png"],
        "author": {
            "@type": "Person",
            "description": "Sgt. Steiner",
            "email": "futitotal@gmail.com",
            "image": "https://sgtsteiner.github.io/icons/apple-touch-icon.png",
            "url": "https://sgtsteiner.github.io/",
            "name": "Antonio Méndez"
        },
        "license": "[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es)",
        "publisher": {
            "@type": "Organization",
            "name": "Lords of the Machine Learning",
            "logo": {
                "@type": "ImageObject",
                "url": "https://sgtsteiner.github.io/icons/apple-touch-icon.png"
            },
            "url": "https://sgtsteiner.github.io/"
        },
        "mainEntityOfPage": {
            "@type": "WebSite",
            "@id": "https://sgtsteiner.github.io/"
        }
    }
</script>

    

<meta name="twitter:card" content="summary_large_image" />


<meta name="twitter:site" content="@Steiner_69" />
<meta name="twitter:creator" content="@Steiner_69" />

    



<meta property="og:title" content="Modelos de conjunto" />
<meta property="og:description" content="En este post veremos en detalle los algoritmos que combinan varios modelos juntos, también llamados conjunto de modelos. Presentaremos dos familias de estas técnicas: (i) basados en bootstrapping y (ii) basados en boosting. Presentaremos bagging y árboles aleatorios como pertenecientes a la primera estrategia y AdaBoost y árboles de decisión gradient boosting que pertenecen a la última estrategia." />
<meta property="og:url" content="https://sgtsteiner.github.io/posts/modelos-conjunto/" />
<meta property="og:site_name" content="Lords of the Machine Learning" />
<meta property="og:locale" content="es" /><meta property="og:image" content="https://sgtsteiner.github.io/images/output_15_1.png" />
<meta property="og:type" content="article" />
    <meta property="article:published_time" content="2022-04-20T10:37:29&#43;02:00" />
    <meta property="article:modified_time" content="2022-05-03T17:20:37&#43;02:00" />
    
    <meta property="article:section" content="posts" />



    
    

    
</head>

    <body>
        <div class="container">
            
    <header class="header">
        
            <div class="header-wrapper">
                <div class="header-inner single">
                    
    <div class="site-brand">
        
            <a href="/" class="brand">Lords of the Machine Learning</a>
        
    </div>

                    <nav class="nav">
    <ul class="menu" id="menu">
        
            
        
        
        
        
            
                <li class="menu-item"><a href="/posts/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7 0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6 0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6 0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3 0 64v48c0 8.8 7.2 16 16 16h480c8.8 0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class="menu-item-name">Posts</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/categories/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon th"><path d="M149.333 56v80c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V56c0-13.255 10.745-24 24-24h101.333c13.255 0 24 10.745 24 24zm181.334 240v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm32-240v80c0 13.255 10.745 24 24 24H488c13.255 0 24-10.745 24-24V56c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24zm-32 80V56c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.256 0 24.001-10.745 24.001-24zm-205.334 56H24c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24zM0 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H24c-13.255 0-24 10.745-24 24zm386.667-56H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zm0 160H488c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H386.667c-13.255 0-24 10.745-24 24v80c0 13.255 10.745 24 24 24zM181.333 376v80c0 13.255 10.745 24 24 24h101.333c13.255 0 24-10.745 24-24v-80c0-13.255-10.745-24-24-24H205.333c-13.255 0-24 10.745-24 24z"/></svg><span class="menu-item-name">Categories</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/tags/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"/></svg><span class="menu-item-name">Tags</span></a>
                </li>
            
        
            
                <li class="menu-item"><a href="/about/"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon user-circle"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm0 96c48.6 0 88 39.4 88 88s-39.4 88-88 88-88-39.4-88-88 39.4-88 88-88zm0 344c-58.7 0-111.3-26.6-146.5-68.2 18.8-35.4 55.6-59.8 98.5-59.8 2.4 0 4.8.4 7.1 1.1 13 4.2 26.6 6.9 40.9 6.9 14.3 0 28-2.7 40.9-6.9 2.3-.7 4.7-1.1 7.1-1.1 42.9 0 79.7 24.4 98.5 59.8C359.3 421.4 306.7 448 248 448z"/></svg><span class="menu-item-name">About</span></a>
                </li>
            
        
            
                
                    
                    
                        <li class="menu-item">
                            <a id="theme-switcher" href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5l48.8-97.5a18 18 0 0128 0l48.8 97.5 103.4 -34.5a18 18 0 0119.8 19.8l-34.5 103.4l97.5 48.8a18 18 0 010 28l-97.5 48.8 34.5 103.4a18 18 0 01-19.8 19.8l-103.4-34.5-48.8 97.5a18 18 0 01-28 0l-48.8-97.5l-103.4 34.5a18 18 0 01-19.8-19.8l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8-34.5-103.4a18 18 0 0119.8-19.8zM256 128a128 128 0 10.01 0M256 160a96 96 0 10.01 0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412a256 256 0 10154-407a11.5 11.5 0 00-5 20a201.5 201.5 0 01-134 374a11.5 11.5 0 00-15 13"/></svg></a>
                        </li>
                    
                
            
        
            
                
            
        
            
                <li class="menu-item search-item">
                        <form id="search" class="search" role="search">
    <label for="search-input"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label>
    <input type="search" id="search-input" class="search-input">
</form>

<template id="search-result" hidden>
    <article class="content post">
        <h2 class="post-title"><a class="summary-title-link"></a></h2>
        <summary class="summary"></summary>
        <div class="read-more-container">
            <a class="read-more-link"> »</a>
        </div>
    </article>
</template>

                    </li>
                
            
        
    </ul>
</nav>

                    
                </div>
            </div>
            
    <input type="checkbox" id="nav-toggle" aria-hidden="true" />
    <label for="nav-toggle" class="nav-toggle"></label>
    <label for="nav-toggle" class="nav-curtain"></label>


        
    </header>




            
            
    <main class="main single" id="main">
    <div class="main-inner">

        

        <article class="content post h-entry" data-small-caps="true" data-align="default" data-type="posts" data-toc-num="true">

            <h1 class="post-title p-name">Modelos de conjunto</h1>

            

            
                
            

            
                

<div class="post-meta">
    
        
        <time datetime="2022-04-20T10:37:29&#43;02:00" class="post-meta-item published dt-published"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M148 288h-40c-6.6 0-12-5.4-12-12v-40c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v40c0 6.6-5.4 12-12 12zm108-12v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 96v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm-96 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm192 0v-40c0-6.6-5.4-12-12-12h-40c-6.6 0-12 5.4-12 12v40c0 6.6 5.4 12 12 12h40c6.6 0 12-5.4 12-12zm96-260v352c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h48V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h128V12c0-6.6 5.4-12 12-12h40c6.6 0 12 5.4 12 12v52h48c26.5 0 48 21.5 48 48zm-48 346V160H48v298c0 3.3 2.7 6 6 6h340c3.3 0 6-2.7 6-6z"/></svg>&nbsp;20.4.2022</time>
    
    
        
        <time datetime="2022-05-03T17:20:37&#43;02:00" class="post-meta-item modified dt-updated"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon post-meta-icon"><path d="M400 64h-48V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H160V12c0-6.627-5.373-12-12-12h-40c-6.627 0-12 5.373-12 12v52H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm-6 400H54a6 6 0 0 1-6-6V160h352v298a6 6 0 0 1-6 6zm-52.849-200.65L198.842 404.519c-4.705 4.667-12.303 4.637-16.971-.068l-75.091-75.699c-4.667-4.705-4.637-12.303.068-16.971l22.719-22.536c4.705-4.667 12.303-4.637 16.97.069l44.104 44.461 111.072-110.181c4.705-4.667 12.303-4.637 16.971.068l22.536 22.718c4.667 4.705 4.636 12.303-.069 16.97z"/></svg>&nbsp;3.5.2022</time>
    
    
    
        
        
        
            
                <span class="post-meta-item category"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M464 128H272l-54.63-54.63c-6-6-14.14-9.37-22.63-9.37H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V176c0-26.51-21.49-48-48-48zm0 272H48V112h140.12l54.63 54.63c6 6 14.14 9.37 22.63 9.37H464v224z"/></svg>&nbsp;<a href="/categories/tutoriales/" class="category-link p-category">tutoriales</a></span>
            
        
    
    
        
        <span class="post-meta-item wordcount"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg>&nbsp;11675</span>
    
    
        
        <span class="post-meta-item reading-time"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon post-meta-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm0 448c-110.5 0-200-89.5-200-200S145.5 56 256 56s200 89.5 200 200-89.5 200-200 200zm61.8-104.4l-84.9-61.7c-3.1-2.3-4.9-5.9-4.9-9.7V116c0-6.6 5.4-12 12-12h32c6.6 0 12 5.4 12 12v141.7l66.8 48.6c5.4 3.9 6.5 11.4 2.6 16.8L334.6 349c-3.9 5.3-11.4 6.5-16.8 2.6z"/></svg>&nbsp;55&nbsp;</span>
    
    
    
</div>

            

            <div class="post-body e-content">
              <p style="text-indent:0"><span class="drop-cap">E</span>n este post veremos en detalle los algoritmos que combinan varios modelos juntos, también llamados conjunto de modelos. Presentaremos dos familias de estas técnicas: (i) basados en <em>bootstrapping</em> y (ii) basados en <em>boosting</em>. Presentaremos <em>bagging</em> y <em>árboles aleatorios</em> como pertenecientes a la primera estrategia y <em>AdaBoost</em> y <em>árboles de decisión gradient boosting</em> que pertenecen a la última estrategia. Finalmente, hablaremos sobre los hiperparámetros que permiten afinar estos modelos y compararlos entre modelos.</p>
<h1 id="ejemplo-introductorio-a-los-modelos-de-conjunto"><a href="#ejemplo-introductorio-a-los-modelos-de-conjunto" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejemplo introductorio a los modelos de conjunto</h1>
<p>En este ejemplo queremos enfatizar los beneficios de los métodos de conjunto sobre los modelos simples (por ejemplo, árbol de decisión, modelo lineal, etc.). Combinar modelos simples produce modelos más poderosos y robustos con menos problemas.</p>
<p>Empezaremos cargando el dataset de propiedades de California. El objetivo de este dataset es prededir el valor medio de la vivienda en algunos distritos de California basándonos en datos geográficos y demográficos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala el objetivo en k$</span></code></pre></div>
<p>Vamos a comprobar el rendimiento de generalización de un árbol de decisión regresor con los parámetros por defecto.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación R2 obtenida por validación cruzada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores.mean():.3f} +/- {scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación R2 obtenida por validación cruzada: 0.354 +/- 0.087
</code></pre>
<p>Obtenemos resultados justos. Sin embargo, ya sabemos que este modelo necesita ser afinado. De hecho, los parámetros por defecto no conducen necesariamente a un árbol de decisión óptimo. En lugar de usar los valores por defecto, debemos buscar a través de validación cruzada el valor óptimo de parámetros importantes, como son <code>max_depth</code>, <code>min_samples_split</code> o <code>min_samples_leaf</code>.</p>
<p>Recordemos que necesitamos ajustar esos parámetros, ya que los árboles de decisión adolecen de overfitting si aumenta la profundidad del árbol, pero no existen reglas de cómo se debe configurar cada parámetro. Por tanto, no realizar una búsqueda nos llevaría a tener un modelo con underfitting u overfitting.</p>
<p>Ahora vamos a hacer un grid search para afinar los hiperparámetros que hemos mencionado anteriormente.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&#34;max_depth&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span>
      <span class="s2">&#34;min_samples_split&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
      <span class="s2">&#34;min_samples_leaf&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]}</span>
<span class="n">cv</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación R2 obtenida por validación cruzada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores.mean():.3f} +/- {scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación R2 obtenida por validación cruzada: 0.523 +/- 0.107
CPU times: total: 15.6 ms
Wall time: 4.01 s
</code></pre>
<p>Vemos que optimizar los hiperparámetros tiene un efecto positivo en el rendimiento de generalización. Sin embargo, conlleva un alto coste computacional.</p>
<p>Podemos crear un dataframe almacenando la información importante recopilada durante el ajuste de hiperparámetros e investigar los resultados.</p>
<p>Ahora usaremos un metodo de conjunto llamado <strong>bagging</strong>. Brevemente, este método usará un regresor de base (por ejemplo, árboles de decisión regresores) y entrenará varios de ellos en una versión ligeramente modificada del conjunto de entrenamiento. Luego, las predicciones de todos esos regresores base se combinarán promediando.</p>
<p>Aquí, usaremos 20 árboles de decisión y comprobaremos el tiempo de entrenamiento, así como el rendimiento de generalización en los datos de prueba externos. Es importante notar que no vamos a ajustar ningún parámetro del árbol de decisión.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="o">%%</span><span class="n">time</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>

<span class="n">base_estimator</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">bagging_regressor</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">bagging_regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuación R2 obtenida por validación cruzada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores.mean():.3f} +/- {scores.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuación R2 obtenida por validación cruzada: 0.642 +/- 0.083
CPU times: total: 31.2 ms
Wall time: 3.32 s
</code></pre>
<p>Sin realizar la búsqueda de los hiperparámetros óptimos, el rendimiento de generalización global del regresor bagging es mucho mejor que un único árbol de decisión. Además, el coste computacional se reduce en comparación a la búsqueda de hiperparámetros óptimos.</p>
<p>Esto muestra la motivación existente detrás del uso de métodos de conjunto: proporcionan una relativamente buena baseline con un rendimiento de generalización decente sin ningún ajuste de hiperparámetros.</p>
<p>A continuación, veremos en detalle dos familias de conjunto: bagging y boosting:</p>
<ul>
<li>conjunto usando bootstrap (por ejemplo, bagging y bosques aleatorios);</li>
<li>conjunto usando boosting (por ejemplo, boosting adaptativo y álboles de decisión gradient-boosting).</li>
</ul>
<h1 id="métodos-de-conjuntos-usando-bootstrapping"><a href="#métodos-de-conjuntos-usando-bootstrapping" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Métodos de conjuntos usando bootstrapping</h1>
<h2 id="bagging"><a href="#bagging" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Bagging</h2>
<p>Vamos a introducir una estrategia muy natural de construir conjuntos de modelos de machine learning llamado &ldquo;bagging&rdquo;.</p>
<p>&ldquo;Bagging&rdquo; proviene de Bootstrap AGGregatING. Utiliza remuestreo bootstrap (muestreo aleatorio con reemplazo) para aprender varios modelos en variaciones aleatorias del conjunto de entrenamiento. A la hora de predecir, se agregan las predicciones de cada aprendiz para obtener las predicciones finales.</p>
<p>En primer lugar, vamos a generar un dataset sintético simple para obtener información de bootstraping.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="n">y</span> <span class="o">/=</span> <span class="n">y</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

    <span class="n">data_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">])</span>
    <span class="n">data_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_max</span><span class="p">,</span> <span class="n">x_min</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">300</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">])</span>
    <span class="n">target_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Target&#34;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_train</span><span class="p">,</span> <span class="n">data_test</span><span class="p">,</span> <span class="n">target_train</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Dataset de regresión sintético&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_15_1.png" alt="png"></p>
<p>La relación entre nuestras features y el objetivo a predecir es no lineal. Sin embargo, un árbol de decisión es capaz de aproximar tal dependencia no lineal:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></div>
<p>En este caso, el término &ldquo;test&rdquo; se refiere a datos que no se han usado previamente para entrenar y calcular una métrica de evaluación en un conjunto de prueba de este tipo no tendría sentido.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Arbol entrenado&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones de un único árbol de decisión&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_19_2.png" alt="png"></p>
<p>Veamos cómo podemos usar bootstraping para entrenar varios árboles.</p>
<h3 id="bootstrap-resampling"><a href="#bootstrap-resampling" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Bootstrap resampling</h3>
<p>Una muestra bootstrap corresponde a un remuestreo con reemplazo del dataset original, una muestra que tiene el mismo tamaño que el dataset original. Por lo tanto, la muestra bootstrap contendrá varias veces algunos puntos de datos, mientras que algunos de los puntos de datos originales no estarán presentes.</p>
<p>Crearemos una función que, dados <code>X</code> e <code>y</code>, devolverá una variación remuestreada <code>X_boostrap</code> e <code>y_boostrap</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">bootstrap_sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="c1"># indices corresponde a un muestreo con reemplazo del misma tamaño de muestra</span>
    <span class="c1"># que los datos originales</span>
    <span class="n">bootstrap_indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">size</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">data_bootstrap</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">bootstrap_indices</span><span class="p">]</span>
    <span class="n">target_bootstrap</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">bootstrap_indices</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">data_bootstrap</span><span class="p">,</span> <span class="n">target_bootstrap</span></code></pre></div>
<p>Vamos a generar 3 muestras bootstrap y verificaremos cualitativamente la diferencia con el dataset original.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">n_bootstraps</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">bootstrap_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_bootstraps</span><span class="p">):</span>
    <span class="n">X_bootstrap</span><span class="p">,</span> <span class="n">y_bootstrap</span> <span class="o">=</span> <span class="n">bootstrap_sample</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_bootstrap</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_bootstrap</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:blue&#34;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s2">&#34;none&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;X remuestreado&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">180</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;X original&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;X remuestrado #{bootstrap_idx}&#34;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span></code></pre></div>
<p><img src="/images/output_23_3.png" alt="png"></p>
<p><img src="/images/output_23_4.png" alt="png"></p>
<p><img src="/images/output_23_5.png" alt="png"></p>
<p>Observemos que las tres variaciones comparten puntos comunes con el dataset original. Algunos puntos son remuestreados aleatoriamente varias veces y aparecen como círculos azul oscuro.</p>
<p>Las tres muestras bootstrap generadas son diferentes del dataset original y entre cada una de ellas. Para confirmar esta intuición, podemos comprobar el número de muestras únicas en las muestras bootstrap.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X_train_huge</span><span class="p">,</span> <span class="n">X_test_huge</span><span class="p">,</span> <span class="n">y_train_huge</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100_000</span><span class="p">)</span>
<span class="n">X_bootstrap_sample</span><span class="p">,</span> <span class="n">y_bootstrap_sample</span> <span class="o">=</span> <span class="n">bootstrap_sample</span><span class="p">(</span>
    <span class="n">X_train_huge</span><span class="p">,</span> <span class="n">y_train_huge</span><span class="p">)</span>

<span class="n">ratio_unique_sample</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X_bootstrap_sample</span><span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span>
                       <span class="n">X_bootstrap_sample</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span>
    <span class="n">f</span><span class="s2">&#34;Porcentaje de muestras presentes en el dataset original: &#34;</span>
    <span class="n">f</span><span class="s2">&#34;{ratio_unique_sample * 100:.1f}%&#34;</span>
<span class="p">)</span></code></pre></div>
<pre><code>Porcentaje de muestras presentes en el dataset original: 63.2%
</code></pre>
<p>En promedio, un 63.2% de los puntos de datos del dataset original estarán presentes en una muestra bootstrap dada. El otro 36.8% son muestras repetidas. Somos capaces de generar muchos datasets, todos ligeramente diferentes.</p>
<p>Ahora podemos entrenar un árbol de decisión para cada uno de esos datasets y todos ellos serán también ligeramente diferentes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">bag_of_trees</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">bootstrap_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_bootstraps</span><span class="p">):</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">X_bootstrap_sample</span><span class="p">,</span> <span class="n">y_bootstrap_sample</span> <span class="o">=</span> <span class="n">bootstrap_sample</span><span class="p">(</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_bootstrap_sample</span><span class="p">,</span> <span class="n">y_bootstrap_sample</span><span class="p">)</span>
    <span class="n">bag_of_trees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span></code></pre></div>
<p>Ahora que podemos crear una bolsa de árboles diferentes, podemos usar cada uno de los tres árboles para predecir las muestras dentro del rango de los datos. Ellos proporcionarán predicciones ligeramente diferentes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bag_of_trees</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">f</span><span class="s2">&#34;Predicciones árbol #{tree_idx}&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones de árboles entrenados en diferentes bootstraps&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_29_0.png" alt="png"></p>
<h3 id="agregación"><a href="#agregación" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Agregación</h3>
<p>Una vez hemos entrenado nuestros árboles somos capaces de obtener predicciones de cada uno de ellos. En regresión, la forma más directa de combinar esas predicciones es promediarlas: para un punto de datos de prueba dado, alimentamos los valores de features de entrada a cada uno de los <code>n</code> models entrenados en el conjunto y, como resultado, calculamos <code>n</code> valores predichos para la variable objetivo. La predicción final del conjunto para los puntos de datos de prueba es la media de esos <code>n</code>valores.</p>
<p>Podemos dibujar las predicciones promediadas del ejemplo anterior.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">bag_predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bag_of_trees</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="n">f</span><span class="s2">&#34;Predicciones árbol #{tree_idx}&#34;</span><span class="p">)</span>
    <span class="n">bag_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

<span class="n">bag_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bag_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">bag_predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicciones medias&#34;</span><span class="p">,</span>
         <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;-&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones de los árboles de la bolsa&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_31_1.png" alt="png"></p>
<p>La línea roja continua muestras las predicciones medias, las cuales serán las predicciones finales proporcionadas por nuestra bolsa de árboles de decisión regresores. Nótese que las predicciones de los conjuntos son más estables debido a la operación de promedio. Como resultado, el conjunto de árboles, en global, es menos probable que adolezca de overfitting que los árboles individuales.</p>
<h3 id="bagging-en-scikit-learn"><a href="#bagging-en-scikit-learn" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Bagging en scikit-learn</h3>
<p>Scikit-learn implementa el procedimiento de bagging como un &ldquo;meta-estimador&rdquo;, que es un estimador que recubre otro estimador: toma un modelo base que es clonado varias veces y entrenado independientemente en cada muestra bootstrap.</p>
<p>El siguiente fragmento de código muestra cómo construir un conjunto bagging de árboles de decisión. Establecemos <code>n_estimators=100</code> en lugar de los 3 que vimos anteriormente en la implementación manual para obtener un efecto de suavizado más fuerte.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>

<span class="n">bagged_trees</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="n">bagged_trees</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></div>
<pre><code>BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                 n_estimators=100)
</code></pre>
<p>Vamos a visualizar las predicciones del conjunto en el mismo intervalo de datos:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bagged_trees</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones de un clasificador bagging&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_37_1.png" alt="png"></p>
<p>Debido a que hemos usado 100 árboles en el conjunto, la predicción media es ligeramente más suave pero muy similar a la del gráfico anterior. Es posible acceder después del entrenamiento a los modelos internos del conjunto, almacenados como una lista de Python en el atributo <code>bagged_trees.estimators_</code>.</p>
<p>Comparemos las predicciones de los modelos base con sus medias:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bagged_trees</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="s2">&#34;Predicciones de árboles individuales&#34;</span> <span class="k">if</span> <span class="n">tree_idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span>
    <span class="c1"># convertirmos `X_test` a un array Numpy para evitar el warning de scikit-learn</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
             <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:blue&#34;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bagged_trees</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicciones del conjunto&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span></code></pre></div>
<p><img src="/images/output_39_0.png" alt="png"></p>
<p>Usamos un valor bajo del parámetro de opacidad <code>alpha</code> para apreciar mejor el solape en las funciones de predicción de los árboles individuales.</p>
<p>Esta visualización nos da algunas ideas sobre la incertidumbre de las predicciones en diferentes áreas del espacio de features.</p>
<h3 id="pipelines-de-bagging-complejos"><a href="#pipelines-de-bagging-complejos" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Pipelines de bagging complejos</h3>
<p>Aunque hemos usado un árbol de decisión como modelo de base, nada nos impide usar cualquier otro tipo de modelo.</p>
<p>Como sabemos que la función de generación de los datos originales es una transformación polinomial ruidosa de la variable de entrada, intentemos entrenar un pipeline de regresión polinomial bagged en este dataset.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">polynomial_regressor</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">MinMaxScaler</span><span class="p">(),</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">),</span>
<span class="p">)</span></code></pre></div>
<p>El pipeline en primer lugar escala los datos a un rango 0-1 con <code>MinMaxScaler</code>. Después extrae features polinomiales de grado 4. Las features resultantes permanecerán en el rango 0-1 por construcción: si <code>x</code> se encuentra en el rango 0-1, entonces <code>x ** n</code> también se encontrará en el rango 0-1 para cualquier valor de <code>n</code>.</p>
<p>Después el pipeline alimenta las features no lineales resultantes a un modelo de regresión lineal regularizado para la predicción final de la variable objetivo.</p>
<p>Nótese que se ha usado intencionadamente un valor pequeño para el parámetro de regularización <code>alpha</code>, ya que esperamos que el conjunto bagging funcione bien con modelos de base con ligero overfitting.</p>
<p>El conjunto en sí mismo se construye simplemente pasando el pipeline resultante al parámetro <code>base_estimator</code> de la clase <code>BaggingRegressor</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">polynomial_regressor</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">bagging</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></div>
<pre><code>BaggingRegressor(base_estimator=Pipeline(steps=[('minmaxscaler',
                                                 MinMaxScaler()),
                                                ('polynomialfeatures',
                                                 PolynomialFeatures(degree=4)),
                                                ('ridge', Ridge(alpha=1e-10))]),
                 n_estimators=100, random_state=0)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">regressor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bagging</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
    <span class="c1"># convertirmos `X_test` a un array Numpy para evitar el warning de scikit-learn</span>
    <span class="n">regressor_predictions</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">())</span>
    <span class="n">base_model_line</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">regressor_predictions</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicciones de los modelos base&#34;</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:blue&#34;</span>
    <span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bagging</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">,</span>
         <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predicciones del conjunto&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Regresión polinómica Bagging&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_46_4.png" alt="png"></p>
<p>Las predicciones de este modelo de regresión polinómica bagged parecen cualitativamente mejores que los áboles bagging. Esto era algo esperado dado que el modelo de base refleja mejor nuestro conocimiento del proceso de generación de los datos verdaderos.</p>
<p>De nuevo las diferentes opacidades inducidas por el solapamiento de las líneas azules nos permite apreciar la incertidumbre de las predicciones en el conjunto bagged.</p>
<p>Nótese que el procedimiento de bootstrapping es una herramienta genérica de estadística y no está limitada a construir conjuntos de modelos de machine learning. Se puede obtener más detalle sobre este punto en el <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" target="_blank" rel="noopener">artículo de Wikipedia sobre bootstrapping</a>.</p>
<h3 id="ejercicio-bagging"><a href="#ejercicio-bagging" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio bagging</h3>
<p>Vamos a realizar un pequeño ejercicio para practicar lo visto hasta ahora. Queremos investigar si podemos ajustar los hiperparámetros de un regresor bagging y evaluar la ganancia obtenida. Para ello usaremos el dataset de propiedades de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># rescale the target in k$</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span></code></pre></div>
<p>Vamos a crear un <code>BaggingRegressor</code> y proporcionaremos un <code>DecisionTreeRegressor</code> a su parámetro <code>base_estimator</code>. Entrenaremos el regresor y evaluaremos su rendimiento de generalización en el conjunto de prueba usando el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">())</span>

<span class="n">bagging</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bagging</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE del regresor bagging básico: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test, y_pred):.2f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE del regresor bagging básico: 35.97 k$
</code></pre>
<p>Ahora crearemos una instancia de <code>RandomizedSearchCV</code> usando el modelo previo y ajustando los parámetros importantes del regresor bagging. Debemos encontrar los mejores parámetros y comprobar que somos capaces de encontrar un conjunto de parámetros que mejoren el regresor por defecto, usando también como métrica el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Comprobemos cuáles son los parámetros de nuestro regresor bagging</span>
<span class="n">bagging</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span></code></pre></div>
<pre><code>{'base_estimator__ccp_alpha': 0.0,
 'base_estimator__criterion': 'squared_error',
 'base_estimator__max_depth': None,
 'base_estimator__max_features': None,
 'base_estimator__max_leaf_nodes': None,
 'base_estimator__min_impurity_decrease': 0.0,
 'base_estimator__min_samples_leaf': 1,
 'base_estimator__min_samples_split': 2,
 'base_estimator__min_weight_fraction_leaf': 0.0,
 'base_estimator__random_state': None,
 'base_estimator__splitter': 'best',
 'base_estimator': DecisionTreeRegressor(),
 'bootstrap': True,
 'bootstrap_features': False,
 'max_features': 1.0,
 'max_samples': 1.0,
 'n_estimators': 10,
 'n_jobs': None,
 'oob_score': False,
 'random_state': None,
 'verbose': 0,
 'warm_start': False}
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;n_estimators&#34;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span>
    <span class="s2">&#34;max_samples&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="s2">&#34;max_features&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
    <span class="s2">&#34;base_estimator__max_depth&#34;</span><span class="p">:</span> <span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">bagging</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></code></pre></div>
<pre><code>RandomizedSearchCV(estimator=BaggingRegressor(base_estimator=DecisionTreeRegressor()),
                   n_iter=20,
                   param_distributions={'base_estimator__max_depth': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4B835A590&gt;,
                                        'max_features': [0.5, 0.8, 1.0],
                                        'max_samples': [0.5, 0.8, 1.0],
                                        'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F4B82D9840&gt;},
                   scoring='neg_mean_absolute_error')
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">best_params_</span></code></pre></div>
<pre><code>{'base_estimator__max_depth': 9,
 'max_features': 0.8,
 'max_samples': 0.8,
 'n_estimators': 25}
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">cv_results</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&#34;mean_test_score&#34;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean_fit_time</th>
      <th>std_fit_time</th>
      <th>mean_score_time</th>
      <th>std_score_time</th>
      <th>param_base_estimator__max_depth</th>
      <th>param_max_features</th>
      <th>param_max_samples</th>
      <th>param_n_estimators</th>
      <th>params</th>
      <th>split0_test_score</th>
      <th>split1_test_score</th>
      <th>split2_test_score</th>
      <th>split3_test_score</th>
      <th>split4_test_score</th>
      <th>mean_test_score</th>
      <th>std_test_score</th>
      <th>rank_test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7</th>
      <td>0.455792</td>
      <td>0.006091</td>
      <td>0.008107</td>
      <td>3.745593e-04</td>
      <td>9</td>
      <td>0.8</td>
      <td>0.8</td>
      <td>25</td>
      <td>{'base_estimator__max_depth': 9, 'max_features...</td>
      <td>-39.279991</td>
      <td>-38.880628</td>
      <td>-38.619408</td>
      <td>-39.440439</td>
      <td>-36.531615</td>
      <td>-38.550416</td>
      <td>1.050122</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.320576</td>
      <td>0.000928</td>
      <td>0.006406</td>
      <td>2.000333e-04</td>
      <td>8</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>21</td>
      <td>{'base_estimator__max_depth': 8, 'max_features...</td>
      <td>-41.914622</td>
      <td>-41.483535</td>
      <td>-39.830309</td>
      <td>-42.232385</td>
      <td>-39.161815</td>
      <td>-40.924533</td>
      <td>1.208945</td>
      <td>2</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.184759</td>
      <td>0.002748</td>
      <td>0.005205</td>
      <td>2.456516e-04</td>
      <td>7</td>
      <td>0.8</td>
      <td>0.5</td>
      <td>17</td>
      <td>{'base_estimator__max_depth': 7, 'max_features...</td>
      <td>-44.589440</td>
      <td>-42.106688</td>
      <td>-40.293463</td>
      <td>-44.670376</td>
      <td>-40.690151</td>
      <td>-42.470024</td>
      <td>1.863912</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.282943</td>
      <td>0.004392</td>
      <td>0.005605</td>
      <td>2.004386e-04</td>
      <td>7</td>
      <td>0.8</td>
      <td>0.8</td>
      <td>19</td>
      <td>{'base_estimator__max_depth': 7, 'max_features...</td>
      <td>-41.832809</td>
      <td>-43.779636</td>
      <td>-42.014627</td>
      <td>-43.711636</td>
      <td>-41.013014</td>
      <td>-42.470344</td>
      <td>1.094805</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.257421</td>
      <td>0.002090</td>
      <td>0.005805</td>
      <td>2.449898e-04</td>
      <td>7</td>
      <td>0.8</td>
      <td>0.8</td>
      <td>17</td>
      <td>{'base_estimator__max_depth': 7, 'max_features...</td>
      <td>-42.989436</td>
      <td>-42.727496</td>
      <td>-40.938417</td>
      <td>-44.601246</td>
      <td>-41.207918</td>
      <td>-42.492903</td>
      <td>1.327728</td>
      <td>5</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.262926</td>
      <td>0.002443</td>
      <td>0.005605</td>
      <td>2.003911e-04</td>
      <td>7</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>19</td>
      <td>{'base_estimator__max_depth': 7, 'max_features...</td>
      <td>-43.521755</td>
      <td>-44.202781</td>
      <td>-42.572490</td>
      <td>-43.749080</td>
      <td>-41.546292</td>
      <td>-43.118480</td>
      <td>0.949303</td>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.222692</td>
      <td>0.001049</td>
      <td>0.005305</td>
      <td>2.453791e-04</td>
      <td>6</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>18</td>
      <td>{'base_estimator__max_depth': 6, 'max_features...</td>
      <td>-45.666625</td>
      <td>-45.603503</td>
      <td>-44.938197</td>
      <td>-45.636675</td>
      <td>-43.376790</td>
      <td>-45.044358</td>
      <td>0.876670</td>
      <td>7</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.195368</td>
      <td>0.001602</td>
      <td>0.004604</td>
      <td>1.995565e-04</td>
      <td>6</td>
      <td>0.8</td>
      <td>0.8</td>
      <td>15</td>
      <td>{'base_estimator__max_depth': 6, 'max_features...</td>
      <td>-45.489748</td>
      <td>-46.381155</td>
      <td>-44.224186</td>
      <td>-45.389135</td>
      <td>-43.919159</td>
      <td>-45.080677</td>
      <td>0.898491</td>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.259023</td>
      <td>0.002806</td>
      <td>0.008507</td>
      <td>2.861023e-07</td>
      <td>9</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>28</td>
      <td>{'base_estimator__max_depth': 9, 'max_features...</td>
      <td>-45.003335</td>
      <td>-46.935110</td>
      <td>-45.408900</td>
      <td>-44.305677</td>
      <td>-44.801457</td>
      <td>-45.290896</td>
      <td>0.895574</td>
      <td>9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.330484</td>
      <td>0.000813</td>
      <td>0.005204</td>
      <td>2.451844e-04</td>
      <td>6</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>17</td>
      <td>{'base_estimator__max_depth': 6, 'max_features...</td>
      <td>-46.330157</td>
      <td>-46.133009</td>
      <td>-44.831581</td>
      <td>-45.898168</td>
      <td>-43.435013</td>
      <td>-45.325586</td>
      <td>1.077706</td>
      <td>10</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.330083</td>
      <td>0.000679</td>
      <td>0.005405</td>
      <td>3.748906e-04</td>
      <td>6</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>17</td>
      <td>{'base_estimator__max_depth': 6, 'max_features...</td>
      <td>-46.233147</td>
      <td>-46.316019</td>
      <td>-44.772506</td>
      <td>-46.858990</td>
      <td>-43.147684</td>
      <td>-45.465669</td>
      <td>1.349592</td>
      <td>11</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.110395</td>
      <td>0.002184</td>
      <td>0.004504</td>
      <td>3.234067e-07</td>
      <td>9</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>12</td>
      <td>{'base_estimator__max_depth': 9, 'max_features...</td>
      <td>-43.518541</td>
      <td>-48.826323</td>
      <td>-43.241693</td>
      <td>-44.959974</td>
      <td>-49.044992</td>
      <td>-45.918305</td>
      <td>2.532756</td>
      <td>12</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.190764</td>
      <td>0.000861</td>
      <td>0.005505</td>
      <td>1.507891e-07</td>
      <td>5</td>
      <td>0.8</td>
      <td>0.5</td>
      <td>23</td>
      <td>{'base_estimator__max_depth': 5, 'max_features...</td>
      <td>-48.199928</td>
      <td>-49.122217</td>
      <td>-47.921591</td>
      <td>-48.442045</td>
      <td>-47.014668</td>
      <td>-48.140090</td>
      <td>0.689006</td>
      <td>13</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.225894</td>
      <td>0.004134</td>
      <td>0.005705</td>
      <td>2.448730e-04</td>
      <td>5</td>
      <td>0.5</td>
      <td>1.0</td>
      <td>25</td>
      <td>{'base_estimator__max_depth': 5, 'max_features...</td>
      <td>-50.437443</td>
      <td>-52.356826</td>
      <td>-51.114233</td>
      <td>-51.058152</td>
      <td>-49.041999</td>
      <td>-50.801730</td>
      <td>1.078062</td>
      <td>14</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.079468</td>
      <td>0.000970</td>
      <td>0.003603</td>
      <td>2.003434e-04</td>
      <td>5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>13</td>
      <td>{'base_estimator__max_depth': 5, 'max_features...</td>
      <td>-56.678626</td>
      <td>-52.958267</td>
      <td>-48.569071</td>
      <td>-52.802426</td>
      <td>-52.683538</td>
      <td>-52.738386</td>
      <td>2.567650</td>
      <td>15</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.091579</td>
      <td>0.002170</td>
      <td>0.003303</td>
      <td>2.452428e-04</td>
      <td>5</td>
      <td>0.5</td>
      <td>1.0</td>
      <td>10</td>
      <td>{'base_estimator__max_depth': 5, 'max_features...</td>
      <td>-51.923348</td>
      <td>-54.387221</td>
      <td>-54.317287</td>
      <td>-59.113367</td>
      <td>-53.374348</td>
      <td>-54.623114</td>
      <td>2.414941</td>
      <td>16</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.192765</td>
      <td>0.000200</td>
      <td>0.004804</td>
      <td>2.449314e-04</td>
      <td>3</td>
      <td>1.0</td>
      <td>0.8</td>
      <td>20</td>
      <td>{'base_estimator__max_depth': 3, 'max_features...</td>
      <td>-57.046149</td>
      <td>-57.125298</td>
      <td>-55.750560</td>
      <td>-57.408820</td>
      <td>-54.716769</td>
      <td>-56.409519</td>
      <td>1.021359</td>
      <td>17</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.209280</td>
      <td>0.001498</td>
      <td>0.004404</td>
      <td>2.003909e-04</td>
      <td>3</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>19</td>
      <td>{'base_estimator__max_depth': 3, 'max_features...</td>
      <td>-57.110032</td>
      <td>-57.542201</td>
      <td>-56.171485</td>
      <td>-57.804065</td>
      <td>-54.725094</td>
      <td>-56.670575</td>
      <td>1.119968</td>
      <td>18</td>
    </tr>
    <tr>
      <th>0</th>
      <td>0.192165</td>
      <td>0.003165</td>
      <td>0.005004</td>
      <td>2.611745e-07</td>
      <td>3</td>
      <td>0.8</td>
      <td>0.8</td>
      <td>25</td>
      <td>{'base_estimator__max_depth': 3, 'max_features...</td>
      <td>-57.606682</td>
      <td>-57.253821</td>
      <td>-57.200298</td>
      <td>-57.434140</td>
      <td>-57.827740</td>
      <td>-57.464536</td>
      <td>0.231201</td>
      <td>19</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.093881</td>
      <td>0.002114</td>
      <td>0.003603</td>
      <td>2.001766e-04</td>
      <td>3</td>
      <td>0.5</td>
      <td>1.0</td>
      <td>15</td>
      <td>{'base_estimator__max_depth': 3, 'max_features...</td>
      <td>-60.743445</td>
      <td>-63.337419</td>
      <td>-62.410111</td>
      <td>-62.969337</td>
      <td>-59.640312</td>
      <td>-61.820125</td>
      <td>1.405829</td>
      <td>20</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE del regresor bagging tuneado: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{mean_absolute_error(y_test, y_pred):.2f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE del regresor bagging tuneado: 37.66 k$
</code></pre>
<p>Podemos ver que el predictor proporcionado por el regresor bagging no necesita de demasiado ajuste de hiperparámetros. No se consigue mejora con el tuneado.</p>
<h2 id="bosque-aleatorio"><a href="#bosque-aleatorio" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Bosque aleatorio</h2>
<p>Vamos a presentar los modelos random forest y mostraremos las diferencias con los conjuntos bagging. Random forest es un modelo muy popular en machine learning. Es una modificación del algoritmo bagging. En bagging, se puede usar cualquier clasificador o regresor. En random forest, el clasificador o regresor base es siempre un árbol de decisión.</p>
<p>Random forest tiene otra particularidad: cuando se entrena un árbol, la búsqueda de la mejor división se realiza solo en un subconjunto de las features originales tomadas al azar. Los subconjuntos aleatorios son diferentes para cada división de nodo. El objetivo es inyectar aleatoriedad adicional en el procedimiento de aprendizaje para intentar decorrelacionar los errores predichos de los árboles individuales.</p>
<p>Por tanto, random forest usan aleatoriedad en ambos ejes de la matriz de datos:</p>
<ul>
<li>en las muestras bootstrapping de cada árbol del bosque;</li>
<li>seleccionando aleatoriamente un subconjunto de features en cada nodo del árbol.</li>
</ul>
<h3 id="un-vistazo-a-random-forests"><a href="#un-vistazo-a-random-forests" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Un vistazo a random forests</h3>
<p>Ilustraremos el uso de un clasificador random forest en el dataset del censo de adultos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">adult_census</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/adult_census.csv&#34;</span><span class="p">)</span>
<span class="n">target_name</span> <span class="o">=</span> <span class="s2">&#34;class&#34;</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">adult_census</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">target_name</span><span class="p">,</span> <span class="s2">&#34;education-num&#34;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">adult_census</span><span class="p">[</span><span class="n">target_name</span><span class="p">]</span></code></pre></div>
<p>El dataset del censo de adultos contiene algunos datos categóricos y codificaremos las variables categóricas usando un <code>OrdinalEncoder</code>, dado que los modelos basados en árbol pueden trabajar muy eficientemente con esta representación tan simple de variables categóricas.</p>
<p>Dado que existen categorías raras en este dataset, necesitamos especificar la codificación de las categorías desconocidas en tiempo de predicción para que sea capaz de usar validación cruzada. De lo contrario, algunas categorías raras solo podrían estar presentes en el lado de validación de la división de validación cruzada y <code>OrdinalEncoder</code> podría lanzar un error cuando llame a su método <code>transform</code> con los puntos de datos del conjunto de validación.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_transformer</span><span class="p">,</span> <span class="n">make_column_selector</span>

<span class="n">categorial_encoder</span> <span class="o">=</span> <span class="n">OrdinalEncoder</span><span class="p">(</span>
    <span class="n">handle_unknown</span><span class="o">=</span><span class="s2">&#34;use_encoded_value&#34;</span><span class="p">,</span> <span class="n">unknown_value</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">categorial_encoder</span><span class="p">,</span> <span class="n">make_column_selector</span><span class="p">(</span><span class="n">dtype_include</span><span class="o">=</span><span class="nb">object</span><span class="p">)),</span>
    <span class="n">remainder</span><span class="o">=</span><span class="s2">&#34;passthrough&#34;</span>
<span class="p">)</span></code></pre></div>
<p>Haremos primero un ejemplo simple donde entrenaremos un único clasificador de árbol de decisión y comprobaremos su rendimiento de generalización a través de validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessor</span><span class="p">,</span>
                     <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">scores_tree</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Clasificador árbol decisión: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_tree.mean():.3f} +/- {scores_tree.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Clasificador árbol decisión: 0.812 +/- 0.002
</code></pre>
<p>De forma similar a como hemos hecho anteriormente, vamos a construir un <code>BaggingClassifier</code> con un clasificador de árbol de decisión como modelo base. Además, necesitamos especificar cuántos modelos queremos combinar. También necesitamos preprocesar los datos y, por tanto, usaremos un pipeline de scikit-learn.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>

<span class="n">bagged_trees</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">preprocessor</span><span class="p">,</span>
    <span class="n">BaggingClassifier</span><span class="p">(</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_bagged_trees</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">bagged_trees</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Clasificador arbol decisión bagged: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_bagged_trees.mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_bagged_trees.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Clasificador arbol decisión bagged: 0.853 +/- 0.003
</code></pre>
<p>El rendimiento de generalización de los árboles bagged es mucho mejor que el rendimiento de un único árbol.</p>
<p>Ahora vamos a usar random forest. Observaremos que no necesitamos especificar ningún <code>base_estimator</code> porque el estimador se fuerza que sea un árbol de decisión. Por tanto, sólo necesitamos especificar el número deseado de árboles del bosque.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">preprocessor</span><span class="p">,</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_random_forest</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Clasificador random forest: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_random_forest.mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_random_forest.std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Clasificador random forest: 0.856 +/- 0.002
</code></pre>
<p>Parece que el rendimiento de random forest es ligeramente superior a los árboles bagged, posiblemente debido a la selección aleatoria de las variables que decorrelacionan los errores de predicción de los árboles individuales y, en consecuencia, hacen el paso de promediado más eficiente al reducir el overfitting.</p>
<h3 id="detalles-sobre-los-hiperparámetros-por-defecto"><a href="#detalles-sobre-los-hiperparámetros-por-defecto" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Detalles sobre los hiperparámetros por defecto</h3>
<p>Para random forest es posible controlar la cantidad de aleatoriedad para cada división estableciendo el valor del hiperparámetro <code>max_features</code>:</p>
<ul>
<li><code>max_features=0.5</code> significa que se considerarán el 50% de las features en cada división;</li>
<li><code>max_features=1.0</code> significa que se considerarán todas las features en cada división, lo que realmente deshabilita el submuestreo de features.</li>
</ul>
<p>Por defecto, <code>RandomForestRegressor</code> deshabilita el submuestreo de features mientras que <code>RandomForestClassifier</code> usa <code>max_features=np.sqrt(n_features)</code>. Estos valores por defecto reflejan buenas prácticas dadas en la literatura científica.</p>
<p>Sin embargo, <code>max_features</code> es uno de los hiperparámetros a tener en consideración cuando se ajusta un random forest:</p>
<ul>
<li>demasiada aleatoriedad en los árboles puede conducir a modelos base con underfitting y puede ir en detrimento del conjunto en su totalidad,</li>
<li>muy poca aleatoriedad en los árboles conduce a mayor correlación de los errores de predicción y, como resultado, a reducir los beneficios del paso de promediado en términos de control del overfitting.</li>
</ul>
<p>En scikit-learn, las clases bagging también exponen un parámetro <code>max_features</code>. Sin embargo, <code>BaggingClassifier</code> y <code>BaggingRegressor</code> son agnósticos respecto a sus modelos base y, por tanto, el submuestro de features aleatorio solo puede producirse una vez antes de entrenar cada modelo base, en lugar de varias veces por modelo base como es el caso cuando se añaden divisiones a un árbol dado.</p>
<p>Se pueden resumir estos detalles en la siguiente tabla:</p>
<div class="table-container"><table>
<thead>
<tr>
<th>Clase modelo de conjunto</th>
<th>Clase modelo base</th>
<th>Valor por defecto para <code>max_features</code></th>
<th>Estrategia de submuestreo de features</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BaggingClassifier</code></td>
<td>especificado por usuario (flexible)</td>
<td><code>n_features</code> (sin submuestreo)</td>
<td>Nivel de modelo</td>
</tr>
<tr>
<td><code>RandomForestClassifier</code></td>
<td><code>DecisionTreeClassifier</code></td>
<td><code>sqrt(n_features)</code></td>
<td>Nivel de nodo de árbol</td>
</tr>
<tr>
<td><code>BaggingRegressor</code></td>
<td>especificado por usuario (flexible)</td>
<td><code>n_features</code> (sin submuestreo)</td>
<td>Nivel de modelo</td>
</tr>
<tr>
<td><code>RandomForestRegressor</code></td>
<td><code>DecisionTreeRegressor</code></td>
<td><code>n_features</code> (sin submuestreo)</td>
<td>Nivel de nodo de árbol</td>
</tr>
</tbody>
</table></div>
<h3 id="ejercicio-random-forest"><a href="#ejercicio-random-forest" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio random forest</h3>
<p>Vamos a realizar un pequeño ejercicio de random forest. El objetivo es explorar algunos atributos disponibles en random forest de scikit-learn. Vamos a usar el dataset de regresión de pingüinos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/penguins_regression.csv&#34;</span><span class="p">)</span>
<span class="n">feature_name</span> <span class="o">=</span> <span class="s2">&#34;Flipper Length (mm)&#34;</span>
<span class="n">target_name</span> <span class="o">=</span> <span class="s2">&#34;Body Mass (g)&#34;</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="n">feature_name</span><span class="p">]],</span> <span class="n">penguins</span><span class="p">[</span><span class="n">target_name</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<p>Crearemos un random forest conteniendo 3 árboles. Entrenaremos el bosque y comprobaremos el rendimiento de generalización en el conjunto de prueba en términos de error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;MAE: {mean_absolute_error(y_test, y_pred):.3f} g&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>MAE: 345.349 g
</code></pre>
<p>Lo siguiente que haremos será:</p>
<ul>
<li>crear un dataset que contenga los pinguinos con un tamaño de aleta entre 170 mm y 230 mm,</li>
<li>dibujar los datos de entrenamiento usando un scatter plot,</li>
<li>dibujar la decisión de cada árbol individual prediciendo en el recién creado dataset,</li>
<li>dibujar la decisión del random forest usando este recién creado dataset.</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">data_range</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">170</span><span class="p">,</span> <span class="mi">235</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">300</span><span class="p">),</span>
                          <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">tree_preds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
    <span class="n">tree_preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_range</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()))</span>

<span class="n">random_forest_preds</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_range</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">penguins</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">feature_name</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">target_name</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Dibuja las predicciones de los árboles</span>
<span class="k">for</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">predictions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tree_preds</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_range</span><span class="p">[</span><span class="n">feature_name</span><span class="p">],</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">f</span><span class="s2">&#34;Arbol #{tree_idx}&#34;</span><span class="p">,</span>
             <span class="n">linestyle</span><span class="o">=</span><span class="s2">&#34;--&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_range</span><span class="p">[</span><span class="n">feature_name</span><span class="p">],</span> <span class="n">random_forest_preds</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">f</span><span class="s2">&#34;Random forest&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_86_0.png" alt="png"></p>
<h1 id="conjuntos-basados-en-boosting"><a href="#conjuntos-basados-en-boosting" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Conjuntos basados en boosting</h1>
<h2 id="boosting-adaptativo-adaboost"><a href="#boosting-adaptativo-adaboost" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Boosting adaptativo (AdaBoost)</h2>
<p>Vamos a presentar el algoritmo Boosting Adaptativo (AdaBoost). El objetivo es obtener información observando la maquinaria interna de AdaBoots y boosting en general.</p>
<p>Cargaremos el dataset de pingüinos y predeciremos su especie a partir de las features de longitud y anchura del pico.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">penguins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/penguins_classification.csv&#34;</span><span class="p">)</span>
<span class="n">culmen_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;Culmen Length (mm)&#34;</span><span class="p">,</span> <span class="s2">&#34;Culmen Depth (mm)&#34;</span><span class="p">]</span>
<span class="n">target_column</span> <span class="o">=</span> <span class="s2">&#34;Species&#34;</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="n">culmen_columns</span><span class="p">],</span> <span class="n">penguins</span><span class="p">[</span><span class="n">target_column</span><span class="p">]</span></code></pre></div>
<p>Entrenaremos un árbol de decisión poco profundo intencionadamente. Dada su poca profundidad, es poco probable que tenga overfitting y algunas de las muestras de entrenamiento incluso serán mal clasificadas.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">palette</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;tab:red&#34;</span><span class="p">,</span> <span class="s2">&#34;tab:blue&#34;</span><span class="p">,</span> <span class="s2">&#34;black&#34;</span><span class="p">]</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>DecisionTreeClassifier(max_depth=2, random_state=0)
</code></pre>
<p>Podemos predecir en el mismo dataset y comprobar qué muestras están mal clasificadas.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">misclassified_samples_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">X_misclassified</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">misclassified_samples_idx</span><span class="p">]</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">helpers.plotting</span> <span class="kn">import</span> <span class="n">DecisionBoundaryDisplay</span>

<span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&#34;predict&#34;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&#34;RdBu&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>

<span class="c1"># dibuja el dataset original</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">penguins</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">hue</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">)</span>
<span class="c1"># Dibuja los ejemplos mal clasificados</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X_misclassified</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Muestras mal clasificadas&#34;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s2">&#34;+&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;center left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones álbol decisión </span><span class="se">\n</span><span class="s2">destacando muestras mal &#34;</span>
              <span class="s2">&#34;clasificadas&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_95_0.png" alt="png"></p>
<p>Observamos que hay varios ejemplos que han sido mal clasificados por el clasificador.</p>
<p>Mencionamos que boosting se basa en la creación de un nuevo clasificador que intenta corregir esas clasificaciones erróneas. En scikit-learn, los aprendices tiene un parámetro <code>sample_weight</code> que fuerza a prestar más atención a los ejemplos con pesos altos durante el entrenamiento.</p>
<p>Este parámetro se establece cuando se llama a <code>classifier.fit(X, y, sample_weight=weights)</code>. Usaremos este truco para crear un nuevo clasificador &ldquo;descartando&rdquo; todas las muestras clasificadas correctamente y considerando únicamente las mal clasificadas. Así, a las muestras mal clasificadas se asignará un peso de 1 y a las bien clasificadas un peso de 0.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">sample_weight</span><span class="p">[</span><span class="n">misclassified_samples_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span></code></pre></div>
<pre><code>DecisionTreeClassifier(max_depth=2, random_state=0)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&#34;predict&#34;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&#34;RdBu&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">penguins</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">hue</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">X_misclassified</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Muestras mal clasificadas previamente&#34;</span><span class="p">,</span>
                <span class="n">marker</span><span class="o">=</span><span class="s2">&#34;+&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;center left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Arbol decisión cambiando los pesos de las muestras&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_98_0.png" alt="png"></p>
<p>Vemos que la función de decisión ha cambiado drásticamente. Cualitativamente vemos que los ejemplos que previamente estaban mal clasificados ahora son clasificados correctamente.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">newly_misclassified_samples_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">y</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">remaining_misclassified_samples_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">intersect1d</span><span class="p">(</span>
    <span class="n">misclassified_samples_idx</span><span class="p">,</span> <span class="n">newly_misclassified_samples_idx</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Número de muestras mal clasificadas previamente y &#34;</span>
      <span class="n">f</span><span class="s2">&#34;todavía mal clasificadas: {len(remaining_misclassified_samples_idx)}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Número de muestras mal clasificadas previamente y todavía mal clasificadas: 0
</code></pre>
<p>Sin embargo, estamos cometiendo errores en muestras bien clasificadas previamente. Por tanto, tenemos la intuición de que debemos ponderar las predicciones de cada clasificador de forma diferente, muy probablemente utilizando el número de errores que comete cada clasificador.</p>
<p>Entonces podríamos usar el error de clasificación para combinar ambos árboles.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ensemble_weight</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">misclassified_samples_idx</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">newly_misclassified_samples_idx</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">ensemble_weight</span></code></pre></div>
<pre><code>[0.935672514619883, 0.6929824561403509]
</code></pre>
<p>El primer clasificador tiene una precisión del 93,5% y el segundo una precisión de 69,2%. Por lo tanto, cuando predecimos una clase, debemos confiar un poco más en el primer clasificador que en el segundo. Podríamos usar estos valores de precisión para ponderar las predicciones de cada aprendiz.</p>
<p>Para resumir, boosting entrena varios clasificadores, cada uno de los cuales se enfocará más o menos en muestras específicas del dataset. Boosting es, por tanto, diferente de bagging: aquí nunca se remuestrea nuestro dataset, solo asignamos pesos diferentes al dataset original.</p>
<p>Boosting requiere alguna estrategia para combinar los aprendices juntos:</p>
<ul>
<li>necesita definir una forma de calcular los pesos que serán asignados a las muestras;</li>
<li>necesita asignar un peso a cada aprendiz al hacer predicciones.</li>
</ul>
<p>De hecho, definimos un esquema realmente simple para asignar pesos a las muestras y pesos a los aprendices. Sin embargo, existen teorías estadísticas (como en AdaBoost) sobre cómo se deben calcular óptimamente estos pesos.</p>
<p>Usaremos el clasificador AdaBoots implementado en scikit-learn y revisaremos los clasificadores de árbol de decisión entrenados subyacentes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="n">base_estimator</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">adaboost</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
                              <span class="n">n_estimators</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s2">&#34;SAMME&#34;</span><span class="p">,</span>
                              <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">adaboost</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>AdaBoostClassifier(algorithm='SAMME',
                   base_estimator=DecisionTreeClassifier(max_depth=3,
                                                         random_state=0),
                   n_estimators=3, random_state=0)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">boosting_round</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">adaboost</span><span class="o">.</span><span class="n">estimators_</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="c1"># covertimos &#34;X&#34; en un array Numpy para eviar el warning lanzado por scikit-learn</span>
    <span class="n">DecisionBoundaryDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
        <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">response_method</span><span class="o">=</span><span class="s2">&#34;predict&#34;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&#34;RdBu&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">culmen_columns</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="n">hue</span><span class="o">=</span><span class="n">target_column</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">penguins</span><span class="p">,</span>
                    <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.04</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;center left&#34;</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Arbol decisión entrenado en ronda {boosting_round}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
<p><img src="/images/output_105_1.png" alt="png"></p>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
<p><img src="/images/output_105_3.png" alt="png"></p>
<pre><code>&lt;Figure size 432x288 with 0 Axes&gt;
</code></pre>
<p><img src="/images/output_105_5.png" alt="png"></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Peso de cada clasificador: {adaboost.estimator_weights_}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Peso de cada clasificador: [3.58351894 3.46901998 3.03303773]
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error de cada clasificador: {adaboost.estimator_errors_}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Error de cada clasificador: [0.05263158 0.05864198 0.08787269]
</code></pre>
<p>Vemos que AdaBoost ha entrenado tres clasificadores diferentes, cada uno de los cuales se enfoca en muestras diferentes. Revisando los pesos de cada aprendiz, vemos que el conjunto ofrece el mayor peso al primer clasificador. Esto de hecho tiene sentido cuando revisamos los errores de cada clasificador. El primer clasificador también obtiene la clasificación más alta en rendimiento de generalización.</p>
<p>Aunque AdaBoost es un buen algoritmo para demostrar la maquinaria interna de los algoritmos de boosting, no es el más eficiente. Este título se otorga al algoritmo de árbol de decisión gradient-boosting (GBDT).</p>
<h2 id="arbol-de-decisión-gradient-boosting-gbdt"><a href="#arbol-de-decisión-gradient-boosting-gbdt" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Arbol de decisión gradient-boosting (GBDT)</h2>
<p>Vamos a ver el algoritmo de árbol de decisión gradient boosting y lo compararemos con AdaBoost.</p>
<p>Gradient-boosting difiere de AdaBoost en lo siguiente: en lugar de asignar pesos a muestras específicas, GBDT entrenará un árbol de decisión en los errores residuales (de ahí el nombre &ldquo;gradiente&rdquo;) del árbol anterior. Por lo tanto, cada nuevo árbol del conjunto predice el error cometido por el anterior en lugar de predecir directamente el objetivo.</p>
<p>Vamos a proporcionar algunas intuiciones sobre la forma en que se combinan los aprendices para proporcionar la predicción final.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Crea un generador de número aleatorio que utilizaremos para establecer la aleatoriedad</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Genera dataset sintético. Devuelve `X_train`, `X_test`,
</span><span class="s2">    `y_train`.&#34;&#34;&#34;</span>
    <span class="n">x_max</span><span class="p">,</span> <span class="n">x_min</span> <span class="o">=</span> <span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4</span>
    <span class="n">len_x</span> <span class="o">=</span> <span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">len_x</span> <span class="o">-</span> <span class="n">len_x</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.3</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">noise</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">])</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_max</span><span class="p">,</span> <span class="n">x_min</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">300</span><span class="p">),</span>
                             <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">])</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Target&#34;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span>


<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">()</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Dataset de regresión sintético&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_112_0.png" alt="png"></p>
<p>Como vimos anteriormente, boosting se basa en ensamblar una secuencia de aprendices. Empezaremos creando un regresor de arbol de decisión. Estableceremos la profundidad del árbol para que el aprendiz resultante produzca underfitting.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></div>
<p>Usamos el término &ldquo;test&rdquo; para referirnos a datos que no se han usado para el entrenamiento. No debemos confundirlo con datos provenientes de una división entrenamiento-prueba, ya que se generó en intervalos espaciados equitativamente para la evaluación visual de las predicciones.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># dibuja los datos</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># dibuja las predicciones</span>
<span class="n">line_predictions</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">)</span>

<span class="c1"># dibuja los residuos</span>
<span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">true</span><span class="p">,</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span>
                                  <span class="n">y_train</span><span class="p">,</span>
                                  <span class="n">y_train_pred</span><span class="p">):</span>
    <span class="n">lines_residuals</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="p">[</span><span class="n">true</span><span class="p">,</span> <span class="n">predicted</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">line_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lines_residuals</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
           <span class="p">[</span><span class="s2">&#34;Arbol entrenado&#34;</span><span class="p">,</span> <span class="s2">&#34;Residuos&#34;</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Función predicción junto </span><span class="se">\n</span><span class="s2">con errores en el conjunto entrenamiento&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_116_0.png" alt="png"></p>
<p>Hemos editado manualmente la leyenda para obtener solo una única etiqueta para todas las líneas de residuos.</p>
<p>Dado que el arbol tiene underfit, su precisión está lejos de la perfección en los datos de entrenamiento. Podemos observar esto en el gráfico viendo las diferencias entre las predicciones y los datos reales. Estos errores, llamados residuos, se presentan mediante líneas rojas continuas.</p>
<p>De hecho, nuestro árbol inicial no era suficientemente expresivo para manejar la complejidad de los datos, como muestran los residuos. En un algoritmo grandient-boosting, la idea es crear un segundo árbol que, dados los mismos datos <code>X</code>, intentará predecir los residuos en lugar del vector <code>y</code>. Tendríamos, por tanto, un árbol que es capaz de predecir los errores que comete el árbol inicial.</p>
<p>Vamos a entrenar un árbol de estas características.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">residuals</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">-</span> <span class="n">y_train_pred</span>

<span class="n">tree_residuals</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree_residuals</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>

<span class="n">target_train_predicted_residuals</span> <span class="o">=</span> <span class="n">tree_residuals</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">target_test_predicted_residuals</span> <span class="o">=</span> <span class="n">tree_residuals</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">residuals</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">line_predictions</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">target_test_predicted_residuals</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">)</span>

<span class="c1"># dibuja los residuos de los residuos predichos</span>
<span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">true</span><span class="p">,</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span>
                                  <span class="n">residuals</span><span class="p">,</span>
                                  <span class="n">target_train_predicted_residuals</span><span class="p">):</span>
    <span class="n">lines_residuals</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="p">[</span><span class="n">true</span><span class="p">,</span> <span class="n">predicted</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">line_predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lines_residuals</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
           <span class="p">[</span><span class="s2">&#34;Arbol entrenado&#34;</span><span class="p">,</span> <span class="s2">&#34;Residuos&#34;</span><span class="p">],</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span>
           <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones de los residuos anteriores&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_119_0.png" alt="png"></p>
<p>Vemos que este nuevo árbol solo logra manejar algunos de los residuos. Nos enfocaremos en una muestra específica del conjunto de entrenamiento (es decir, sabemos que la muestra será bien predicha usando dos árboles sucesivos). Usaremos este ejemplo para explicar cómo se combinan las predicciones de ambos árboles. Primero seleccionaremos esta muestra en <code>X_train</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sample</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
<span class="n">x_sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">target_true</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="n">target_true_residual</span> <span class="o">=</span> <span class="n">residuals</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span></code></pre></div>
<p>Vamos a dibujar la información previa y destacaremos nuestra muestra. Empezaremos dibujando los datos originales y la predicción del primer árbol de decisión.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Dibuja la información previa:</span>
<span class="c1">#   * el dataset</span>
<span class="c1">#   * las predicciones</span>
<span class="c1">#   * los residuos</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span>
                <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">true</span><span class="p">,</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span>
                                  <span class="n">y_train</span><span class="p">,</span>
                                  <span class="n">y_train_pred</span><span class="p">):</span>
    <span class="n">lines_residuals</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="p">[</span><span class="n">true</span><span class="p">,</span> <span class="n">predicted</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>

<span class="c1"># Destacamos la muestra de interés</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">target_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Muestra de interés&#34;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.05</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&#34;upper left&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicciones del árbol&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_123_0.png" alt="png"></p>
<p>Ahora dibujaremos la información de los residuos. Dibujaremos los residuos calculados por el primer árbol de decisión y mostraremos las predicciones de residuos.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Dibuja la información previa:</span>
<span class="c1">#   * los residuso cometidos por el primer árbol</span>
<span class="c1">#   * las predicciones de residuos</span>
<span class="c1">#   * los residuos de las predicciones de residuos</span>

<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">residuals</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span> <span class="n">target_test_predicted_residuals</span><span class="p">,</span> <span class="s2">&#34;--&#34;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">true</span><span class="p">,</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s2">&#34;Feature&#34;</span><span class="p">],</span>
                                  <span class="n">residuals</span><span class="p">,</span>
                                  <span class="n">target_train_predicted_residuals</span><span class="p">):</span>
    <span class="n">lines_residuals</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">value</span><span class="p">],</span> <span class="p">[</span><span class="n">true</span><span class="p">,</span> <span class="n">predicted</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>

<span class="c1"># Destaca la muestra de interés</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">target_true_residual</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Muestra de interés&#34;</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Predicción de los residuos&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_125_0.png" alt="png"></p>
<p>Para nuestra muestra de interés, nuestro árbol inicial cometía un error (residuo pequeño). Cuando entrenamos el segundo árbol, en este caso el residuo se entrena y se predice perfectamente. Verificaremos cuantitativamente esta predicción usando el árbol entrenado. Primero, verificaremos la predicción de árbol inicial y compararemos con su valor real.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;El valor real a predecir para &#34;</span>
      <span class="n">f</span><span class="s2">&#34;f(x={x_sample:.3f}) = {target_true:.3f}&#34;</span><span class="p">)</span>

<span class="n">y_pred_first_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Predicción del primer árbol de decisión para x={x_sample:.3f}: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;y={y_pred_first_tree:.3f}&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error del árbol: {target_true - y_pred_first_tree:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>El valor real a predecir para f(x=-0.517) = -0.393
Predicción del primer árbol de decisión para x=-0.517: y=-0.145
Error del árbol: -0.248
</code></pre>
<p>Como observamos visualmente, tenemos un pequeño error. Ahora, usaremos el segundo árbol para intentar predecir este residuo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Predicción del residuo para x={x_sample:.3f}: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{tree_residuals.predict(sample)[0]:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Predicción del residuo para x=-0.517: -0.248
</code></pre>
<p>Vemos que nuestro segundo árbol es capaz de predecir el residuo exacto (error) de nuestro primer árbol. Por tanto, podemos predecir el valor de <code>x</code> sumando la predicción de todos los árboles del conjunto.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred_first_and_second_tree</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">y_pred_first_tree</span> <span class="o">+</span> <span class="n">tree_residuals</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Predicción del primer y segundo árbol de decisión combinados para &#34;</span>
      <span class="n">f</span><span class="s2">&#34;x={x_sample:.3f}: y={y_pred_first_and_second_tree:.3f}&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error del árbol: {target_true - y_pred_first_and_second_tree:.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Predicción del primer y segundo árbol de decisión combinados para x=-0.517: y=-0.393
Error del árbol: 0.000
</code></pre>
<p>Elegimos una muestra para la cual solo fueron suficientes dos árboles para hacer una predicción perfecta. Sin embargo, vimos anteriormente en el gráfico anterior que dos árboles no son suficientes para corregir los residuos de todas las muestras. Por tanto, se necesita añadir varios árboles al conjunto para corregir adecuadamente el error (es decir, el segundo árbol corrige el error del primer árbol, mientras que el tercer árbol corrige los errores del segundo árbol y así sucesivamente).</p>
<p>Compararemos el rendimiento de generalización de random forest y gradient boosting en el dataset de propiedades de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala el objetivo a k$</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="n">gradient_boosting</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">cv_results_gbdt</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">gradient_boosting</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Arbol decisión gradient boosting&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto medio a través de validación cruzada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_gbdt[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_gbdt[&#39;test_score&#39;].std():.3f} k$&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Tiempo de entrenamiento medio: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;fit_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Tiempo de puntuación medio: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;score_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Arbol decisión gradient boosting
Error absoluto medio a través de validación cruzada: 46.408 +/- -2.907 k$
Tiempo de entrenamiento medio: 6.463 seconds
Tiempo de puntuación medio: 0.008 seconds
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cv_results_rf</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">random_forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Random forest&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto medio a través de validación cruzada: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_rf[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_rf[&#39;test_score&#39;].std():.3f} k$&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Tiempo de entrenamiento medio: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_rf[&#39;fit_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Tiempo de puntuación medio: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_rf[&#39;score_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Random forest
Error absoluto medio a través de validación cruzada: 46.510 +/- -4.689 k$
Tiempo de entrenamiento medio: 7.761 seconds
Tiempo de puntuación medio: 0.216 seconds
</code></pre>
<p>En términos de rendimiento computacional, el bosque se puede paralelizar, beneficiándose del uso de múltiples cores de CPU. En términos de rendimiento de puntuación, ambos algoritmos conducen a resultados muy cercanos.</p>
<p>Sin embargo, vemos que gradient boosting es un algoritmo muy rápido para predecir comparado con random forest. Esto se debe al hecho de que gradient boosting usa árboles poco profundos.</p>
<h2 id="ejercicio-de-boosting"><a href="#ejercicio-de-boosting" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio de boosting</h2>
<p>Vamos a realizar un ejercicio donde verificaremos si un random forest o un árbol de decisión gradient boosting producen overfitting si el número de estimadores no se elige apropiadamente. Para obtener los mejores rendimientos de generalización, usaremos la estrategia de parada temprana (<em>early-stopping</em>) para evitar añadir innecesariamente árboles.</p>
<p>Para conducir nuestro experimento, usaremos el dataset de propiedades de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala el objetivo a k$</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span></code></pre></div>
<p>Vamos a crear un árbol de decisión gradient-boosting con <code>max_depth=5</code> y <code>learning_rate=0.5</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="n">gradient_boosting</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span></code></pre></div>
<p>También crearemos un random forest con árboles completamente desarrollados estableciendo <code>max_depth=None</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span></code></pre></div>
<p>Para ambos modelos, crearemos una curva de validación usando el conjunto de entrenamiento para comprobar el impacto del número de árboles en el rendimiento de cada modelo.</p>
<p>Evaluaremos la lista de parámetros <code>param_range = [1, 2, 5, 10, 20, 50, 100]</code> y usaremos el error absoluto medio.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">validation_curve</span>

<span class="n">param_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="n">gradient_boosting_train_scores</span><span class="p">,</span> <span class="n">gradient_boosting_val_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
    <span class="n">gradient_boosting</span><span class="p">,</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">param_name</span><span class="o">=</span><span class="s2">&#34;n_estimators&#34;</span><span class="p">,</span>
    <span class="n">param_range</span><span class="o">=</span><span class="n">param_range</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">gradient_boosting_train_errors</span><span class="p">,</span> <span class="n">gradient_boosting_val_errors</span> <span class="o">=</span> <span class="o">-</span><span class="n">gradient_boosting_train_scores</span><span class="p">,</span> <span class="o">-</span><span class="n">gradient_boosting_val_scores</span>

<span class="n">random_forest_train_scores</span><span class="p">,</span> <span class="n">random_forest_val_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
    <span class="n">random_forest</span><span class="p">,</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">param_name</span><span class="o">=</span><span class="s2">&#34;n_estimators&#34;</span><span class="p">,</span>
    <span class="n">param_range</span><span class="o">=</span><span class="n">param_range</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">random_forest_train_errors</span><span class="p">,</span> <span class="n">random_forest_val_errors</span> <span class="o">=</span> <span class="o">-</span><span class="n">random_forest_train_scores</span><span class="p">,</span> <span class="o">-</span><span class="n">random_forest_val_scores</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">gradient_boosting_train_errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">gradient_boosting_train_errors</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">gradient_boosting_val_errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">gradient_boosting_val_errors</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Validación cruzada&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Arbol decisión gradient boosting&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;# estimadores&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;Error absoluto medio en k$</span><span class="se">\n</span><span class="s2">(cuanto más pequeño mejor)&#34;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">random_forest_train_errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">random_forest_train_errors</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">random_forest_val_errors</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">random_forest_val_errors</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Cross-validation&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Random forest&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;# estimadores&#34;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&#34;Curvas de validación&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_148_0.png" alt="png"></p>
<p>Tanto gradient boosting como random forest mejorarán siempre cuando se incrementa el número de árboles del conjunto. Sin embargo, se alcanza una meseta donde añadir nuevos árboles solo provoca que se ralentice el entrenamiento y el tiempo de puntuación.</p>
<p>Para evitar añadir nuevos árboles innecesariamente, a diferencia de random forest, gradient boosting ofrece una opción de parada temprana. Internamente, el algoritmo usará un conjunto externo de muestras para calcular el rendimiento de generalización del modelo en cada adicción de un arbol. Por tanto, si el rendimiento de generalización no mejora en varias iteraciones, dejará de agregar árboles.</p>
<p>Ahora, crearemos un modelo gradient boosting con <code>n_estimators=1_000</code>. Este número de árboles será demasiado grande. Cambiaremos el parámetro <code>n_iter_no_change</code> de forma que gradient boosting detendrá el entrenamiento tras añadir 5 árboles sin obtener mejora en el rendimiento de generalización global.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">gradient_boosting</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">gradient_boosting</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">gradient_boosting</span><span class="o">.</span><span class="n">n_estimators_</span></code></pre></div>
<pre><code>127
</code></pre>
<p>Vemos que el número de árboles usados está bastante por debajo de 1000 con el dataset actual. Entrenar el modelo gradient boosting con el total de 1000 árboles hubiera sido inútil.</p>
<p>Vamos a estimar otra vez el rendimiento de generalización de este modelo usando la métrica <code>sklearn.metrics.mean_absolute_error</code>, pero esta vez usando el conjunto de prueba y compararemos el valor resultante con los valores observados en la curva de validación.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">error</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">gradient_boosting</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;De media, nuestro regresor Gradient Boosting tiene un error de {error:.2f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>De media, nuestro regresor Gradient Boosting tiene un error de 36.58 k$
</code></pre>
<p>Observamos que la medida del MAE en el conjunto de prueba está cercano al error de validación medido en el lado derecho de la curva de validación. Esto es tranquilizador, ya que significa que tanto el procedimiento de validación cruzada como la división externa entrenamiento-prueba coinciden aproximadamente como aproximaciones al rendimiento de generalización verdadero del modelo. Podemos observar que la evaluación final del error de prueba parece estar incluso ligeramente por debajo de las puntuaciones de prueba de validación cruzada. Esto puede explicarse porque el modelo final ha sido entrenado en el conjunto completo de entrenamiento mientras que los modelos de validación cruzada han sido entrenamos en subconjuntos más pequeños: en general, cuanto mayor sea el número de puntos de entrenamiento, menor es el error de prueba.</p>
<h2 id="acelerando-gradient-boosting"><a href="#acelerando-gradient-boosting" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Acelerando gradient boosting</h2>
<p>Vamos a presentar una versión modificada de gradient boosting que usa un número reducido de divisiones cuando construye los diferentes árboles. Este algoritmo se denomina &ldquo;histogram gradient boosting&rdquo; en scikit-learn.</p>
<p>Hemos mencionado anteriormente que random forest es un eficiente algoritmo dado que los árboles del conjunto se pueden entrenar al mismo tiempo de forma independiente. Por tanto, el algoritmo escala eficientemente tanto con el número de cores como con el número de muestras.</p>
<p>En gradient boosting, el algoritmo es secuencial. Requiere <code>N-1</code> árboles entrenados para ser capaz de entrenar el árbol de la fase <code>N</code>. Por tanto, el algoritmo es bastante costoso desde el punto de vista computacional. La parte más costosa es la búsqueda de la mejor división del árbol, que se realiza con un enfoque de fuerza bruta: se evalúan todas las posibles divisiones y se elige la mejor.</p>
<p>Para acelerar el algoritmo gradient boosting, podríamos reducir el número de divisiones a evaluar. Como consecuencia, el rendimiento de generalización de un árbol de este tipo también se reduciría. Sin embargo, dado que tenemos varios árboles combinados en gradient boosting, podemos añadir más estimadores para superar este problema.</p>
<p>Realizaremos una implementación simple de este algoritmo construyendo bloques de scikit-learn. En primer lugar, carguemos el dataset de propiedades de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala el objetivo en k$</span></code></pre></div>
<p>Vamos a realizar una rápida comparativa del gradient boosting original.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="n">gradient_boosting</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">cv_results_gbdt</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">gradient_boosting</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Arbol Decisión Gradient Boosting&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto medio via cross-validation: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_gbdt[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;test_score&#39;].std():.3f} k$&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average fit time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;fit_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average score time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;score_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Arbol Decisión Gradient Boosting
Error absoluto medio via cross-validation: 46.407 +/- 2.908 k$
Average fit time: 6.512 seconds
Average score time: 0.008 seconds
</code></pre>
<p>Recordemos que una forma de acelerar gradient boosting es reducir el número de divisiones a considerar dentro de la estructura del árbol. Una forma es agrupar los datos antes de pasarlos a gradient boosting. Un transformador llamado <code>KBinsDiscretizer</code> se encarga de tal transformación. Por lo tanto, podemos usar un pipeline para este preprocesamiento de gradient boosting.</p>
<p>En primer lugar, vamos a desmostrar la transformación realizada por <code>KBinsDiscretizer</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">KBinsDiscretizer</span>

<span class="n">discretizer</span> <span class="o">=</span> <span class="n">KBinsDiscretizer</span><span class="p">(</span>
    <span class="n">n_bins</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s2">&#34;ordinal&#34;</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&#34;quantile&#34;</span><span class="p">)</span>
<span class="n">X_trans</span> <span class="o">=</span> <span class="n">discretizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_trans</span></code></pre></div>
<pre><code>C:\Program Files\Python310\lib\site-packages\sklearn\preprocessing\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.
  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\preprocessing\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.
  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\preprocessing\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.
  warnings.warn(
C:\Program Files\Python310\lib\site-packages\sklearn\preprocessing\_discretization.py:230: UserWarning: Bins whose width are too small (i.e., &lt;= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.
  warnings.warn(





array([[249.,  39., 231., ...,  83., 162.,  30.],
       [248.,  19., 203., ...,  28., 161.,  30.],
       [242.,  49., 249., ..., 125., 160.,  29.],
       ...,
       [ 17.,  15., 126., ...,  49., 200.,  82.],
       [ 23.,  16., 136., ...,  29., 200.,  77.],
       [ 53.,  14., 130., ...,  93., 199.,  81.]])
</code></pre>
<p>El código anterior generará una serie de alertas. De hecho para algunas features, solicitamos demasiados contenedores (bins) con respecto a la dispersión de los datos para dichas features. Los pequeños contenedores se eliminarán.</p>
<p>Vemos que <em>discretizer</em> transforma los datos originales en valores enteros (aunque estén codificados usando una representación de coma flotante). Cada valor representa el índice de contenedor cuando se realiza la distribución por cuantiles. Podemos verificar el número de contenedores por feature.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">col</span><span class="p">))</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_trans</span><span class="o">.</span><span class="n">T</span><span class="p">]</span></code></pre></div>
<pre><code>[256, 50, 256, 253, 256, 256, 207, 235]
</code></pre>
<p>Después de la transformación, vemos que tenemos una mayoría de 256 valores únicos por feature. Ahora, usaremos el transformador para discretizar los datos antes del entrenamiento del regresor gradient boosting.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">gradient_boosting</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">discretizer</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">))</span>
<span class="n">cv_results_gbdt</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">gradient_boosting</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Arbol Decisión Gradient Boosting con KBinsDiscretizer&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto medio via cross-validation: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_gbdt[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;test_score&#39;].std():.3f} k$&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average fit time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;fit_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average score time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_gbdt[&#39;score_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Arbol Decisión Gradient Boosting con KBinsDiscretizer
Error absoluto medio via cross-validation: 46.130 +/- 2.230 k$
Average fit time: 4.108 seconds
Average score time: 0.010 seconds
</code></pre>
<p>Vemos que el tiempo de entrenamiento se ha reducido, pero el rendimiento de generalización del modelo es muy parecido. Scikit-learn proporciona clases específicas que están aún más optimizadas para grandes datasets, llamadas <code>HistGradientBoostingClassifier</code> y <code>HistGradientBoostingRegressor</code>. Cada feature del dataset <code>X</code> se agrupa primero calculando los histogramas, que se usarán posteriormente para evaluar potenciales divisiones. El número de divisiones a evaluar es entonces mucho más pequeño. Este algoritmo se convierte en mucho más eficiente que el de gradient boosting cuando el dataset tiene más de 10.000 muestras.</p>
<p>Vamos a dar un ejemplo de dataset grande y compararemos los tiempos de cálculo con el experimento anterior.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="n">histogram_gradient_boosting</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results_hgbdt</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">histogram_gradient_boosting</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Arbol Decisión Histogram Gradient Boosting&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Error absoluto medio via cross-validation: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{-cv_results_hgbdt[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_hgbdt[&#39;test_score&#39;].std():.3f} k$&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average fit time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_hgbdt[&#39;fit_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Average score time: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{cv_results_hgbdt[&#39;score_time&#39;].mean():.3f} seconds&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Arbol Decisión Histogram Gradient Boosting
Error absoluto medio via cross-validation: 43.758 +/- 2.694 k$
Average fit time: 1.000 seconds
Average score time: 0.062 seconds
</code></pre>
<p>El histogram gradient boosting es el mejor algoritmo en términos de puntuación. También escala cuando el número de muestras incrementa, mientras que el gradient boosting normal no.</p>
<h1 id="ajuste-de-hiperparámetros-con-métodos-de-conjunto"><a href="#ajuste-de-hiperparámetros-con-métodos-de-conjunto" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ajuste de hiperparámetros con métodos de conjunto</h1>
<h2 id="random-forest"><a href="#random-forest" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Random Forest</h2>
<p>El principal parámetro de random forest que hay que ajustar es el parámetro <code>n_estimators</code>. En general, cuantos más árboles en el bosque, mejor será el rendimiento de generalización. Sin embargo, se ralentizarán los tiempos de entrenamiento y predicción. El objetivo es balancear el tiempo de cálculo y el rendimiento de generalización al establecer el número de estimadores cuando se ponga en producción.</p>
<p>Luego podríamos ajustar también un parámetro que controla la profundidad de cada árbol del bosque. Dos parámetros son los importantes para esto: <code>max_depth</code> y <code>max_leaf_nodes</code>. Difieren en la forma en que controlan la estructura del árbol. <code>max_depth</code> obliga a tener un árbol más simétrico, mientras que <code>max_leaf_nodes</code> no impone tal restricción.</p>
<p>Tengamos en cuenta que con random forest los árboles son generalmente profundos, dado que se busca overfitting en cada árbol de cada muestra bootstrap porque esto será mitigado al combinarlos todos juntos. El ensamblaje de árboles con underfitting (es decir, árboles poco profundos) también podría conducir a bosques con underfitting.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># reescala en k$</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">param_distributions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;n_estimators&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&#34;max_leaf_nodes&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">search_cv</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_distributions</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">search_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="s2">&#34;param_{name}&#34;</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">param_distributions</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">columns</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">,</span> <span class="s2">&#34;std_test_error&#34;</span><span class="p">]</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">search_cv</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;mean_test_score&#34;</span><span class="p">]</span>
<span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;std_test_error&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;std_test_score&#34;</span><span class="p">]</span>
<span class="n">cv_results</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">)</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>param_n_estimators</th>
      <th>param_max_leaf_nodes</th>
      <th>mean_test_error</th>
      <th>std_test_error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>500</td>
      <td>100</td>
      <td>40.785932</td>
      <td>0.704460</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10</td>
      <td>100</td>
      <td>41.661666</td>
      <td>0.991435</td>
    </tr>
    <tr>
      <th>7</th>
      <td>100</td>
      <td>50</td>
      <td>43.925528</td>
      <td>0.767559</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>100</td>
      <td>47.130459</td>
      <td>0.846356</td>
    </tr>
    <tr>
      <th>6</th>
      <td>50</td>
      <td>20</td>
      <td>49.303910</td>
      <td>0.822624</td>
    </tr>
    <tr>
      <th>1</th>
      <td>100</td>
      <td>20</td>
      <td>49.483635</td>
      <td>0.835842</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>20</td>
      <td>50.097904</td>
      <td>0.558017</td>
    </tr>
    <tr>
      <th>3</th>
      <td>500</td>
      <td>10</td>
      <td>54.572481</td>
      <td>0.776725</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>5</td>
      <td>61.592333</td>
      <td>0.943317</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>2</td>
      <td>72.811144</td>
      <td>0.945929</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>Podemos observar en nuestra búsqueda que estamos obligados a tener un número alto de hojas y, por tanto, árboles profundos. Este parámetro parece particularmente impactante en comparación con el número de árboles para este dataset en particular: con al menos 50 árboles, el rendimiento de generalización será conducido por el número de hojas.</p>
<p>Ahora estimaremos el rendimiento de generalización del mejor modelo reentrenandolo con el conjunto de entrenamiento completo y usando el conjunto de prueba para evaluarlo en datos nunca vistos. Esto se hace por defecto llamando al método <code>.fit</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">error</span> <span class="o">=</span> <span class="o">-</span><span class="n">search_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;De media, nuestro regresor random forest tiene un error de {error:.2f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>De media, nuestro regresor random forest tiene un error de 42.00 k$
</code></pre>
<h2 id="arboles-de-decisión-gradient-boosting"><a href="#arboles-de-decisión-gradient-boosting" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Arboles de decisión gradient-boosting</h2>
<p>Para gradient boosting, los parámetros están emparejados, ya no se pueden establecer los parámetros uno tras otro. Los parámetros importantes son <code>n_estimators</code>, <code>learning_rate</code> y <code>max_depth</code> o <code>max_leaf_nodes</code> (como vimos anteriormente con random forest).</p>
<p>Primero veamos los parámetros <code>max_depth</code> (o <code>max_leaf_nodes</code>). Vimos anteriormente en gradient boosting que el algoritmo ajusta el error del árbol precedente en el conjunto. Por lo tanto, el ajuste de árboles completamente desarrollados sería perjudicial. De hecho, el primer árbol del conjunto ajustaría perfectamente (overfitting) los datos y, por lo tanto, no se requeriría ningún árbol posterior, dado que no habría residuos. Por lo tanto, el árbol usado en gradient boosting debe tener una profundidad baja, típicamente entre 3 y 8 niveles, o pocas hojas ($2^3=8$ a $2^8=256$). Tener árboles muy débiles en cada paso nos ayudará a reducir el overfitting.</p>
<p>Con esta consideración en mente, cuanto más profundos sean los árboles, más rápido se corregirán los residuos y menos árboles se requerirán. Por tanto, <code>n_estimators</code> deberá incrementarse si <code>max_depth</code> es bajo.</p>
<p>Finalmente, hemos pasado por alto el impacto del parámetro <code>learning_rate</code> hasta ahora. Cuando ajustamos los residuos, nos gustaría que el árbol intentara corregir todos los posibles errores o solo una fracción de ellos. La tasa de aprendizaje permite controlar este comportamiento. Una tasa baja podría corregir solo los resiudos de muy pocas muestras. Si se establece una tasa alta (por ejemplo, 1), podríamos ajustar los residuos de todas las muestras. Asi que, con una tasa de aprendizaje muy baja, necesitaremos más estimadores para corregir el error global. Sin embargo, una tasa de aprendizaje demasiado alta haría que obtuviéramos un conjunto con overfitting, de forma similar a tener una profundidad de árbol demasiado alta.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">loguniform</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="n">param_distributions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;n_estimators&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span>
    <span class="s2">&#34;max_leaf_nodes&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="s2">&#34;learning_rate&#34;</span><span class="p">:</span> <span class="n">loguniform</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">search_cv</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">GradientBoostingRegressor</span><span class="p">(),</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">param_distributions</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&#34;neg_mean_absolute_error&#34;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">search_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="s2">&#34;param_{name}&#34;</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">param_distributions</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">columns</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">,</span> <span class="s2">&#34;std_test_error&#34;</span><span class="p">]</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">search_cv</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;mean_test_score&#34;</span><span class="p">]</span>
<span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;std_test_error&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="s2">&#34;std_test_score&#34;</span><span class="p">]</span>
<span class="n">cv_results</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&#34;mean_test_error&#34;</span><span class="p">)</span></code></pre></div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<div class="table-container"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>param_n_estimators</th>
      <th>param_max_leaf_nodes</th>
      <th>param_learning_rate</th>
      <th>mean_test_error</th>
      <th>std_test_error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>200</td>
      <td>20</td>
      <td>0.160519</td>
      <td>33.914794</td>
      <td>0.421969</td>
    </tr>
    <tr>
      <th>12</th>
      <td>200</td>
      <td>50</td>
      <td>0.110585</td>
      <td>34.783054</td>
      <td>0.288929</td>
    </tr>
    <tr>
      <th>17</th>
      <td>500</td>
      <td>5</td>
      <td>0.771785</td>
      <td>34.857510</td>
      <td>0.570347</td>
    </tr>
    <tr>
      <th>10</th>
      <td>200</td>
      <td>20</td>
      <td>0.109889</td>
      <td>35.009108</td>
      <td>0.379727</td>
    </tr>
    <tr>
      <th>6</th>
      <td>500</td>
      <td>100</td>
      <td>0.709894</td>
      <td>35.540170</td>
      <td>0.393424</td>
    </tr>
    <tr>
      <th>18</th>
      <td>10</td>
      <td>5</td>
      <td>0.637819</td>
      <td>42.535730</td>
      <td>0.338066</td>
    </tr>
    <tr>
      <th>3</th>
      <td>500</td>
      <td>2</td>
      <td>0.07502</td>
      <td>43.457866</td>
      <td>0.704599</td>
    </tr>
    <tr>
      <th>4</th>
      <td>100</td>
      <td>5</td>
      <td>0.0351</td>
      <td>46.558900</td>
      <td>0.578629</td>
    </tr>
    <tr>
      <th>19</th>
      <td>5</td>
      <td>20</td>
      <td>0.202432</td>
      <td>61.387176</td>
      <td>0.610988</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5</td>
      <td>2</td>
      <td>0.462636</td>
      <td>65.114017</td>
      <td>0.846987</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>5</td>
      <td>0.088556</td>
      <td>66.243538</td>
      <td>0.720131</td>
    </tr>
    <tr>
      <th>15</th>
      <td>50</td>
      <td>100</td>
      <td>0.010904</td>
      <td>71.847050</td>
      <td>0.683326</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>2</td>
      <td>0.421054</td>
      <td>74.384704</td>
      <td>0.791104</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>100</td>
      <td>0.070357</td>
      <td>77.007841</td>
      <td>0.789595</td>
    </tr>
    <tr>
      <th>16</th>
      <td>2</td>
      <td>50</td>
      <td>0.167568</td>
      <td>77.131005</td>
      <td>0.850380</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1</td>
      <td>5</td>
      <td>0.190477</td>
      <td>82.819015</td>
      <td>0.976351</td>
    </tr>
    <tr>
      <th>13</th>
      <td>5</td>
      <td>20</td>
      <td>0.033815</td>
      <td>83.765509</td>
      <td>0.974672</td>
    </tr>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>100</td>
      <td>0.125207</td>
      <td>85.363288</td>
      <td>1.040982</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1</td>
      <td>10</td>
      <td>0.081715</td>
      <td>87.373374</td>
      <td>1.071555</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>20</td>
      <td>0.014937</td>
      <td>90.531295</td>
      <td>1.113892</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>Aquí hemos ajustado <code>n_estimators</code>, pero habría que tener en cuenta que sería mejor usar parada temprana, como vimos anteriormente.</p>
<p>En esta búsqueda, vemos que <code>learning_rate</code> es obligado que sea suficientemente alto, es decir, &gt;0.1. También observamos que para los modelos mejor clasificados, al tener un <code>learning_rate</code> más pequeño, se requerirán más árboles o un número de hojas más grande por cada árbol. Sin embargo, es particularmente difícil dibujar conclusiones con más detalle, dado que el mejor valor de un hiperparámetro depende de los valores de los otros hiperparámetros.</p>
<p>Ahora estimaremos el rendimiento de generalización del mejor modelo usando el conjunto de prueba.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">error</span> <span class="o">=</span> <span class="o">-</span><span class="n">search_cv</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;De media, nuestro regresor GBDT tiene un error de {error:.2f} k$&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>De media, nuestro regresor GBDT tiene un error de 32.99 k$
</code></pre>
<p>La puntuación de prueba media en el conjunto de prueba externo es ligeramente mejor que la puntuación del mejor modelo. La razón es que el modelo final se entrena en el conjunto de entrenamiento completo y, por tanto, en más datos que los modelos de validación cruzada internos del procedimiento grid search.</p>
<h2 id="ejercicio-ajuste-hiperparámetros"><a href="#ejercicio-ajuste-hiperparámetros" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio ajuste hiperparámetros</h2>
<p>El objetivo es familizarizarse con histogram gradient boosting en scikit-learn. Además, usaremos el modelo dentro de un framework de validación cruzada para inspeccionar los parámetros internos que encontramos vía grid search.</p>
<p>Cargaremos el dataset de propiedades de California.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">*=</span> <span class="mi">100</span></code></pre></div>
<p>En primer lugar, creamos un regresor histogram gradient boosting. Podemos establecer que el número de árboles sea grande y configurar el modelo para que use parada temprana.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="n">histogram_gradient_boosting</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span>
    <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span></code></pre></div>
<p>Usaremos grid search para encontrar algunos parámetros óptimos para este modelo. En este grid search, deberemos buscar los siguientes parámetros:</p>
<ul>
<li><code>max_depth: [3, 8]</code>;</li>
<li><code>max_leaf_nodes: [15, 31]</code>;</li>
<li><code>learning_rate: [0.1, 1]</code>.</li>
</ul>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&#34;max_depth&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
    <span class="s2">&#34;max_leaf_nodes&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">31</span><span class="p">],</span>
    <span class="s2">&#34;learning_rate&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">}</span>

<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">histogram_gradient_boosting</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">)</span></code></pre></div>
<p>Finalmente, ejecutaremos nuestro modelo a través de validación cruzada. En esta ocasión, definiremos 5 particiones de validación cruzada. Además, nos aseguraremos de mezclar los datos. En consecuencia, usaremos la función <code>sklearn.model_selection.cross_validate</code> para ejecutar la validación cruzada. También debemos establecer <code>return_estimator=True</code>, así podremos investigar los modelos internos entrenados por validación cruzada.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">KFold</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">search</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></div>
<p>Ahora que tenemos los resultados de la validación cruzada, mostremos la media y la desviación típica de la puntuación.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;R2 score con validación cruzada:</span><span class="se">\n</span><span class="s2">&#34;</span>
      <span class="n">f</span><span class="s2">&#34;{results[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{results[&#39;test_score&#39;].std():.3f}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>R2 score con validación cruzada:
0.839 +/- 0.006
</code></pre>
<p>Ahora inspeccionaremos la entrada <code>estimator</code> de los resultados y comprobaremos los valores de los mejores parámetros. Además, verificaremos el número de árboles usados por el modelo.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">results</span><span class="p">[</span><span class="s2">&#34;estimator&#34;</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;# trees: {estimator.best_estimator_.n_iter_}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>{'learning_rate': 0.1, 'max_depth': 3, 'max_leaf_nodes': 15}
# trees: 528
{'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15}
# trees: 447
{'learning_rate': 0.1, 'max_depth': 3, 'max_leaf_nodes': 15}
# trees: 576
{'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15}
# trees: 290
{'learning_rate': 0.1, 'max_depth': 8, 'max_leaf_nodes': 15}
# trees: 414
</code></pre>
<p>Inspeccionaremos los resultados de la validación cruzada interna para cada estimador de la validación cruzada externa. Calcularemos la puntuación de prueba media agregada para cada combinación de parámetro y dibujaremos un box plot de esas puntuaciones.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">index_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="s2">&#34;param_{name}&#34;</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">param_grid</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="n">index_columns</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&#34;mean_test_score&#34;</span><span class="p">]</span>

<span class="n">inner_cv_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">cv_idx</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s2">&#34;estimator&#34;</span><span class="p">]):</span>
    <span class="n">search_cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
    <span class="n">search_cv_results</span> <span class="o">=</span> <span class="n">search_cv_results</span><span class="p">[</span><span class="n">columns</span><span class="p">]</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">index_columns</span><span class="p">)</span>
    <span class="n">search_cv_results</span> <span class="o">=</span> <span class="n">search_cv_results</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
        <span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;mean_test_score&#34;</span><span class="p">:</span> <span class="n">f</span><span class="s2">&#34;CV {cv_idx}&#34;</span><span class="p">})</span>
    <span class="n">inner_cv_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">search_cv_results</span><span class="p">)</span>
<span class="n">inner_cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">inner_cv_results</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">color</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;whiskers&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;medians&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="s2">&#34;caps&#34;</span><span class="p">:</span> <span class="s2">&#34;black&#34;</span><span class="p">}</span>
<span class="n">inner_cv_results</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">box</span><span class="p">(</span><span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;R2 score&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Parámetros&#34;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34; Resultados de CV interna con parámetros</span><span class="se">\n</span><span class="s2">&#34;</span>
              <span class="s2">&#34;(max_depth, max_leaf_nodes, learning_rate)&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_199_0.png" alt="png"></p>
<p>Vemos que los primeros 4 conjuntos de parámetros clasificados están muy cercanos. Podríamos seleccionar cualquier de esas 4 combinaciones. Coincide con los resultados que observamos inspeccionando los mejor parámetros de la validación cruzada externa.</p>
<h1 id="ejercicio"><a href="#ejercicio" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Ejercicio</h1>
<p>Vamos a poner en práctica lo aprendido en este post con un ejercicio. Para ellos usaremos el dataset de pingüinos, pero no usaremos el objetivo tradicional de predecir la especie.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;../data/penguins.csv&#34;</span><span class="p">)</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&#34;Culmen Length (mm)&#34;</span><span class="p">,</span>
    <span class="s2">&#34;Culmen Depth (mm)&#34;</span><span class="p">,</span>
    <span class="s2">&#34;Flipper Length (mm)&#34;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">target_name</span> <span class="o">=</span> <span class="s2">&#34;Body Mass (g)&#34;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_names</span> <span class="o">+</span> <span class="p">[</span><span class="n">target_name</span><span class="p">]]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&#34;rows&#34;</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s2">&#34;any&#34;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">target_name</span><span class="p">]</span></code></pre></div>
<p>El objetivo es predicir la masa corporal (<code>Body Mass (g)</code>) de un pingüino, dadas sus medidas de pico y aleta. Por lo tanto, se trata de un problema de regresión.</p>
<p>Véase que hemos mezclado aleatoriamente las filas del dataset después de cargarlo (<code>df.sample(frac=1, random_state=0)</code>). La razón es romper una dependencia estadística espúrea relacionada con el orden que pudiera causar problemas con el procedimiento simple de validación cruzada que usamos en este post. Evaluaremos los siguientes modelos basados en árboles:</p>
<ul>
<li>un árbol de decisión regresor, es decir, <code>sklearn.tree.DecisionTreeRegressor</code></li>
<li>un random forest regresor, e decir, <code>sklearn.ensemble.RandomForestRegressor</code></li>
</ul>
<p>Usaremos las configuraciones por defecto de hiperparámetros para ambos modelos. Tenemos que evaluar el rendimiento de generalización de dichos modelos usando una validación cruzada de 10-particiones:</p>
<ul>
<li>usaremos <code>sklearn.model_selection.cross_validate</code> para ejecutar la rutina de validación cruzada</li>
<li>estableceremos el parámetro <code>cv=10</code> para usar una estrategia de validación cruzada de 10-particiones. Almacenaremos la puntuación de entrenamiento de validación cruzada estableciendo el parámetro <code>return_train_score=True</code> en la función <code>cross_validate</code>.</li>
</ul>
<p><strong>Comparando las puntuaciones de prueba de validación cruzada partición a partición, cuenta el número de veces que un random forest es mejor que un único árbol de decisión.</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span>

<span class="n">cv</span><span class="o">=</span><span class="mi">10</span>
<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores_tree</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                             <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">scores_random_forest</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">random_forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                                      <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_tree</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores_tree</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.6274987889886428, 0.099704183300966)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_random_forest</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores_random_forest</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.8035860871438268, 0.04467350184659844)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_random_forest</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">scores_tree</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True])
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Random forest es mejor que un árbol de decisión en &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{sum(scores_random_forest[&#39;test_score&#39;] &gt; scores_tree[&#39;test_score&#39;])} &#34;</span>
      <span class="n">f</span><span class="s2">&#34;iteraciones de {cv}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Random forest es mejor que un árbol de decisión en 10 iteraciones de 10
</code></pre>
<p>Ahora vamos a entrenar y evaluar con la misma estrategia de validación cruzada un random forest con 5 árboles de decisión y otro que contenga 100 árboles de decisión. También almacenaremos la puntuación de entrenamiento.</p>
<p><strong>Comparando las puntuaciones de test partición a partición, cuenta el número de veces que un random forest con 100 árboles de decisión es mejor que un árbol de decisión con 5 árboles decisión.</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">random_forest_5</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">random_forest_100</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores_rf_5</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">random_forest_5</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                                      <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                      <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores_rf_100</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">random_forest_100</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                                      <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                      <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_rf_5</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores_rf_5</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.7668640137103877, 0.07359278138744398)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">scores_rf_100</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores_rf_100</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.8035860871438268, 0.04467350184659844)
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Random forest (100) es mejor que un random forest (5) en &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{sum(scores_rf_100[&#39;test_score&#39;] &gt; scores_rf_5[&#39;test_score&#39;])} &#34;</span>
      <span class="n">f</span><span class="s2">&#34;iteraciones de {cv}&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Random forest (100) es mejor que un random forest (5) en 9 iteraciones de 10
</code></pre>
<p>Añadir árboles al bosque ayuda a mejorar el rendimiento de generalización del modelo. Podemos obtener algún conocimiento adicional comparando las puntuaciones de prueba y entrenamiento de cada modelo:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuaciones para random forest con 5 árboles: </span><span class="se">\n</span><span class="s2">&#34;</span>
      <span class="n">f</span><span class="s2">&#34; Entrenamiento: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_5[&#39;train_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_5[&#39;train_score&#39;].std():.3f}</span><span class="se">\n</span><span class="s2">&#34;</span>
      <span class="n">f</span><span class="s2">&#34; Prueba       : &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_5[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_5[&#39;test_score&#39;].std():.3f}</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Puntuaciones para random forest con 100 árboles: </span><span class="se">\n</span><span class="s2">&#34;</span>
      <span class="n">f</span><span class="s2">&#34; Entrenamiento: &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_100[&#39;train_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_100[&#39;train_score&#39;].std():.3f}</span><span class="se">\n</span><span class="s2">&#34;</span>
      <span class="n">f</span><span class="s2">&#34; Prueba       : &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_100[&#39;test_score&#39;].mean():.3f} +/- &#34;</span>
      <span class="n">f</span><span class="s2">&#34;{scores_rf_100[&#39;test_score&#39;].std():.3f}</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span></code></pre></div>
<pre><code>Puntuaciones para random forest con 5 árboles: 
 Entrenamiento: 0.950 +/- 0.003
 Prueba       : 0.767 +/- 0.074

Puntuaciones para random forest con 100 árboles: 
 Entrenamiento: 0.972 +/- 0.001
 Prueba       : 0.804 +/- 0.045
</code></pre>
<p>En el modelo con 5 árboles, la puntuación media de entrenamiento ya era bastante alta pero la puntuación media de prueba es bastante baja. El rendimiento de este random forest pequeño está limitado, por tanto, por el overfitting.</p>
<p>En el bosque con 100 árboles, la puntuación de entrenamiento sigue siendo alta (incluso ligeramente más alta), y la puntuación de prueba se ha reducido. El overfitting se ha reducido añadiendo más árboles al bosque.</p>
<p><strong>Dibuja la curva de estimación de los parámetros <code>n_estimators</code> definidos por <code>n_estimators = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000]</code></strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">validation_curve</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">param_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">]</span>

<span class="n">rf_train_scores</span><span class="p">,</span> <span class="n">rf_test_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
    <span class="n">rf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">param_name</span><span class="o">=</span><span class="s2">&#34;n_estimators&#34;</span><span class="p">,</span>
    <span class="n">param_range</span><span class="o">=</span><span class="n">param_range</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_train_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_test_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_test_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación prueba&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&#34;log&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Nº de árboles&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;R2 score&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Curva de validación de Random Forest&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_221_0.png" alt="png"></p>
<p>Repite el experimento previo pero esta vez, en lugar de elegir los parámetros por defecto para random forest, establece el parámetro <code>max_depth=5</code> y contruye la curva de validación.</p>
<p><strong>Comparando la curva de validación (entrenamiento y prueba) del random forest con profundidad total y el random forest con la profundidad limitada, ¿Qué conclusiones se pueden obtener?</strong></p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_5</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">param_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">]</span>

<span class="n">rf_5_train_scores</span><span class="p">,</span> <span class="n">rf_5_test_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
    <span class="n">rf_5</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">param_name</span><span class="o">=</span><span class="s2">&#34;n_estimators&#34;</span><span class="p">,</span>
    <span class="n">param_range</span><span class="o">=</span><span class="n">param_range</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_train_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_test_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_test_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación prueba&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Random forest profundidad total&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&#34;log&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;Nº de árboles&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;R2 score&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_5_train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_5_train_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">rf_5_test_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">rf_5_test_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación prueba&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;Random forest profundidad 5&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&#34;log&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;Nº de árboles&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;R2 score&#34;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&#34;Curva de validación de Random Forest (max_dept=5)&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_224_0.png" alt="png"></p>
<p>La puntuación de entrenamiento del random forest con profundidad total es casi siempre mejor que la puntuación de entrenamiento del random forest con una profundidad limitada. La diferencia entre las puntuaciones de entrenamiento y prueba disminuye cuando se reduce la profundidad de los árboles del random forest.</p>
<p>También observamos que el random forest con profundidad limitada tiene un mejor rendimiento de generalización para un pequeño número de árboles pero que es equivalente para un número alto de árboles. Podemos concluir que los modelos random forest con una profundidad limitada de árboles tienen menos overfitting que los random forest con árboles totalmente desarrollados, especialmente cuando el número de árboles del conjunto es pequeño.</p>
<p>También observamos que limitar la profundidad tiene un efecto significativo en la limitación de la puntuación de entrenamiento (habilidad para memorizar exactamente los datos de entrenamiento) y que este efecto sigue siendo importante incluso cuando se aumenta el tamaño del conjunto.</p>
<p>Vamos a centrarnos ahora en el principio de las curvas de validación y a considerar la puntuación de entrenamiento de un random forest con un único árbol mientras usamos la configuración del parámetro por defecto <code>max_depth=None</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_1_tree</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results_tree</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">rf_1_tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">cv_results_tree</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>array([0.83120264, 0.83309064, 0.83195043, 0.84834224, 0.85790323,
       0.86235297, 0.84791111, 0.85183089, 0.82241954, 0.85045978])
</code></pre>
<p>El hecho de que este random forest de un único árbol nunca pueda alcanzar la puntuación perfecta R2 de 1.0 en el entrenamiento puede ser sorprendente. De hecho, si evaluamos la precisión de entrenamiento del único <code>DecissionTreeRegressor</code> se obtiene una perfecta memorización de los datos de entrenamiento:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results_tree</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">cv_results_tree</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
</code></pre>
<p><strong>¿Cómo se explica que un random forest de un único árbol no pueda alcanzar la puntuación perfecta de entrenamiento?</strong></p>
<p>La respuesta es que el único árbol del random forest se entrena usando bootstrap en el conjunto de entrenamiento y no el conjunto de entrenamiento en sí mismo (porque <code>bootstrap = True</code> por defecto).</p>
<p>Por defecto, random forest entrena los árboles con el procedimiento de bootstrapping. Dado que cada árbol es entrenado en una muestra bootstrap, algunos puntos de datos del conjunto de entrenamiento original no son vistos por árboles individuales en el bosque. Como resultado, el único árbol del modelo random forest no hace predicciones perfectas en esos puntos (denominados muestras out-of-bag), lo cual previene que la puntuación de entrenamiento alcance 1.0.</p>
<p>Podemos confirmar esta hipótesis desactivando la opción bootstrap y comprobando de nuevo las puntuaciones de entrenamiento (dejando resto de hiperparámetros inalterados):</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">rf_1_tree</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                  <span class="n">bootstrap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                  <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results_tree</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">rf_1_tree</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">cv_results_tree</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">]</span></code></pre></div>
<pre><code>array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
</code></pre>
<p>En este caso, recurrimos al mismo algoritmo que el árbol de decisión único, entrenandolo en los datos originales y, por tanto, generando overfitting en el dataset de entrenamiento original.</p>
<p>También podemos advertir que cuando se incrementa el número de árboles a 5 ó 10, el entrenamiento sub-óptimo causado por bootstrapping se desvanece rápidamente. Los random forest son entrenados con al menos 10 árboles y, típicamente, muchos más que esos, así este efecto casi no se observa en la práctica.</p>
<p>Además, incluso con un único árbol con bootstrapping, una puntuación de R2 mayor que 0.85 todavía es muy alta y, típicamente, mucho mayor que la puntuación de prueba del mismo modelo. Como resultado, incluso si la puntuación de entrenamiento de este modelo no es perfecta, no se puede concluir que el rendimiento de generalización de un random forest con un único árbol está limitado por el underfitting. En cambio, los modelos de árboles únicos normalmente están limitados por overfitting cuando la profundidad no está limitada.</p>
<p>Construye una curva de validación para un <code>sklearn.ensemble.HistGradientBoostingRegressor</code>, variando <code>max_iter</code> como sigue:</p>
<p><code>max_iter = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1_000]</code></p>
<p>Recordemos que <code>max_iter</code> corresponde con el número de árboles en los modelos boosting.</p>
<p><strong>Dibuja las puntuaciones medias de entrenamiento y prueba para cada valor de <code>max_iter</code></strong>.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="n">hgbr</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">param_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1_000</span><span class="p">]</span>

<span class="n">hgbr_train_scores</span><span class="p">,</span> <span class="n">hgbr_test_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span>
    <span class="n">hgbr</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
    <span class="n">param_name</span><span class="o">=</span><span class="s2">&#34;max_iter&#34;</span><span class="p">,</span>
    <span class="n">param_range</span><span class="o">=</span><span class="n">param_range</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span></code></pre></div>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">hgbr_train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">hgbr_train_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación entrenamiento&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span>
    <span class="n">param_range</span><span class="p">,</span>
    <span class="n">hgbr_test_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">yerr</span><span class="o">=</span><span class="n">hgbr_test_scores</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Puntuación prueba&#34;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&#34;log&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Nº de árboles&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;R2 score&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Curva de validación de HGBR&#34;</span><span class="p">)</span></code></pre></div>
<p><img src="/images/output_236_0.png" alt="png"></p>
<p>En la gráfica podemos observar claramente las tres fases del comportamiento &ldquo;undefitting / mejor generalización / overfitting&rdquo; de los modelos gradient boosting. Con un número bajo de árboles, el modelo tiene una puntuaciones baja tanto para el entrenamiento como en la prueba. Podemos ver claramente que la puntuación de prueba está limitada por la puntuación de entrenamiento, que es una característica de los modelos con underfitting (contrariamente a lo observado anteriormente en la curva de aprendizaje de los modelos random forest).</p>
<p>Ambas puntuaciones mejoran hasta alcanzar un punto dulce (alrededor de 50 árboles) donde la puntuación de prueba es máxima. Después de esto, el algoritmo de gradient boosting empieza con overfitting: la puntuación de entrenamiento mejora hasta alcanzar una puntuación perfecta de 1.0, mientras que la puntuación de prueba se reduce. De hecho, el modelo empieza a memorizar reglas específicas que solo se cumplen en el conjunto de entrenamiento. Estas reglas aprendidas van en detrimento del rendimiento de generalización del modelo.</p>
<p>Aquí se muestra la importancia de no añadir demasiados árboles a nuestro conjunto de gradient boosting. De hecho, podemos usar parada temprana y monitorizar el rendimiento en un conjunto de validación interno para detener la adición de nuevos árboles cuando la puntuación de validación no mejore. Este es un ejemplo que muestra cómo hacer esto de forma automática:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">hgbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv_results_hgbdt</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span>
    <span class="n">hgbdt</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">cv_results_hgbdt</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">cv_results_hgbdt</span><span class="p">[</span><span class="s2">&#34;train_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.8802093174685013, 0.009772033922083758)
</code></pre>
<p>Vemos que la puntuación de entrenamiento no es perfecta, lo que significa que nuestro modelo se ha detenido mucho antes de añadir demasiados árboles. Podemos comprobar el rendimiento de generalización para asegurar que es un modelo equivalente al random forest previo:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">cv_results_hgbdt</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">cv_results_hgbdt</span><span class="p">[</span><span class="s2">&#34;test_score&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></code></pre></div>
<pre><code>(0.8075456252855009, 0.030400979794505564)
</code></pre>
<p>Observamos que, de media, el rendimiento del modelo es tan bueno como un gran random forest. Por último, podemos comprobar cuántos árboles ha usado por cada iteración de validación cruzada:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">est</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv_results_hgbdt</span><span class="p">[</span><span class="s2">&#34;estimator&#34;</span><span class="p">]):</span>
    <span class="k">print</span><span class="p">(</span>
        <span class="n">f</span><span class="s2">&#34;Para la iteración CV {idx + 1}, se han construido {est.n_iter_} árboles&#34;</span>
    <span class="p">)</span></code></pre></div>
<pre><code>Para la iteración CV 1, se han construido 60 árboles
Para la iteración CV 2, se han construido 50 árboles
Para la iteración CV 3, se han construido 46 árboles
Para la iteración CV 4, se han construido 29 árboles
Para la iteración CV 5, se han construido 33 árboles
Para la iteración CV 6, se han construido 33 árboles
Para la iteración CV 7, se han construido 36 árboles
Para la iteración CV 8, se han construido 31 árboles
Para la iteración CV 9, se han construido 24 árboles
Para la iteración CV 10, se han construido 23 árboles
</code></pre>
<p>Por lo tanto, vemos que nunca usamos más de 60 árboles, antes de entrar en la zona de overfitting que observamos en la curva de validación.</p>
<p>Incluso si este modelo no es tan fuerte como un gran random forest, es más pequeño lo que significa que puede ser más rápido para predecir y usará menos memoria (RAM) en las máquinas donde se despliegue. Esta es una ventaja práctica de los árboles gradient boosting con parada temprana sobre random forest con una gran número de profundidad de árboles.</p>
<h1 id="resumen"><a href="#resumen" class="anchor-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a>Resumen</h1>
<p>En este post hemos discutido los predictores de conjunto, que son un tipo de predictores que combinan predictores más simples juntos. Hemos visto dos estrategias:</p>
<ul>
<li>una basada en muestras bootstrap que permite a los predictores ser entrenados en paralelo;</li>
<li>la otra llamada boosting que entrena los predictores secuencialmente.</li>
</ul>
<p>De estas dos familias nos hemos enfocado principalmente en proporcionar intuiciones sobre la maquinaria interna de los modelos de random forest y gradient-boosting, los cuales son métodos de última generación.</p>
<p>Algunas referencias a seguir con ejemplos de algunos conceptos mencionados:</p>
<ul>
<li><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-early-stopping-py" target="_blank" rel="noopener">Parada temprana en gradient boosting</a></li>
<li><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py" target="_blank" rel="noopener">Combinación de predictores usando stacking</a></li>
</ul>

            </div>

            
    
    
        <ul class="post-copyright">
            <li class="copyright-item author"><span class="copyright-item-text"></span><a href="https://sgtsteiner.github.io/" class="p-author h-card" target="_blank" rel="noopener">Antonio Méndez</a></li>
            
                
                
                
                
                <li class="copyright-item link"><span class="copyright-item-text"></span><a href="/posts/modelos-conjunto/" target="_blank" rel="noopener">https://sgtsteiner.github.io/posts/modelos-conjunto/</a></li>
            
            <li class="copyright-item license"><span class="copyright-item-text"></span><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></li>
            
        </ul>
    



        </article>

        

        
    <div class="updated-badge-container">
        <span title="Updated @ 2022-05-03 17:20:37 CEST" style="cursor:help">

<svg xmlns="http://www.w3.org/2000/svg" width="130" height="20" class="updated-badge"><linearGradient id="b" x2="0" y2="100%"><stop offset="0" stop-color="#bbb" stop-opacity=".1"/><stop offset="1" stop-opacity=".1"/></linearGradient><clipPath id="a"><rect width="130" height="20" rx="3" fill="#fff"/></clipPath><g clip-path="url(#a)"><path class="updated-badge-left" d="M0 0h55v20H0z"/><path class="updated-badge-right" d="M55 0h75v20H55z"/><path fill="url(#b)" d="M0 0h130v20H0z"/></g><g fill="#fff" text-anchor="middle" font-size="110"><text x="285" y="150" fill="#010101" fill-opacity=".3" textLength="450" transform="scale(.1)">updated</text><text x="285" y="140" textLength="450" transform="scale(.1)">updated</text><text x="915" y="150" fill="#010101" fill-opacity=".3" textLength="650" transform="scale(.1)">2022-05-03</text><text x="915" y="140" textLength="650" transform="scale(.1)">2022-05-03</text></g></svg>
        </span></div>



        


        <div class="post-share">

        

        <div class="share-items">

            
                <div class="share-item twitter">
                    
                    <a href="https://twitter.com/share?url=https://sgtsteiner.github.io/posts/modelos-conjunto/&amp;text=Modelos%20de%20conjunto&amp;hashtags=ensemblemodels,bagging,bootstrap,boosting,randomforest,gradient-boosting,paradatemprana,early-stopping,&amp;via=Steiner_69" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon twitter-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </div>
            

            

            
                <div class="share-item linkedin">
                    
                    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://sgtsteiner.github.io/posts/modelos-conjunto/&amp;title=Modelos%20de%20conjunto&amp;summary=En%20este%20post%20veremos%20en%20detalle%20los%20algoritmos%20que%20combinan%20varios%20modelos%20juntos,%20tambi%c3%a9n%20llamados%20conjunto%20de%20modelos.%20Presentaremos%20dos%20familias%20de%20estas%20t%c3%a9cnicas:%20%28i%29%20basados%20en%20bootstrapping%20y%20%28ii%29%20basados%20en%20boosting.%20Presentaremos%20bagging%20y%20%c3%a1rboles%20aleatorios%20como%20pertenecientes%20a%20la%20primera%20estrategia%20y%20AdaBoost%20y%20%c3%a1rboles%20de%20decisi%c3%b3n%20gradient%20boosting%20que%20pertenecen%20a%20la%20%c3%baltima%20estrategia.&amp;source=Lords%20of%20the%20Machine%20Learning" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon linkedin-icon"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg></a>
                </div>
            

            
                <div class="share-item telegram">
                    
                    <a href="https://t.me/share/url?url=https://sgtsteiner.github.io/posts/modelos-conjunto/&amp;text=Modelos%20de%20conjunto" title="" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon telegram-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </div>
            

            

            

            

            

            
                <div class="share-item qrcode">
                    <div class="qrcode-container" title=""><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon qrcode-icon"><path d="M0 224h192V32H0v192zM64 96h64v64H64V96zm192-64v192h192V32H256zm128 128h-64V96h64v64zM0 480h192V288H0v192zm64-128h64v64H64v-64zm352-64h32v128h-96v-32h-32v96h-64V288h96v32h64v-32zm0 160h32v32h-32v-32zm-64 0h32v32h-32v-32z"/></svg><div id="qrcode-img"></div>
                    </div>
                    <script src="https://cdn.jsdelivr.net/npm/qrcode-generator@1.4.4/qrcode.min.js"></script>

<script>
    var typeNumber = 0;
    var errorCorrectionLevel = 'L';
    var qr = qrcode(typeNumber, errorCorrectionLevel);
    qr.addData('https:\/\/sgtsteiner.github.io\/posts\/modelos-conjunto\/');
    qr.make();
    document.getElementById('qrcode-img').innerHTML = qr.createImgTag();
</script>

                </div>
            

        </div>

    </div>




        
    
    
        <div class="related-posts">
            <h2 class="related-title"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon related-icon"><path d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm144 276c0 6.6-5.4 12-12 12h-92v92c0 6.6-5.4 12-12 12h-56c-6.6 0-12-5.4-12-12v-92h-92c-6.6 0-12-5.4-12-12v-56c0-6.6 5.4-12 12-12h92v-92c0-6.6 5.4-12 12-12h56c6.6 0 12 5.4 12 12v92h92c6.6 0 12 5.4 12 12v56z"/></svg></h2>
            <ul class="related-list">
                
                    <li class="related-item">
                        <a href="/posts/wine-quality-clasificacion-multiclase/" class="related-link">Calidad del vino - Clasificación multiclase</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/evaluacion-modelos/" class="related-link">Evaluación del rendimiento de modelos</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/modelos-arbol-decision/" class="related-link">Modelos de árbol de decisión</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/modelos-lineales/" class="related-link">Modelos Lineales</a>
                    </li>
                
                    <li class="related-item">
                        <a href="/posts/hyperparameters-tuning/" class="related-link">Ajuste de hiperparámetros</a>
                    </li>
                
            </ul>
        </div>
    



        
    
        <div class="post-tags">
            
                
                
                
                
                    
                    <a href="/tags/ensemble-models/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>ensemble models</a>
                
            
                
                
                
                
                    
                    <a href="/tags/bagging/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>bagging</a>
                
            
                
                
                
                
                    
                    <a href="/tags/bootstrap/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>bootstrap</a>
                
            
                
                
                
                
                    
                    <a href="/tags/boosting/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>boosting</a>
                
            
                
                
                
                
                    
                    <a href="/tags/random-forest/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>random forest</a>
                
            
                
                
                
                
                    
                    <a href="/tags/gradient-boosting/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>gradient-boosting</a>
                
            
                
                
                
                
                    
                    <a href="/tags/parada-temprana/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>parada temprana</a>
                
            
                
                
                
                
                    
                    <a href="/tags/early-stopping/" rel="tag" class="post-tags-link"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 0 1 0 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>early-stopping</a>
                
            
        </div>
    



        


        


        
    
        
        
    
    
    
    
        <ul class="post-nav">
            
                <li class="post-nav-prev">
                    <a href="/posts/evaluacion-modelos/" rel="prev">&lt; Evaluación del rendimiento de modelos</a>
                </li>
            
            
                <li class="post-nav-next">
                    <a href="/posts/modelos-arbol-decision/" rel="next">Modelos de árbol de decisión &gt;</a>
                </li>
            
        </ul>
    



        


    </div>
</main>


            
    <div id="back-to-top" class="back-to-top">
        <a href="#"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6 0-33.9L207 39c9.4-9.4 24.6-9.4 33.9 0l194.3 194.3c9.4 9.4 9.4 24.6 0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3 0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a>
    </div>


            
    <footer id="footer" class="footer">
        <div class="footer-inner">
            <div class="site-info">©&nbsp;2021–2022&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3 0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;Antonio Méndez</div><div class="site-copyright"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.es" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a></div>

            
    
        <ul class="socials"><li class="socials-item">
                    <a href="/rss.xml" target="_blank" rel="external noopener" title="RSS"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"/></svg></a>
                </li><li class="socials-item">
                    <a href="mailto:futitotal@gmail.com" target="_blank" rel="external noopener" title="Email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://github.com/SgtSteiner" target="_blank" rel="external noopener" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://twitter.com/Steiner_69" target="_blank" rel="external noopener" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></a>
                </li><li class="socials-item">
                    <a href="https://t.me/SgtSteiner" target="_blank" rel="external noopener" title="Telegram"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" class="icon social-icon"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm121.8 169.9l-40.7 191.8c-3 13.6-11.1 16.9-22.4 10.5l-62-45.7-29.9 28.8c-3.3 3.3-6.1 6.1-12.5 6.1l4.4-63.1 114.9-103.8c5-4.4-1.1-6.9-7.7-2.5l-142 89.4-61.2-19.1c-13.3-4.2-13.6-13.3 2.8-19.7l239.1-92.2c11.1-4 20.8 2.7 17.2 19.5z"/></svg></a>
                </li></ul>
    



            
        </div>
    </footer>


        </div>
        

        
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css" integrity="sha256-gPJfuwTULrEAAcI3X4bALVU/2qBU+QY/TpoD3GO+Exw=" crossorigin="anonymous">
<script>
    if (typeof renderMathInElement === 'undefined') {
        var getScript = (options) => {
            var script = document.createElement('script');
            script.defer = true;
            script.crossOrigin = 'anonymous';
            Object.keys(options).forEach((key) => {
                script[key] = options[key];
            });
            document.body.appendChild(script);
        };
        getScript({
            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js',
            integrity: 'sha256-YTW9cMncW/ZQMhY69KaUxIa2cPTxV87Uh627Gf5ODUw=',
            onload: () => {
                getScript({
                    src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js',
                    integrity: 'sha256-yzSfYeVsWJ1x+2g8CYHsB/Mn7PcSp8122k5BM4T3Vxw=',
                    onload: () => {
                        getScript({
                            src: 'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js',
                            integrity: 'sha256-fxJzNV6hpc8tgW8tF0zVobKa71eTCRGTgxFXt1ZpJNM=',
                            onload: () => {
                                renderKaTex();
                            }
                        });
                    }
                });
            }
        });
    } else {
        renderKaTex();
    }
    function renderKaTex() {
        renderMathInElement(
            document.body,
            {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ]
            }
        );
    }
</script>










    <script src="https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js"></script>

<script>
    mediumZoom(document.querySelectorAll('div.post-body img'), {
        background: 'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'
    })
</script>




    <script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js" type="module" defer></script>







    </body>
</html>
